---
layout: post
title: gre协议源码解析
category: kernel
typora-root-url: ../../..
---

### GRE协议标准

Generic Routing Encapsulation (GRE)通用路由封装协议，基于IP网络层协议封装以太网报文，可用于在IPSec VPN网络间传输多播路由信息报文，或者在PPTP协议中，承载PPP数据报文。其在数据帧中的位置如下：

    |-------------------|----------------|----------------------|------------------|
    |  Outer IP Header  |   GRE Header   |    Inner IP Header   |    Payload       |
    |-------------------|----------------|----------------------|------------------|
GRE头部最小4个字节长度，其后为可选字段，第一个字节中的标志位决定是否存在后面的字段，最大长度为16个字节。以下为一个标准的GRE报头格式：

    |------------------------------------------------------------------------------|
    |    bit0-3   |      4-12      | 13-15 |                16 - 31                |
    |-------------|----------------|-------|---------------------------------------|
    | C | | K | S |    Reserved0   |  Ver  |            Protocol Type              |
    |--------------------------------------|---------------------------------------|
    |           Checksum(optional)         |         Reserved1(optional)           |         
    |------------------------------------------------------------------------------|
    |                                 Key(optional)                                |
    |------------------------------------------------------------------------------|
    |                           Sequence Number (optional)                         |
    |------------------------------------------------------------------------------|
C, K, and S      : 分别对应Checksum、Key和Sequence Number字段，置1表示存在相应的字段，否则无此字段。
Ver                   : GRE版本号(为0)，对于PPTP GRE，此字段为1。
Protocol Type  : 封装的以太网协议类型(例如IPv4，此处为0x0800)
Checksum       : 如果C位为1，此字段包含由GRE头部开始的所有数据的校验和
Key                  : 如果K位为1，此字段包含秘钥信息
Sequence Number: 如果S位为1，此字段包含GRE数据包的序号

### 内核实现

相关代码目录:

* net/ipv4/gre.c: 注册GRE协议, 用于协议栈接收GRE协议包
* net/ipv4/ip_gre.c: CISCO类型gre协议包处理模块
* drivers/net/ppp/pptp.c (在linux4.9.170版本中): PPTP类型gre协议包处理模块

##### 接收数据包调用过程:

这里先分析CISCO类型收包处理

```shell
net/ipv4/ip_input.c::ip_local_deliver_finish()
  ipprot->handler() => net/ipv4/gre.c::gre_rcv()
    proto->handler() => net/ipv4/ip_gre.c::ipgre_rcv()
      ipgre_tunnel_lookup()
      include/net/dst.h::__skb_tunnel_rx()	#解封装
```

gre模块收包过程, 主要就是(1)按照数据包源地址目标地址等作为键, 找到注册了的ip_tunnel结构, 利用该结构解封装; (2)将解封后的数据包重新调用链路层接受函数输入协议栈

大致看, 收包过程不是必须要先注册绑定虚拟网卡的.

##### 发送数据包调用过程:

(1)注册`modprobe ip_gre`

```shell
net/ipv4/ip_gre.c::ipgre_init()
  register_pernet_device(&ipgre_net_ops)
    net/ipv4/ip_gre.c::ipgre_init_net()
  rtnl_link_unregister(&ipgre_link_ops)
```

之后, 会出现gre0设备.

(2)通过netlink建立连接`ip tunnel add gre1 mode gre remote 10.10.10.9 local 10.10.10.11`

```shell
# 初始化gre1设备
net/core/rtnetlink.c::rtnl_newlink()
  ops = rtnl_link_ops_get(kind)
  rtnl_create_link()
    net/core/dev.c::alloc_netdev_mqs()
      net/ipv4/ip_gre.c::ipgre_tunnel_setup()
  ops->newlink() => net/ipv4/ip_gre.c::ipgre_newlink()
    ipgre_netlink_parms()
    ipgre_tunnel_find()
    ipgre_tunnel_link()
```

(3)发送数据包.

```shell
net/ipv4/ip_gre.c::ipgre_tunnel_xmit()
  include/net/route.h::ip_route_output_gre()
  # 为gre头和ip头预留出空间
  skb_push(skb, gre_hlen);
  # 1. 外部ip头部字段填充
  # 2. gre头各字段填充
  # 使用标准ip层发包流程发送封装好的ip包
  net/ipv4/ip_output.c::ip_local_out()
```

这里需要关注的问题是: gre隧道如何知道待封装数据包的外部目标ip? 代码中把这个外部目标ip用变量dst表示, 他其实来自于gre虚拟网卡设备的private data. 结合上面(1)和(2)对于虚拟网卡的注册, 那时就已经绑定了这个dst. 

##### 总结: 如何实现服务端gre网卡一对多?

综合以上, 接收gre包其实并不对多个gre网卡有要求, 主要是发送gre包, 需要从路由所确定的gre虚拟网卡上提取隧道绑定的目标ip. 我们要自己设计一对多gre网卡驱动, **要改的就是发送gre包时, 可以通过待封数据包目标ip, 找到外部目标ip所对应的ip_tunnel结构**; 而不是必须通过虚拟网卡设备的private data来获取这些信息.



### PPTP类型gre协议处理

首先, PPTP协议由两个部分组成:
+ **PPTP链路控制**：负责创建、维护、终止PPTP连接, 使用TCP协议
+ **PPTP数据传输**：负责传送PPP数据包，PPP数据包封装在GRE包里进行传送

链接控制阶段的任务, 主要就是进行身份认真, 下发内部ip并构建隧道. 后续的所有数据报文都直接由数据传输部分负责处理. linux3.x中将pptp数据传输处理部分(gre组包解包过程)作为内核模块, 则在通过隧道转发数据包时, 可以认为其转发包的效率与CISCO-GRE隧道是相同的. 这里分析PPTP隧道收发包过程. 它的特点是需要先在用户态创建AF_PPPOX类型套接口, 用于实现客户端和服务端的绑定, **之后的解包组包就全由内核态来处理, 所以AF_PPPOX类型套接口只有bind()和connect()两种合法操作**

ref: https://blog.csdn.net/eydwyz/article/details/54879787



##### 接收数据包调用过程:

这里分析PPTP-GRE收包处理, 首先, 需要创建一个AF_PPPOX的套接口，以便接收PPTP-GRE报文，同时需要绑定本地的call id和地址，用于区分不同的pptp连接, 另外此套接口还需要连接一个对端通信的AF_PPPOX套接口地址，以及作为一个通道channel注册自身到内核的PPP系统中

```shell
# 创建一个AF_PPPOX的套接口. 注意, 此处指定了收包函数为pptp_rcv_core()
drivers/net/ppp/pptp.c::pptp_create()

# 绑定本地的call id和地址
drivers/net/ppp/pptp.c::pptp_bind()

# 连接对端通信的AF_PPPOX套接口地址
drivers/net/ppp/pptp.c::pptp_connect()
```

注册完通道后, 才能做收包和发包处理, 下面是收包调用流程:

```shell
net/ipv4/ip_input.c::ip_local_deliver_finish()
  ipprot->handler() => drivers/net/ppp/pptp.c::pptp_rcv()
    proto->handler() => net/ipv4/ip_gre.c::ipgre_rcv()
      lookup_chan()
      #将数据包接受到对应套接口的接收队列
      net/core/sock.c::sk_receive_skb()	
        include/net/sock.h::sk_backlog_rcv()
          drivers/net/ppp/pptp.c::pptp_rcv_core()
            #剥离pptp-gre包头
            skb_pull(skb, headersize)
            #输入ppp协议层进行PPP包接收处理
            drivers/net/ppp/ppp_generic.c::ppp_input()
              ppp_do_recv()
                ...
                netif_rx()
```

lookup_chan()中会查找是否有AF_PPPOX套接口在监听此数据包中指定的call_id，判断数据包的源地址是否与套接口中指定目的地址相同，此两个条件已经在套接口初始化中分别使用bind和connect系统调用完成.

找到套接口之后, 就调用套接口的sk->sk_backlog_rcv()函数, 在创建此套接口时pptp_create()已经绑定为pptp_rcv_core(). 接收函数剥离pptp-gre包头, 此时就是一个内部的PPP协议包, 调用PPP协议层处理接受函数进行处理即可.

##### 发送数据包调用过程:

PPTP-GRE通道提供给内核**PPP系统**发送和ioctl控制的函数接口，注册完成之后ppp报文即可由此通道发送。注册过程在pptp_connect()中:

```shell
drivers/net/ppp/pptp.c::pptp_connect()
  ...
  po->chan.ops = &pptp_chan_ops;
  ...
  drivers/net/ppp/ppp_generic.c::ppp_register_channel()
    ppp_register_net_channel()
      ...
      pch->chan = chan;
```

ppp报文发送到隧道的过程:

```shell
# 注册虚拟网卡时, 绑定了dev->netdev_ops = &ppp_netdev_ops
drivers/net/ppp/ppp_generic.c::ppp_start_xmit()
  ppp_xmit_process()
    ppp_push()
      pch->chan->ops->start_xmit()
      => drivers/net/ppp/pptp.c::pptp_xmit()
        # 1. 外部ip头部字段填充
        # 2. gre头各字段填充
        # 使用标准ip层发包流程发送封装好的ip包
        net/ipv4/ip_output.c::ip_local_out()
```

ref: https://blog.csdn.net/sinat_20184565/article/details/83989539





### 基于GRE的VPN的NAT穿透问题, 该何去何从?

以上基于GRE协议的VPN实现, 虽然数据包处理效率很高, 但却存在**无法穿越NAT设备**问题. 

对于tcp和udp协议而言, 由于其中含有端口, 所以一般路由器有标准的按端口映射的方式使数据包成功穿越NAT. 但诸如**pptp协议，l2tp协议，ftp 协议，ipsec 协议等，根据其各自协议特殊原因，均无法顺利通过nat**. 所以市面上出现了一些路由器, 开启了对ipsec隧道与pptp隧道协议的支持, 即利用协议相关的一些字段唯一标识NAT内部的客户端. 不过这种需要特殊设备支持的方法显然不是一个好方法.

那么怎么办, 我们只能寻求ip over tcp或ip over udp的隧道代理方案了. 对比下openvpn, **它的隧道是ip over ssl的, 不过它最终还是将ssl数据包下层还是使用udp来传输**. 我们可以基于openvpn的已有模式, 自定义出自己的ip over tcp或ip over udp的隧道代理方案:

1. 客户端使用tuntap设备, 将应用程序对游戏服发包通过路由导向tuntap设备. 这样我们客户端就可以收到游戏数据包, 进行ip over tcp或ip over udp隧道封装. 这点模仿openvpn来做就行
2. 由于封装好的包是udp或tcp包, 所以穿透NAT或防火墙不成问题
3. VPN网关节点收到隧道包, 需要解包到达传输层, 此处可以利用类似PPTP协议的经验, 做成内核模块, 来处理数据包的解封与重新注入协议栈, 这样转发包的效率就会得到提升. 







### 关于p2p技术

按我的理解, p2p技术就是利用udp和tcp对于NAT穿透的技术, 通过第三方服务器协调, 实现双方经过NAT情况下直接互联互通. 

附注: 关于tcp如何实现普通意义上的p2p, 其实需要利用SO_REUSEADDR, ref: https://www.cnblogs.com/mq0036/p/6589955.html