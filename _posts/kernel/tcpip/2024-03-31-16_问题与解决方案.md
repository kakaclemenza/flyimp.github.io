---
layout: post
title: 16_问题与解决方案
category: kernel
tag: tcpip
typora-root-url: ../../..
---







### 2. tcp校验和





### 3. imp为啥会收到无关探测包

描述: 启动imp协议栈时, 由于新加了tap0接口, 会收到一堆探测包. hexdump也发现不了问题. 数据包形如:

```
[DEBUG veth_recv:214] veth_recv dump skb->data:
0x000000: 45 00 00 af bb 0a 40 00 ff 11 d5 32 0a 00 00 05 E.....@....2....
0x000010: e0 00 00 fb 14 e9 14 e9 00 9b 78 b5 00 00 00 00 ..........x.....
0x000020: 00 02 00 00 00 02 00 00 01 33 01 63 01 65 01 33 .........3.c.e.3
0x000030: 01 37 01 35 01 65 01 66 01 66 01 66 01 33 01 39 .7.5.e.f.f.f.3.9
0x000040: 01 63 01 32 01 30 01 38 01 30 01 30 01 30 01 30 .c.2.0.8.0.0.0.0
0x000050: 01 30 01 30 01 30 01 30 01 30 01 30 01 30 01 30 .0.0.0.0.0.0.0.0
0x000060: 01 30 01 38 01 65 01 66 03 69 70 36 04 61 72 70 .0.8.e.f.ip6.arp
0x000070: 61 00 00 ff 00 01 03 66 6c 79 05 6c 6f 63 61 6c a......fly.local
0x000080: 00 00 ff 00 01 c0 5a 00 1c 00 01 00 00 00 78 00 ......Z.......x.
0x000090: 10 fe 80 00 00 00 00 00 00 80 2c 93 ff fe 57 3e ..........,...W>
0x0000a0: c3 c0 0c 00 0c 00 01 00 00 00 78 00 02 c0 5a 00 ..........x...Z.
0x0000b0: 00 00 00 00 00 00 00 00 00 00 00 00 00          .............   
[DEBUG netif_receive_skb:176] netif_receive_skb proto:8
```

解决: 按关键字来查找也没查到. 于是利用wireshark分析tcpdump抓到的数据包, 从包类型得知属于mdns数据包

```shell
sudo ./imp 1>/dev/null &
sudo tcpdump -XX -ni tap0 -w tt.pcap
```

网上查到为avahi-daemon服务开启导致, 该服务是 zeroconf 协议的实现. 它可以在没有 DNS 服务的局域网里发现基于 zeroconf 协议的设备和服务. 它跟 mDNS 一样, 除非你有兼容的设备或使用 zeroconf 协议的服务, 否则应该关闭它

> **Zeroconf**
> Zero configuration networking(zeroconf)零配置网络服务规范，是一种用于自动生成可用IP地址的网络技术，不需要额外的手动配置和专属的配置服务器。
>
> “零 配置网络服务”的目标，是让非专业用户也能便捷的连接各种网络设备，例如计算机，打印机等。整个搭建网络的过程都是通过程式自动化实现。如果没有 zeroconf，用户必须手动配置一些服务，例如DHCP、DNS，计算机网络的其他设置等。这些对非技术用户和新用户们来说是很难的事情。
>
> Zeroconf规范的提出者是Apple公司.
>
> **Avahi**
> Avahi 是Zeroconf规范的开源实现，常见使用在Linux上。包含了一整套多播DNS(multicastDNS)/DNS-SD网络服务的实现。它使用 的发布授权是LGPL。Zeroconf规范的另一个实现是Apple公司的Bonjour程式。Avahi和Bonjour相互兼容(废话，都走同一个 规范标准嘛，就象IE，Firefox，chrome都能跑HTTP1.1一样)。
>
> Avahi允许程序在不需要进行手动网络配置的情况 下，在一个本地网络中发布和获知各种服务和主机。例如，当某用户把他的计算机接入到某个局域网时，如果他的机器运行有Avahi服务，则Avahi程式自 动广播，从而发现网络中可用的打印机、共享文件和可相互聊天的其他用户。这有点象他正在接收局域网中的各种网络广告一样。
>
> Linux下系统实际启动的进程名，是avahi-daemon
>
> 除非你有兼容的设备或使用 zeroconf 协议的服务，否则应该关闭它

关闭使用

```shell
sudo systemctl stop avahi-daemon.service
sudo systemctl stop avahi-daemon.socket
sudo systemctl disable avahi-daemon.socket
sudo systemctl disable avahi-daemon.service
```



### 4. 目标地址是本机ip的数据包处理流程

(1)首先, 无论目标ip是像127.0.0.1这样配在lo网卡上的, 还是项10.32.16.30这样配在eth0上的ip, 只要是配置到了本机的某个网卡上, linux内部都会统一将数据包按LOCAL包处理.

关键的处理位置在ip_queue_xmit()处调用ip_route_output_ports()执行路由判断. 此处判断数据包为RTN_LOCAL, 就会设置路由

```c
rth->dst.output = ip_output
rth->dst.input = ip_local_deliverxiaofeng
```

(2)数据包会经过OUTPUT, POSTROUTING钩子, 依次执行钩子上注册的函数对数据包进行处理.

(3)处理完毕后, 数据包会正常发到loopback网卡的驱动程序. 

(4) loopback驱动程序的发包操作就是把数据包重新递交给链路层

(5)链路层判断该数据包为ip包, 递交给网络层处理函数ip_rcv(), 这里就有对PREROUTING的钩子函数处理逻辑.

```c
int ip_rcv(...) {
    ...
	return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL, ip_rcv_finish);
}
```

(6)我们着重看看PREROUTING钩子上挂载的处理函数, 优先级数值越小, 级别越高, 目前主要有:

```shell
# 文件在linux-3.2.63/include/linux/netfilter_ipv4.h
enum nf_ip_hook_priorities {
	NF_IP_PRI_FIRST = INT_MIN,
	NF_IP_PRI_CONNTRACK_DEFRAG = -400,
	NF_IP_PRI_RAW = -300,
	NF_IP_PRI_SELINUX_FIRST = -225,
	NF_IP_PRI_CONNTRACK = -200,
	NF_IP_PRI_MANGLE = -150,
	NF_IP_PRI_NAT_DST = -100,
	NF_IP_PRI_FILTER = 0,
	NF_IP_PRI_SECURITY = 50,
	NF_IP_PRI_NAT_SRC = 100,
	NF_IP_PRI_SELINUX_LAST = 225,
	NF_IP_PRI_CONNTRACK_CONFIRM = INT_MAX,
	NF_IP_PRI_LAST = INT_MAX,
};
```

通过上面这个枚举可以知道, PREROUTING链上钩子函数的执行顺序为: raw->mangle->nat. (NF_IP_PRI_CONNTRACK也是有意义的, 它负责关联或创建ct). 这个顺序是与netfilter官方给的流程对应的:

![](/img/kernel/Netfilter-packet-flow.svg)

至于每个优先级对应的处理函数, 大家在内核代码中搜索对应关键字就能较容易找到. 比如我想找mangle表在PREROUTING钩子上的处理函数, 则搜索:

```shell
# 到内核目录下
> cd linux-3.2.63/
# 递归搜索
> grep -rniI NF_IP_PRI_MANGLE
include/linux/netfilter_ipv4.h:63:	NF_IP_PRI_MANGLE = -150,
net/ipv4/netfilter/iptable_mangle.c:36:	.priority	= NF_IP_PRI_MANGLE,
```

我们看到net/ipv4/netfilter/iptable_mangle.c:36, 注册的处理函数为: iptable_mangle_hook()

(7) 我们关注数据包到达nat表处理函数时的处理情况: 

a. 此前NF_IP_PRI_CONNTRACK对应的处理函数, 已经将数据包关联的ct状态标记为IP_CT_ESTABLISHED_REPLY. 

b. NF_IP_PRI_NAT_DST对应的处理函数如下. 可以知道我们直接执行了nf_nat_packet()函数, 而没有执行nf_nat_rule_find(), 所以不回去查iptables配置规则!

```c
static unsigned int nf_nat_fn(...) {
    ...
    switch (ctinfo) {
	case IP_CT_RELATED:
	case IP_CT_RELATED_REPLY:
		...
	case IP_CT_NEW:
		if (!nf_nat_initialized(ct, maniptype)) {
			unsigned int ret;

			ret = nf_nat_rule_find(skb, hooknum, in, out, ct);
			if (ret != NF_ACCEPT)
				return ret;
		/* Seen it before?  This can happen for loopback, retrans,
		   or local packets.. */
		} else
			pr_debug("Already setup manip %s for ct %p\n",
				 maniptype == IP_NAT_MANIP_SRC ? "SRC" : "DST",
				 ct);
		break;
	default:
		/* ESTABLISHED */
		NF_CT_ASSERT(ctinfo == IP_CT_ESTABLISHED ||
			     ctinfo == IP_CT_ESTABLISHED_REPLY);
	}

	return nf_nat_packet(ct, ctinfo, hooknum, skb);
}
```

(8)在ip_rcv_finish()中, 会判断到skb已经关联了路由项, 就直接走INPUT钩子. 不做路由判断, 就不可能走FORWARD. 如下:

```c
static int ip_rcv_finish(struct sk_buff *skb)
{
	const struct iphdr *iph = ip_hdr(skb);
	struct rtable *rt;

	//如果是向loopback设备发送的skb, 会被虚拟网卡调用netif_rx(skb)送回IP协议栈, skb_dst(skb)
	// 是存在的, 所以这里无需再获取输入路由. 
	// 又因为此时的dst_input在输出路由缓存项中构建为ip_local_deliver, 所以后面调用的就是
	// ip_local_deliver()
	if (skb_dst(skb) == NULL) {
		int err = ip_route_input_noref(skb, iph->daddr, iph->saddr,
					       iph->tos, skb->dev);
		if (unlikely(err)) {
			if (err == -EHOSTUNREACH)
				IP_INC_STATS_BH(dev_net(skb->dev),
						IPSTATS_MIB_INADDRERRORS);
			else if (err == -ENETUNREACH)
				IP_INC_STATS_BH(dev_net(skb->dev),
						IPSTATS_MIB_INNOROUTES);
			else if (err == -EXDEV)
				NET_INC_STATS_BH(dev_net(skb->dev),
						 LINUX_MIB_IPRPFILTER);
			goto drop;
		}
	}
	...
}
```

**其他**

（2023-02-12）看到dog250也有讨论过发往本机数据包的处理流程，其中涉及内核4.x之后引入`accept_local`之后带来的变化，参考：[关于Linux内核引入的accept_local参数的一个问题](https://blog.csdn.net/dog250/article/details/78746198)

> 为什么ping不通呢？
>
>   问题在于，数据包虽然可以从veth0发出最终通过其peer网卡veth1接收，然而当执行流经由veth1接收逻辑到达ip_route_input_slow的时候，以目标地址1.1.1.1查询路由表的结果显然是一个LOCAL路由，而Linux的IP路由实现中有个限制，即任何从非loopback网卡进来的任何数据包的源地址不能是本机地址，然而以源地址1.1.1.2来查询路由表，结果却也是一条LOCAL路由，显然1.1.1.2是一个本机地址，这违背了上面的约束，因此数据包被丢弃。
>
>   具体在代码上，这是由ip_route_input_slow函数里的fib_validate_source调用来判断的：
>
> ```
> int fib_validate_source(...)
> {
>     ...
>     if (fib_lookup(net, &fl, &res)) // 查询失败是允许的，只要rp_filter未启用
>         goto last_resort;
>     if (res.type != RTN_UNICAST) // 然而更严格的约束是，以源地址查询路由必须是UNICAST而不能是LOCAL路由
>         goto e_inval_res;
>     ...
> }
> ```
>
> 这意味着只要有上述的代码逻辑存在，这个就是避不开的。且慢，这里还有一个疑问，为什么题目中要有把loopback网口down掉这个关键的约束呢？答案在于，如果开启loopback，那么内核在路由到达1.1.1.1的数据包的时候，会直接将其路由到loopback接口，直接将本地环回路由绑定到数据包，从而在loopback的接收路径上会绕开ip_route_input_slow函数。
>
>   这样就不会进入fib_validate_source判断逻辑。
>
>   也就是说，在上述情况下，本文中我的这个问题的答案就可以毫不犹豫地回答：做不到！

### 5. ip_forward参数作用原理

设置流程：

![这里写图片描述](/assets/20161201215703931)

生效位置：在`ip_route_input_slow()`中如下位置：

```shell
if (!IN_DEV_FORWARD(in_dev))
        goto e_hostunreach;
```



参考：https://blog.csdn.net/viewsky11/article/details/53427293