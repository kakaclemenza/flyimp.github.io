---
layout: post
title: 三.scull并发
category: kernel
typora-root-url: ../../..
---

\<LDD3\>::第五章

### 并发处理原则

引起并发竞争的原因主要有如下几种:

* a
* b
* c
* d





定义声明:

本系列中, 以"进程上下文"代表内核中获得CPU而执行的代码片段.

内核中的interruptiable和uninterruptiable针对的是用户层是否可以发信号(如: ctrl+c)打断当前进程的执行. 而不是针对底层中断的.

### 信号量和互斥体

休眠(阻塞)就是当一个进程上下文执行到某个时间点不能进行任何处理时, **让出处理器**的一种状态.

对于我们的scull来说, 需要一种锁机制保护读写, **并且**能够在获得锁的进程上下文中**依然允许休眠**. 所以我们选择使用信号量(Semaphore).

信号量的值初始化为1时, 就成为了互斥体(Mutex), 它保证了任何给定是个只能被单个进程上下文拥有. 当然3.2.x内核中也有专门的struct metux结构. **linux内核中几乎所有信号量均用于互斥**

scull的设计中, 每一个设备拥有属于自己的信号量, 因为不同设备间不共享资源, 所以不会有全局的信号量. scull的信号量是在运行时动态分配的, 所以使用`init_MUTEX()`进行了初始化.

内核的很多结构都有提供这种初始化模式, 假设以X结构为例:

```C
DECLEARE_X(name);	// 使用宏进行声明并静态初始化(栈空间)
init_X(&name); 		// 对动态分配的结构初始化
```



**嵌套互斥体**(mutex_lock_nested)

TODO



### 读写信号量(rwsem)

是一种读写锁, 所有读者进入和退出临界区也需要获得和释放锁, 但多个读者允许同时进入临界区, 如果有写者进入了临界区, **则所有读者都无法进入临界区**

另一点比较重要, 写入者拥有更高的优先级. 即当多个写者试图进入临界区, 它们会等待已在临界区所有读者完成, 然后所有写者依次优先进入临界区, 直到所有写者完成任务, 读者才允许继续读取. 大量写者的情况下, 读者会饿死



### complete机制

信号量机制不适用于**等待某任务完成后继续执行**的场景. 在保证互斥的代码中, 信号量几乎总是可用, 但是在等待任务完成的同步场景中, 信号量则总是需要等待, 性能会受到影响. 

这种情况应该是使用内核提供的complete机制, 不必每次获得调度继续检查等待任务是否完成, 而直接由该任务"告知"工作已完成. 这是一种轻量级的机制.

scull_step3被改成了complete机制, 运行起来有点怪怪的, 但是能反映其工作原理. 

具体使用过程中要注意下面的问题:

* 多写入和多读取等待情况下, 使用complete()通知会通知随机一个. (在我的3.2内核中似乎是顺序通知). 使用complete_all()则需要重新初始化m_completion变量
* 等待是uninterruptiable的, 在我的4.9内核中, scull_step3如果用户层尝试使用ctrl+c去打断`cat /dev/scull0`, 将没有任何反应.
* 常用于模块退出时在exit()函数中使用`complete_and_exit(&name)`, 通知处于死循环中的内核线程退出

=> wait_for_complete()等待太久的话, 会触发内核告警的. 并且无法卸载, rmmod会自动检测模块的状态, 在调用模块相关函数处理期间无法卸载.(THIS_MODULE参数的作用, 见<内核模块开发基础知识>)

### Spinlock

自旋锁的使用应该特别注意, 使用场景可以结合其原理分析出来, 不确定的, 一定要参照\<ldd3\>::第五章进行详细分析. 

spin_lock_irq() 和 spin_lock_irqsave() 用于可能在硬件中断上下文被获得的自旋锁, 避免中断到来导致死锁.

spin_lock_bh() 则用于可能在软中断(tasklet)中被获得的自旋锁

**rwlock**: spinlock版的读写锁, 效果与rwsem相同, 但保持spinlock的限制.

关于spinlock的实现细节可参考: https://blog.csdn.net/u012603457/article/details/52895537

nymph版spinlock借助于gcc提供的`__sync_bool_compare_and_swap()`, 源码参见gcc的atomic.h文件, 内部也是利用了LOCK_PREFIX cmpxchg保证了原子性. 参见
https://wiki.osdev.org/Synchronization_Primitives
https://wiki.osdev.org/Spinlock



### RCU





### 深入话题

#### 中断嵌套, 中断丢失

首先明确没有"中断重入"的说法. 中断处理程序其实是一段执行代码片段, 下面称之为"中断通道", 某个中断通道不会被重复执行, 只可能会在打开中断后被其他中断通道抢占.

函数重入的意思就是，在任何情况下，都可以跳进去执行这个函数，也不会对原先的程序逻辑造成损害。比如多线程，同时执行这个东西不会影响其他线程等等。
这就要求它执行的时候不能更改全局变量、静态变量等等，所有一切对外界有影响的东西他都不能改，因为一旦改了，就有可能影响其他线程或中断调用那个变量。这样就“不可重入”了。这种函数对外界的唯一回馈，就是一个返回值

**中断的抢占规则 **
硬件中断可以抢占软件中断，硬件中断可以被更高优先级的硬件中断抢占，软件中断不能抢占别的中断。内核保证中断和异常执行的过程中不会发生新的异常，实际上内核态能触发的异常只有缺页异常，但是中断处理程序不执行会引发缺页异常的操作。 
**中断的嵌套**
中断的抢占是可以嵌套的，导致内核控制路径的嵌套执行。中断执行过程中不允许睡眠和进程切换（一般情况下睡眠都会导致进程切换），而且对中断的抢占只能发生在中断服务程序执行期间，只有这期间中断服务才是开启的。同时，同一中断通道不允许嵌套执行，即同一中断向量上的中断服务程序至多只能有一个出现在所有CPU的中断控制路径中。 
**软中断，tasklet和工作队列**
软中断不同于软件中断，软中断发生的时机是从中断、调用或者异常返回用户空间之前，按照软中断在结构数组中定义的顺序依次执行。同一个软中断可以在不同CPU上并发执行。软中断执行过程中也不允许睡眠和进程切换。 
tasklet基于软中断实现，在软中断结构数组中占用两项。当软中断执行到这两项时就会跳转到tasklet函数入口处，依次执行队列中的tasklet函数。同一个tasklet不能在不同CPU上并发执行，但是不同tasklet可以在不同CPU上执行。tasklet始终运行在被初始提交的同一处理器上。 
工作队列实际上是使用内核线程来实现的，工作队列允许睡眠，可以当做普通的内核线程来看待，他有自己的上下文环境。

ref: https://blog.csdn.net/u012603457/article/details/52972044 

**中断丢失**



#### 内存屏障, 内存总线锁/缓存锁, 原子操作

* 内存屏障: 用于保证插入屏障前的指令一定比屏障后的指令早执行.

  linux中内存屏障源码随单核还是多核处理器的不同而不同, 这个其实取决于原子性问题:

  1.单核处理器下中断发生在指令之间，因此单指令操作都是原子的
  2.多核处理器下进行零次或一次对齐内存访问的汇编指令是原子的

  ref: https://www.jianshu.com/p/1d90fe6627ad

* 原子操作

  **首先处理器会自动保证基本的内存操作的原子性**。处理器保证从系统内存当中读取或者写入一个字节是原子的，意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。奔腾6和最新的处理器能自动保证单处理器对同一个缓存行里进行16/32/64位的操作是原子的，但是复杂的内存操作处理器不能自动保证其原子性，比如跨总线宽度，跨多个缓存行，跨页表的访问。但是**处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性**

* 内存总线锁/缓存锁(arch/x86/include/asm/atomic.h)

  linux中的内存总线锁/缓存锁是通过一条声明LOCK#前缀的指令. Intel的手册上对其的解释是：

  Causes the processor's LOCK# signal to be asserted during execution of the accompanying instruction (turns the instruction into an atomic instruction). In a multiprocessor environment, **the LOCK# signal insures that the processor has exclusive use of any shared memory while the signal is asserted**.

  即"lock; nop;"编译为汇编其实就是一条指令"lock nop"!!! 在当前的处理器版本中, 使用的其实是缓存锁, 各个核可以同时往自己独有的L1/L2缓存中写入对于同一内存地址的操作, 但最终只有一个核能在缓存仲裁协议中获胜, 成功回写内存, 其他核则失败.



  ref: 

  [L4中lock nop的用法含义](https://blog.csdn.net/zhangxinrun/article/details/5843393)

  [内存总线锁/缓存锁原理](https://zhuanlan.zhihu.com/p/24146167)

  [原子操作的实现原理](https://www.infoq.cn/article/atomic-operation)


