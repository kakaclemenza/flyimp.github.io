---
layout: post
title: 六.内存分配问题
category: kernel
typora-root-url: ../../..
---

### UMA, NUMA和COMA

SMP称为共享存储型多处理机, 就是多CPU(不是多核)物理上对于内存进行共享, 其共享内存的模型有三种: UMA(均匀存储器存取), NUMA(非均匀存储器存取)和COMA(只用高速缓存的存储器结构)

UMA是当前我这个机器使用的, 所有CPU通过共用同一条总线对内存进行访问, 带来相同的新能损耗(总线仲裁)

NUMA和COMA则是将多个存储器Node分别绑定到不同CPU上, 加速访问, COMA的存储器Node则是使用了高速缓存实现的.

### 内存区段

linux内核在linux/mmzone.h和mm/mm_init.c中将内存分成了主要的三段: ZONE_DMA(DMA段), ZONE_NORMAL(常规区段), ZONE_HIGHMEM(高端区段).		

### 内存寻址和映射

IO端口或IO内存是BIOS映射好的

### 内核空间内存分配

**kmalloc**

分配小内存块时适用, 基于slab分配器. 在mm/slab.c::kmem_cache_init()中先为各种大小内存块管理数组malloc_sizes进行初始化. 后续调用kmalloc时直接匹配到对应的cachep, 利用kmem_cache_alloc()从该cachep所指的高速缓存中分配内存块.

**mempool**

内存池, 基于slab分配器, 不推荐使用

**get_free_pages**

分配大内存块, 按页分配, 基于buddy分配器.



### Direct IO, Buffer IO与内存映射

Direct IO和Buffer IO相对应。 
Direct IO就是直接把IO请求提交给底层IO设备，不经过缓存处理。

Buffer IO就是IO请求经过缓存子系统处理，例如Linux 上的VFS Cache层；
写数据会先写入内存，写入内存后就会返回，不等数据刷到磁盘上。
读数据会从先尝试从内存中读取，如果内存中命中，就不需要从磁盘上读取了。
Buffer IO会带来读写性能的大幅度提升，这和在数据库前面加一个Memcached是一样的概念。
大多数场景下，Buffer IO都是最优选择。

以下情况下我们可能需要考虑Direct IO：

1. 对数据写的可靠性要求很高，必须确保数据落到磁盘上，业务逻辑才可以继续执行。
2. 特定场景下，系统自带缓存算法效率不高，应用层自己实现出更高的算法。

而内存映射是另外一个很大的概念，展开会有很多内容；仅看文件IO这部分，像通过Linux mmap这样的方式访问文件，实际上就是由应用分配了一段内存，这段内存直接充当了文件读写过程中的缓存，用户态可以直接访问，而不是像普通的read/write方式需要把用户态的buffer拷贝到内核分配的缓存上，其本质还是buffer IO，只是省去了用户态到内核态的拷贝开销。