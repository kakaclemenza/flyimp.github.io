---
layout: post
title: 一.scull初步
category: kernel
typora-root-url: ../../..
---

scull将一块内存区域作为一个字符设备. 初步实现的scull为 scull0~scull3

所有的内容都合代码一一对应, 可以直接参考代码实现.

### 内核模块模型

只要insmod调用的init函数返回**<0**的值, 则模块的插入是不成功的, 调用rmmod只会提示模块未插入.

所以init函数中所有返回<0的值的情况, 都应该将先前申请的内存, 所作的注册等进行复原!!!



### 主设备号和次设备号

`ls -l /dev`可以看到/dev目录下的设备文件信息. 现在linux的原则:

* 一个主设备号对应一个驱动程序
* 一个次设备号锁定该驱动程序支持的一个设备

动态绑定的主设备号的读取: `cat /proc/devices`; 获取到之后就可以用这个主设备号来创建对应的设备文件了.



### 调试

qemu+gdb调试

首先编译内核, 可以将不必要的driver模块去掉, 则内核编译会快很多. 参考<u>d0u9/Linux-Device-Driver.git::00_preface/01_development_with_qemu.md</u>

后续在宿主机连接qemu虚机进行调试时, 需要用到虚机加载scull.ko模块的各个ELF sections的内存地址, 以便设置断点. 则在加载模块脚本中可以一并打印出来, 如:

```bash
#!/bin/sh
#file: load_module.sh

if [ $# -lt 1 ]; then
	echo "usage: ./start_module.sh (module_name)"
	exit 1
fi

module=$1
device=${module}

insmod ./${module}.ko || exit 1
if [ $? -ne 0 ]; then
	echo "insmod failed, exit..."
	exit 1
fi

rm -f /dev/${device}[0-2]

major=$(awk -v device="$device" '$2==device {print $1}' /proc/devices)
mknod /dev/${device}0 c $major 0
mknod /dev/${device}1 c $major 1
mknod /dev/${device}2 c $major 2

text_section=$(cat /sys/module/${module}/sections/.text)
data_section=$(cat /sys/module/${module}/sections/.data)
bss_section=$(cat /sys/module/${module}/sections/.bss)

echo "generate gdb add-symbol:"
echo "add-symbol-file ${pwd}/${module}.ko ${text_section} -s .data ${data_section} -s .bss ${bss_section}"
```



### gdb调试中断点处执行c/n/s跳转错误

在scull.ko模块中的断点, 执行`n`后, 会直接跳转到`read_hpet`; 目前该问题没有找到答案, 应该是gdb的bug, 相关跟踪参考https://stackoverflow.com/questions/53478689/next-step-error-when-debugging-android-kernel



### devfs涉及的基本结构

* cdev: 内核内部使用这个结构表示字符设备. 在内核调用设备操作前必须注册(cdev_add)一个或多个该结构
* file, file_operations, inode: 文件系统相关的结构.

### scull中的读写



**scull_read()** 

在我的Debian9中, 每次vfs传给scull_read()的count其实是131072, 即0x20000. 而scull_read的实现则负责驱动层面如何实际进行"读"操作, 这里每次只会读取一个scull_block的最大存储大小, 即SCULL_BLOCK_SIZE

如果使用`cat /dev/scull0`则会一直读到文件尾结束, 而vfs上层根据读取的返回字节数是否为0判断是否读取到文件尾, 所以读取的次数会比实际dev->block_counter要多一次, 最后一次是读取返回0

> 另外也有一种情况, 假设cat的实现是每次只读取100bytes, 最终vfs传递过来的count每次还会是0x20000. 这是vfs的预读优化, 防止频繁系统调用. [@xiaofeng: 未验证, 原理未知]

关于vfs和各个实际文件系统的细节可以参考我的nymph系统: [文件系统]()

**scull_write()**

写就是读的逆操作了, 就是申请分配足够的scull_block存取写入的内容, 这里从略, 详见代码



本节中潜在的问题:

一次读操作后, 没读完, 如果下次获得CPU的是写操作, 会导致后续读取混乱/越界之类的. 所以应当在**一次信号量持有中完成对某设备的所有操作**, 后续我们会对此进行完善. 

