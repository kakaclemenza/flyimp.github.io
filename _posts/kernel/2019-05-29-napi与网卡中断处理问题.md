---
layout: post
title: napi与网卡中断处理问题
category: kernel
---

在大流量下，由于napi的缘故，网卡的中断被关闭了，此时那个第一次被中断的cpu正在poll这个网卡，因此所有的流量都会聚集到这个cpu上.

当CPU处理能力不足时, 就容易在top命令检查时看到单核上面的si指标偏高, 即软中断处理占用CPU比例高. 这里的解决方法参照[服务器维护经验总结](/system/2019/03/12/服务器维护经验总结)

### NAPI方式的处理

这里以linux3.2.78下e100网络设备驱动为例.

* 初始化, 注册poll函数过程

  ```
  drivers/net/ethernet/intel/e100.c::e100_init_module
    drivers/net/ethernet/intel/e100.c::e100_probe
      drivers/net/ethernet/intel/e100.c::netif_napi_add
  ```

  调用netif_napi_add()就是注册ep100_poll()为napi轮询处理函数, 并指定了一次数据包输入软中断中读取报文数(配额)为E100_NAPI_WEIGHT

* 打开网卡中断

  上面的e100_probe()在模块插入时调用, 同时也注册了ethtool操作结构体e100_netdev_ops. 当使用`ip l s ethx up`的时候, 实际就会调用到这里的`e100_netdev_ops.ndo_open`, 也就是e100_open():

  ```
  drivers/net/ethernet/intel/e100.c::e100_open
    drivers/net/ethernet/intel/e100.c::e100_up
      drivers/net/ethernet/intel/e100.c::e100_open
        include/linux/interrupt.h::request_irq
          kernel/irq/manage.c::request_threaded_irq
            kernel/irq/manage.c::__setup_irq
        include/linux/netdevice.h::netif_wake_queue
        include/linux/netdevice.h::napi_enable
        drivers/net/ethernet/intel/e100.c::e100_enable_irq
  ```

  这里e100_open主要做了: 

  (1)注册收包硬件中断处理函数为e100_intr()

  (2)开启接收唤醒队列.

  (3)开启NAPI轮询方式

  (4)e100_enable_irq()打开硬中断

* 读取报文: 报文到来时, 引发中断, 通过中断门最终找到中断处理函数e100_intr()

  ```
  drivers/net/ethernet/intel/e100.c::e100_intr
    include/linux/netdevice.h::napi_schedule_prep
    drivers/net/ethernet/intel/e100.c::e100_disable_irq
    net/core/dev.c::__napi_schedule
  ```

  napi_schedule_prep()判断系统是否正在轮询接收报文, 不是则屏蔽设备硬中断中断并加入轮询队列尾部: 

  `net/core/dev.c::__napi_schedule + ____napi_schedule`

  ```c
  void __napi_schedule(struct napi_struct *n)
  {
      unsigned long flags;
      /*链表操作必须在关闭本地中断的情况下操作，防止硬中断抢占*/
      local_irq_save(flags);               
      /*把网卡NIC的NAPI加入到本地CPU的softnet_data 的pool_list 链表上*/
      list_add_tail(&n->poll_list, 
      			&__get_cpu_var(softnet_data).poll_list);
      /*调度收包软中断*/
      __raise_softirq_irqoff(NET_RX_SOFTIRQ);           
      local_irq_restore(flags);
  }
  ```

  重点: 这里传入的napi_struct在非NAPI方式中直接传入的是softnet_data.backlog, 这个在后续主要是指定了进一步处理函数为process_backlog(), 是为了兼容非NAPI方式, 从而能把两种方式用相同的模式处理. 而在NAPI方式, 传入的就是网卡的nic.napi, 在netif_napi_add()中绑定了从网卡轮询获取skb的所有必要信息. 

  软中断NET_RX_SOFTIRQ在net/core/dev.c::net_dev_init()中调用open_softirq()进行登记了网络报文接收软中断处理函数为net_rx_action(). 所以这里触发调度程序调用到软中断处理程序do_softirq():

  ```
  kernel/softirq.c::do_softirq
    net/core/dev.c::net_rx_action
  ```

  net_rx_action取出绑定在当前CPU上的softnet_data结构, 该结构定义在net/core/dev.c中, 该结构参见<Linux内核源码剖析-tcpip实现:7.4>. 

  ```c
  DEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);
  EXPORT_PER_CPU_SYMBOL(softnet_data);
  ```

  这里依次读取每个网卡驱动在前面__napi_schedule中挂接到softnet_data.poll_list中的napi_struct结构, 该结构在网卡设备初始化就是由前面e100_probe()中调用的netif_napi_add()完成. 调用napi_struct结构的poll方法, 就是调用注册的e100_poll()函数

  ```
  drivers/net/ethernet/intel/e100.c::e100_poll
    drivers/net/ethernet/intel/e100.c::e100_rx_clean
      drivers/net/ethernet/intel/e100.c::e100_rx_indicate
        net/core/dev.c::netif_receive_skb
          net/core/dev.c::__netif_receive_skb
    drivers/net/ethernet/intel/e100.c::e100_tx_clean
  ```

  e100_rx_clean()中, 每次只会从网卡收包队列中读取E100_NAPI_WEIGHT指定数目的数据包, 并最终通过__netif_receive_skb()递交协议栈处理. 回到e100_poll()和net_rx_action()中会判断是否读足够多, 否则会将当前napi_strauct移到链表尾部等待下一次继续轮询.

  注意, 此处数据包skb是在从e100_rx_clean中网卡驱动收包队列nic->rx_to_clean中取出来的, 至于如何从网卡队列将包放到内核网卡驱动队列, 涉及到dma等技术后续再进一步深入.

  __netif_receive_skb()递交具体上层协议处理, 可参考<Linux内核源码剖析-tcpip实现:7.7.2>, 这里列一下IP包的函数转移过程:

  ```
  net/core/dev.c::__netif_receive_skb
    net/core/dev.c::deliver_skb
      net/ipv4/ip_input.c::ip_rcv
  ```

  ipv4的packet_type结构实例定义为net/ipv4/af_inet.c::ip_packet_type, 注册的包处理函数为ip_rcv()

