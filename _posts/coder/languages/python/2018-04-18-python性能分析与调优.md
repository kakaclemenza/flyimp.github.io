---
layout: post
title: python性能分析与调优
category: coder
tag: python
---

### 已经测试过的工具大致的作用:
* strace: 定位进程运行过程中 syscalls 相关信息
* Pympler和objgraph: Pympler用于实时分析内存对象, objgraph用于可视化打印对象的引用关系.
* meliae: 是对python层的内存对象dump到文件, 然后进行分析.

实际使用中, meliae已经足够.

### python中内存分配所经历的过程

1. C层ptmalloc分配策略

2. python中三层内存分配机制 (缓存池, 不交还给操作系统)

3. 对于int类型, float类型, 小字符串的intern机制 (源码 codeobject.c)

   all_name_chars() 保证默认只会对由字符"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz"构成字符串进行intern

### 不同版本python对于缓存对象的处理差异

python2.7.3只支持256字节以内的对象使用缓存池, 大于这个值的直接使用C层ptmalloc分配器. python2.7.14则支持大小扩大到512字节. 所以有如下现象

```python
import time

g_list = []

def Allocate():
	global g_list
	for i in xrange(1000000):
		t = "%d" % i
		t = t * 100
        # t = t * 500
		g_list.append(t)

if __name__ == '__main__':
	Allocate()
	print "alloc mem"
	time.sleep(5)
	del g_list
	g_list = []
	print "free mem"
	while True: 
		time.sleep(1)

# 在python2.7.14及以上, 这段代码del之后还是持续占用大量内存. 而如果将t*100该为t*500, 则del之后, 不再占用大内存.
# 在python2.7.3中, 则在两种情况下del之后, 都不会再占用大内存.
```



### python各类对象占用内存分析

> How do I determine the size of an object in Python?

The answer, "Just use `sys.getsizeof`", is not a complete answer.

That answer *does* work for builtin objects directly, but it does not account for what those objects may contain, specifically, what types, such as custom objects, tuples, lists, dicts, and sets contain. They can contain instances each other, as well as numbers, strings and other objects.

#### A More Complete Answer

Using 64-bit Python 3.6 from the Anaconda distribution, with `sys.getsizeof`, I have determined the minimum size of the following objects, and note that sets and dicts preallocate space so empty ones don't grow again until after a set amount (which may vary by implementation of the language):

Python 3:

```none
Empty
Bytes  type        scaling notes
28     int         +4 bytes about every 30 powers of 2
37     bytes       +1 byte per additional byte
49     str         +1-4 per additional character (depending on max width)
48     tuple       +8 per additional item
64     list        +8 for each additional
224    set         5th increases to 736; 21nd, 2272; 85th, 8416; 341, 32992
240    dict        6th increases to 368; 22nd, 1184; 43rd, 2280; 86th, 4704; 171st, 9320
136    func def    does not include default args and other attrs
1056   class def   no slots 
56     class inst  has a __dict__ attr, same scaling as dict above
888    class def   with slots
16     __slots__   seems to store in mutable tuple-like structure
                   first slot grows to 48, and so on.
```

How do you interpret this? Well say you have a set with 10 items in it. If each item is 100 bytes each, how big is the whole data structure? The set is 736 itself because it has sized up one time to 736 bytes. Then you add the size of the items, so that's 1736 bytes in total

Some caveats for function and class definitions:

Note each class definition has a proxy `__dict__` (48 bytes) structure for class attrs. Each slot has a descriptor (like a `property`) in the class definition.

Slotted instances start out with 48 bytes on their first element, and increase by 8 each additional. Only empty slotted objects have 16 bytes, and an instance with no data makes very little sense.

Also, each function definition has code objects, maybe docstrings, and other possible attributes, even a `__dict__`.

Also note that we use `sys.getsizeof()` because we care about the marginal space usage, which includes the garbage collection overhead for the object, [from the docs](https://docs.python.org/3/library/sys.html#sys.getsizeof):

> `getsizeof()` calls the object’s `__sizeof__` method and adds an additional garbage collector overhead if the object is managed by the garbage collector.

Also note that resizing lists (e.g. repetitively appending to them) causes them to preallocate space, similarly to sets and dicts. From the [listobj.c source code](https://github.com/python/cpython/blob/master/Objects/listobject.c):

```cxx
    /* This over-allocates proportional to the list size, making room
     * for additional growth.  The over-allocation is mild, but is
     * enough to give linear-time amortized behavior over a long
     * sequence of appends() in the presence of a poorly-performing
     * system realloc().
     * The growth pattern is:  0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ...
     * Note: new_allocated won't overflow because the largest possible value
     *       is PY_SSIZE_T_MAX * (9 / 8) + 6 which always fits in a size_t.
     */
    new_allocated = (size_t)newsize + (newsize >> 3) + (newsize < 9 ? 3 : 6);
```



#### Historical data

Python 2.7 analysis, confirmed with `guppy.hpy` and `sys.getsizeof`:

```none
Bytes  type        empty + scaling notes
24     int         NA
28     long        NA
37     str         + 1 byte per additional character
52     unicode     + 4 bytes per additional character
56     tuple       + 8 bytes per additional item
72     list        + 32 for first, 8 for each additional
232    set         sixth item increases to 744; 22nd, 2280; 86th, 8424
280    dict        sixth item increases to 1048; 22nd, 3352; 86th, 12568 *
120    func def    does not include default args and other attrs
64     class inst  has a __dict__ attr, same scaling as dict above
16     __slots__   class with slots has no dict, seems to store in 
                    mutable tuple-like structure.
904    class def   has a proxy __dict__ structure for class attrs
104    old class   makes sense, less stuff, has real dict though.
```

Note that dictionaries ([but not sets](https://mail.python.org/pipermail/python-dev/2016-September/146472.html)) got a [more compact representation](https://docs.python.org/3.6/whatsnew/3.6.html#other-language-changes) in Python 3.6

I think 8 bytes per additional item to reference makes a lot of sense on a 64 bit machine. Those 8 bytes point to the place in memory the contained item is at. The 4 bytes are fixed width for unicode in Python 2, if I recall correctly, but in Python 3, str becomes a unicode of width equal to the max width of the characters.

And for more on slots, [see this answer](https://stackoverflow.com/a/28059785/541136).

#### A More Complete Function

We want a function that searches the elements in lists, tuples, sets, dicts, `obj.__dict__`'s, and `obj.__slots__`, as well as other things we may not have yet thought of.

We want to rely on `gc.get_referents` to do this search because it works at the C level (making it very fast). The downside is that get_referents can return redundant members, so we need to ensure we don't double count.

Classes, modules, and functions are singletons - they exist one time in memory. We're not so interested in their size, as there's not much we can do about them - they're a part of the program. So we'll avoid counting them if they happen to be referenced.

We're going to use a blacklist of types so we don't include the entire program in our size count.

```py
import sys
from types import ModuleType, FunctionType
from gc import get_referents

# Custom objects know their class.
# Function objects seem to know way too much, including modules.
# Exclude modules as well.
BLACKLIST = type, ModuleType, FunctionType


def getsize(obj):
    """sum size of object & members."""
    if isinstance(obj, BLACKLIST):
        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))
    seen_ids = set()
    size = 0
    objects = [obj]
    while objects:
        need_referents = []
        for obj in objects:
            if not isinstance(obj, BLACKLIST) and id(obj) not in seen_ids:
                seen_ids.add(id(obj))
                size += sys.getsizeof(obj)
                need_referents.append(obj)
        objects = get_referents(*need_referents)
    return size
```

To contrast this with the following whitelisted function, most objects know how to traverse themselves for the purposes of garbage collection (which is approximately what we're looking for when we want to know how expensive in memory certain objects are. This functionality is used by `gc.get_referents`.) However, this measure is going to be much more expansive in scope than we intended if we are not careful.

For example, functions know quite a lot about the modules they are created in.

Another point of contrast is that strings that are keys in dictionaries are usually interned so they are not duplicated. Checking for `id(key)` will also allow us to avoid counting duplicates, which we do in the next section. The blacklist solution skips counting keys that are strings altogether.

#### Whitelisted Types, Recursive visitor (old implementation)

To cover most of these types myself, instead of relying on the `gc` module, I wrote this recursive function to try to estimate the size of most Python objects, including most builtins, types in the collections module, and custom types (slotted and otherwise).

This sort of function gives much more fine-grained control over the types we're going to count for memory usage, but has the danger of leaving types out:

```py
import sys
from numbers import Number
from collections import Set, Mapping, deque

try: # Python 2
    zero_depth_bases = (basestring, Number, xrange, bytearray)
    iteritems = 'iteritems'
except NameError: # Python 3
    zero_depth_bases = (str, bytes, Number, range, bytearray)
    iteritems = 'items'

def getsize(obj_0):
    """Recursively iterate to sum size of object & members."""
    _seen_ids = set()
    def inner(obj):
        obj_id = id(obj)
        if obj_id in _seen_ids:
            return 0
        _seen_ids.add(obj_id)
        size = sys.getsizeof(obj)
        if isinstance(obj, zero_depth_bases):
            pass # bypass remaining control flow and return
        elif isinstance(obj, (tuple, list, Set, deque)):
            size += sum(inner(i) for i in obj)
        elif isinstance(obj, Mapping) or hasattr(obj, iteritems):
            size += sum(inner(k) + inner(v) for k, v in getattr(obj, iteritems)())
        # Check for custom object instances - may subclass above too
        if hasattr(obj, '__dict__'):
            size += inner(vars(obj))
        if hasattr(obj, '__slots__'): # can have __slots__ with __dict__
            size += sum(inner(getattr(obj, s)) for s in obj.__slots__ if hasattr(obj, s))
        return size
    return inner(obj_0)
```

And I tested it rather casually (I should unittest it):

```sh
>>> getsize(['a', tuple('bcd'), Foo()])
344
>>> getsize(Foo())
16
>>> getsize(tuple('bcd'))
194
>>> getsize(['a', tuple('bcd'), Foo(), {'foo': 'bar', 'baz': 'bar'}])
752
>>> getsize({'foo': 'bar', 'baz': 'bar'})
400
>>> getsize({})
280
>>> getsize({'foo':'bar'})
360
>>> getsize('foo')
40
>>> class Bar():
...     def baz():
...         pass
>>> getsize(Bar())
352
>>> getsize(Bar().__dict__)
280
>>> sys.getsizeof(Bar())
72
>>> getsize(Bar.__dict__)
872
>>> sys.getsizeof(Bar.__dict__)
280
```

This implementation breaks down on class definitions and function definitions because we don't go after all of their attributes, but since they should only exist once in memory for the process, their size really doesn't matter too much.



### 使用meliae定位解决BS服高内存占用问题(2020-12-26)

首先, 排查python程序内存泄漏时, 要明确两点:

1. python层内部通过垃圾回收机制为主, 标记-清除和分代回收为辅这三种机制, 确保了最终python层不存在不可索引内存问题. 所以python层的内存泄漏就是全局对象占用内存未及时释放的问题
2. python程序依赖的一些底层C/C++编写的扩展库, 也会存在内存泄漏问题. 这类问题则是C/C++中经典的申请分配堆内存却最终没有了释放

meliae主要作用, 是对python层的内存对象dump到文件, 然后进行分析.

> A simple way to dump memory consumption of a running python program

使用方法也很简单, 示例如下:

1. 首先, 在线上python代码中加入如下代码, 热更后触发, 将内存dump到文件:

   ```shell
   import meliae.scanner
   meliae.scanner.dump_all_objects("memdump.txt")
   ```

   

2. 将线上memdump.txt文件下载到本地, 使用ipython分析, 如:

   ```shell
   >>> import meliae.loader
   >>> om = meliae.loader.load("memdump.txt")
   >>> print om.summarize()
   Total 700975 objects, 874 types, Total size = 276.9MiB (290327578 bytes)
    Index   Count   %      Size   % Cum     Max Kind
        0  183823  26 176767720  60  60  786712 dict
        1   65303   9  72616936  25  85    1112 InstanceState
        2  151981  21  10535744   3  89     520 tuple
   ```

   可以看到总内存在276.9MB.

   om.summarize() 打印的结果中, builtin类型占据的内存是能直接真实统计出来的, 而像dict之类的复合类型则只是打印该容器的大小, 并没有加入内部元素的大小. 像这种dict复合类型站内存大的, 一般就是在前10个元素容量最多的对象. 于是执行如下操作:

   ```shell
   >>> dct_sorted = om.get_all('dict')
   >>> for i in xrange(10):
   ...:    dct_sorted[i].compute_total_size()
   >>> dct_sorted[:10]
   
   # 假设dct_sorted[2]统计出来内存最高, 那么这个dict被引用情况在:
   >>> dct_sorted[2].p
   ```


#### 其他工具

1. guppy: 打印当前内存被各类对象的引用情况
2. pytracemalloc: 详细跟踪python层每次内存分配情况
3. line_profiler: 分析每行代码效率
4. memory_profiler: 分析每行内存变化
5. cProfile: line_profiler运行开销较高, 所以可以使用cProfile替代

#### 解决问题实例记录

我们分几个步骤逐步定位到问题所在:

- 首先确定当时程序在做什么, 是否有异常行为.
- 排除行为异常之后, 查看python的内存使用情况, 是否所有该回收的对象都回收了.
- 排除垃圾回收等python内部的内存泄漏问题后, 定位到是libc的malloc实现的问题.

而最后的解决方法也很简单, 直接替换malloc模块为tcmalloc:

```
LD_PRELOAD="/usr/lib64/libtcmalloc.so" python x.py
```

ref: http://drmingdrmer.github.io/tech/programming/2017/05/06/python-mem.html
