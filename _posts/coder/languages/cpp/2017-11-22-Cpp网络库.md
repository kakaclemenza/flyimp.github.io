---
layout: post
title: Cpp网络库
category: coder
typora-root-url: ../../../..
---

### 一般网络库结构
* 日志系统
* 测试系统
* 时间系统
* 事件循环

### 服务器架构

### poll
* 玩具代码 poll.cpp 
* 存在问题  
1. read 读取接收缓冲区, 一次可能无法读完, 下次connfd仍然活跃, 需要使用应用层缓冲区, 接受完整. 至于怎么处理, 交给应用层去解决
2. write 发送缓冲区满了, 那么数据不能再写入, 需要应用层缓冲区.   
POLLOUT事件触发条件: 发送缓冲区不满时. --> 如果持续关注该事件, 会导致busy-wait(忙等待)  
应用层缓冲区发送完毕时, 应该取消关注 POLLOUT 事件防止 busy-wait

* accept 返回 EMFILE 错误的处理
1. 调高进程文件描述符数目 (No)
2. 死等(仍是高电平, busy-loop)/退出程序 (Not 7\*24)
3. 关闭监听套接字, 那什么时候打开呢?
4. epoll模型的ET模式 -> 问题是如果漏掉了一次accept, 程序再也不会收到新链接(见下一节)
5. (优)预备空闲描述符, 打开这个文件描述符, 先用accept接收, 然后close()掉, 打开这个空闲描述符. 这样可以优雅的通知客户端, 服务器已经无法处理. 同时服务端也不至于busy-loop. 

* 在高可用编程中, 不因为错误退出程序的, 而是会一直容忍, 尽量continue


### epoll
* epoll LT模式
1. epoll_wait() 返回的都是活跃的套接字, 不需要像poll一样还要遍历一次
2. epoll_wait() 中每次监听的文件描述符有限, 所以传入的数组需要能动态调整大小, 比如当所剩容量不足一半时, 下次调用 epoll_wait() 前就进行扩容一倍.
3. epoll_create(int size) 目前size参数已被忽略, 也就是说系统能创建多少个套接字, epoll就能管理那么多.  
4. epoll_ctl() 默认是 LT 模式的
5. 与poll相比, poll每次都需要把监听套接字和已连接套接字拷贝到内核空间, epoll不需要从用户空间把所关心的数据拷贝到内核空间. 数据拷贝是服务器性能杀手.
6. EPOLLOUT 的处理, 和 poll 的 POLLOUT 处理相似

* epoll ET模式
EPOLLIN事件:  
	内核中socket接受缓冲区为空		低电平
	内核中socket接受缓冲区不为空	高电平
EPOLLOUT事件:
	内核中socket接受缓冲区为满		低电平
	内核中socket接受缓冲区不满		低电平

1. 同时关注套接字EPOLLOUT和EPOLLIN事件
2. EPOLLIN事件, 要read()直到出现EAGAIN错误, 否则持续高电平, 后续就不会触发
3. EPOLLOUT事件, 一定要 write() 直到出现 EAGAIN, 否则也会一直处于高电平
4. 存在问题: 如果accept 出现 EMFILE 问题, 激活的监听套接字没有接收走, 则该内核缓冲区就会持续高电平, 导致后面永远无法接收新连接.  
这个需要应用层再进行一些信号的监听, 处理起来更复杂.


### select/poll/epoll 对比
![06-01](/img/muduo/06-01.png)
![06-02](/img/muduo/06-02.png)
![06-03](/img/muduo/06-03.png)
![06-04](/img/muduo/06-04.png)

* epoll是否一定比poll/select好?
如果"已连接套接字不太多, 且这些套接字都非常活跃", poll/select的遍历开销不大, 而实现简单, 则性能开销会优于epoll.  
而处理大量连接时, epoll就会明显体现优势.

### muduo库简介
#### 概述
* 是作者的多年经验总结, 学会了这个库找工作应该没有问题
* 正规linux服务器的现代C++做法, 使用大量的Boost库

#### 特点
* 不做跨平台, 减少代码复杂性(不然读起来很折磨)
* non-blocking IO + one event-loop per thread + thread pool
* 不考虑公网, 不为安全性做特别性增强
* 基于对象编程思想, 只暴露具体类和标准库的类. API不使用 non-trival constructor, 也不使用虚函数.

#### 框架与库的区别
* library 需要用户自己编写胶水代码, 进行使用
* framwork 应用于特定领域, 会调用用户提供的一些回调函数, 实现控制反转

#### 例子: inspector_test
任务服务器将任务负载均衡的分配给应用服务器, 需要接口查询应用服务器的当前负载状态, 这个例子就是做这个功能用的.  
libmuduo_inspect.a -> libmuduo_http.a -> libmuduo_net.a -> libmuduo_base.a

使用 muduo 库:
```
g++ -I[头文件目录] -L[库文件目录] -l[库名]
```
动态库和静态库的编译格式都是这样的


### 面向对象和基于对象
#### 面向对象程序
1. Run() 函数为什么要做成 private. 因为它应该在线程中被调用, 而不应该作为公共函数在外部被调用, 否则就不是线程行为了
2. 那么如何调用这个 Run() 呢, 详见代码. pthread_create要求传入一个普通函数指针, 而不能是类内的函数指针(带this了), 所以使用了私有的 static 函数
3. 线程生存周期 和 线程对象生存周期 是不一样的
4. 那么这样如果我们有线程池, 我们可能希望线程运行完毕会自动销毁. 可以设置CThread类auto_delete\_进行判断(见代码)
5. 注意自动销毁这里创建对象时不能使用实体对象, 要使用指针, 否则会导致重复销毁问题.

#### 基于对象程序
1. 函数适配器 boost/bind 函数. 在 C++11 中已经成为标准

#### 比较
1. C编程风格: 注册三个全局函数到网络库, 网络库通过函数指针来回调
2. 面向对象风格: 用一个EchoServer继承自TcpServer(抽象类), 实现三个接口 OnConnection, OnMessage, OnClose
3. 基于对象风格: 用一个EchoServer包含一个TcpServer(具体类)对象, 在构造函数中用boost::bind来注册三个成员函数OnConnection, OnMessage, OnClose.

总结: 3没有使用虚函数, 效率会比2高, 并且相对简单化开发和使用

### 原子性操作
#### 原子操作
* 解决锁竞争的另一种高效方法. 现代CPU就会提供原子性操作.
* gcc提供的原子性操作方法:
```cpp
// 原子自增操作
type __sync_fetch_and_add(type* ptr, type value)

// 原子比较和交换(设置)操作
type __sync_val_compare_and_swap(type* ptr, type oldval, type newval)
bool __sync_val_compare_and_swap(type* ptr, type oldval, type newval)
// 代码解释一下:
// if (ptr == oldval) {
//   ptr = newval;
//   return true;
// else {
//   return false;
// }

// 原子赋值操作
type __sync_lock_test_and_set(type* ptr, type value)
```
* 编译是记得添加选项 --match=native (native 表示根据本地CPU自动获得CPU架构)

#### 无锁队列
参看网页解释. 
1. 如果有一个线程T1，它的while中的CAS如果成功的话，那么其它所有的 随后线程的CAS都会失败，然后就会再循环，
2. 此时，如果T1 线程还没有更新tail指针，其它的线程继续失败，因为tail->next不是NULL了。
3. 直到T1线程更新完tail指针，于是其它的线程中的某个线程就可以得到新的tail指针，继续往下走了。

问题: 如果该T1线程挂了, 那程序会一直卡在这里, 怎么办?

#### volitale关键字
使用 volatile 声明的变量, 系统每次用到该变量时, 总是会重新从它所在的内存中读取数据, 而不是是使用保存在寄存器中的备份.   
这种做法对于**多线程编程**十分重要, 保证每次使用该值时都是最新的, 避免读取到寄存器中的旧值

#### muduo库中的一些编译选项
!(编译选项)[/img/muduo/11_1.png]

#### muduo库中其他一些注意事项:
* \_\_sso\_string
* implicit_cast<>
* down_cast<>
* RTTI技术

### 异常处理
* backtrace函数
* backtrace_symbols() 返回一个指针数组, 这个指针数组是由malloc创建, 需要由调用者自行进行销毁
* c++ mangle技术. --> 使用`abi::__cxa_demangle`函数

### 线程封装
* BOOST_STATIC_ASSERT() --> 编译时断言
* `__thread`变量每一个线程有一份独立实体，各个线程的值互不干扰。可以用来修饰那些带有全局性且值可能变，但是又不值得用全局变量保护的变量。主要在用于线程内部缓存一些线程间相互独立的数据, 比如 tid.    
使用方法上, 类似于全局变量, 就是要加上`__thread`进行修饰
* `__thread`关键字只能由于POD类型, 非POD类型需要使用**线程特定数据**方式实现.(跟我一起学C++第一季)


##### pthread_atfork() 引起的系列问题
* fork()函数的调用, 可能在主线程, 也可能在子线程. 如果在子线程中进行调用, fork本质上是拷贝内存中的执行状态, 那么只会将当前子线程中的执行状态复制过来, 其他线程不会复制. 所以muduo中调用 pthread_atfork() 确保了子线程中调用的fork(), 会把fork出来的线程设置为新进程的主线程
* **安全规范**避免多线程+多进程程序. 因为这样非常容易造成死锁. (视频中有两个例子), 需要综合设置好 prepare, parent函数进行加减锁


### 14. 锁
#### 批注(2018-04-24):
* `int ret = xxx; (void)ret;` 这样的用法, 唯一的用处就是避免"unused variable warnings". 其实这完全是无意义的做法.

#### 互斥锁 MutexLock
* 加锁对效率影响的测试

#### MutexLockGuard
* 使用 RAII 技术(资源获取即初始化)进行封装
* MutexLockGuard 和 MutexLock 之间是关联关系

#### 条件变量
* 条件变量大致的使用过程
```
// 消费者
锁住 mutex
	while (!条件)
		解锁
		等待条件变量
		加锁
解锁 mutex

// 生产者
锁住 mutex
更改条件
解锁 mutex
signal或broadcast
```

#### 倒计时门闩量
* 使用 条件变量 进行实现
* 用于所有子线程等待主线程发起"起跑"命令
* 用于主线程等待子线程初始化完毕才开始工作
* mutable 修饰符. 使得 const 成员函数可以去改变被mutable修饰的变量, 但没有被修饰的就不能改动

### 15. 两个队列
* 条件变量的等待需要使用while循环等待, 防止"虚假唤醒"
* 本质上是生产者-消费者问题

### 16. ThreadPool
* 生产者线程向任务队列添加线程, 线程池中的线程函数从任务队列获取任务, 在线程队列取线程去执行任务
* 

### 17. Singleton
* pthread\_once() 这个函数能保证传进去给它的函数只被执行一次. 并且是线程安全的. 效率相比于自己加锁高
* 完全类型和不完全类型: 类A如果只进行了声明, 也是可以部分进行使用的, 此时类A就是不完全类型, sizeof(A) == 0
* TODO: 使用智能指针进行释放

### 18. 线程特定数据 TSD


### 19. ThreadLocalSingleton
* 利用 Singleton + TSD 也可以达到 ThreadLocalSingleton 效果


### 20. 日志系统
#### 日志的作用
* 日志可以将运行时的系统状态进行输出, 可以帮助更快的调试错误, 更好的理解程序. **记录系统的运行状态**
* 更多的使用日志进行调试, 通过分析日志能理清当前程序运行状态, 从而比较快的找出错误. 而不是gdb. 因为一些逻辑错误的处理使用gdb犹如大海捞针.

#### 日志级别
!()[/img/muduo/20-1.png]
* muduo默认日志级别是 INFO, 会去找环境变量, 没有设置则默认使用INFO级别. ==> 这个可以作为配置文件选项了
* 



### 多线程与并发服务器设计
#### 循环式/迭代式(iterative)服务器
* 阻塞
* 不能利用多核

#### concurrent服务器


#### prefork服务器
* 预先创建线程/进程. 即线程池. 省去请求到来时的创建开销
* 问题: 惊群现象 -> unp2e Ch27

#### 反应式(reactive)服务器(reactor模式)
* select/poll/epoll, 并发处理多个请求, 实际上是在一个线程中完成的
* 无法充分利用CPU
* 效率比concurrent高, 原因在于能监听比concurrent更多的文件描述符
* 请求执行时间不能过长. 为了让客户感觉是在"并发". 过长的请求要拆分为有限状态自动机

#### reactor + thread per request(过渡方案)
一个请求用一个线程进行处理

#### reactor + worker thread

#### reactor + threadpool 
* 请求在线程池取一个线程进行处理, 处理完要扔回reactor进行发送
* 能处理计算密集型请求

#### multiple reactors
* 每个线程/进程都有一个reactor (one eventloop per thread)
* 如果一个线程来处理io, 会有瓶颈. 这样处理能适应突发的IO请求
* mainReactor 作为accepter, 将请求轮流均匀分配到几个reactor里面(round robin)
* 一般, 一个SubReactor能适应一个千兆网口, 如果服务器有3个千兆网卡, 则设置3个SubRector

#### multiple reactors + threadpool (one eventloop per thread + thread pool)
* 每个SubRector中的请求, 会交给线程池里面的的一个线程做处理
* 这样的模型能处理突发IO + 计算密集型任务

#### proactor服务器(proactor模式, 基于异步IO)
* 实现了计算与IO重叠
* 利用了硬件的DMA特性, 实现存储访问
* 与非阻塞套接字区别: 非阻塞是同步IO, 还是要手动检查IO是否完成, 处于忙等待
* 与reactor区别: reactor也是同步IO, 也是基于类似轮询处理 需要程序主动将IO数据"拉"到用户空间进行处理. 而proactor是IO完成后内核主动将数据推到用户空间进行处理
* 理论上proactor比reactor高效些
* 但是 Linux 下  
a. glibc aio 有bug   
b. 内核 native aio, 实现不完美.   
boost中实现的aio不是真正的异步, 底层也是epoll实现



总结:

(1) **线程池数目大小选择经验公式**

密集计算占时间(计算+IO)比重为P, 线程数为T, CPU数为C, 则:   
```
P * T = C
```
注意P小于0.2时公式就不太合适了

(2) reactor和proactor, 如果在软件层面实现, 最主要的区别是看IO任务是否在IO线程中先处理完, 如果是则是proactor. 这么看来, 如果是软件层面实现的proactor, 天生对于事件的响应速度就不如reactor了, 读写集中在一个线程将导致大量的阻塞!!!


## 第二部分: Muduo Net 
### 25. TcpServer实现
#### TCP网络编程最主要是处理三个半事件
1. 连接建立
2. 连接断开: 主动断开(close, shutdown), 被动断开(read返回0)
3. 消息到达: 文件描述符可读.
4. 消息发送完毕(半个): 低流量服务可不关心. 指数据写入操作系统发送缓冲区, 将由TCP协议栈负责数据的发送与重传.

#### muduo封装好的接口的使用规范:
1. CEchoServer类持有CTcpServer的一个实例, 同时自己定义了 OnConnect, OnMessage等方法
2. 在CEchoServer构造函数中, 通过CTcpServer的实例向CTcpServer注册自己定义的方法, 供回调使用.


#### Eventloop实现
* 一个线程最多只有一个 Eventloop
* io线程的概念
* 目前先简化代码

### 26. 


### 27. muduo处理定时器
1. timerfd\_\* 选择这一类函数的理由
2. 见下一节


### 28. 定时器
1. GetExpired() 函数:   
* 返回超时的定时器, 可能有多个. 这时需要使用 Entry pair 进行比较, 找到第一个大于该超时时间的定时器位置, 将之前的都认为是超时了.    
Tip: 这里使用 upper_bound() 应该也是可以的?
* 返回值是一个列表, 是否有性能问题? 不会, 因为有 RVO 优化  
Tip: VS 的 Debug 模式不会进行 RVO 优化. Release 模式才会. gcc 的则一直有 RVO 优化

2. TODO: 裸指针优化. 采用 C++11 标准的智能指针 
3. TimerQueue数据结构选择: 能快速找到已到期的定时器, 也要能高效的删除. 使用二叉搜索树. C++可以使用 map 或者 set, 但是map不允许重复的key, 所以选择set+pair

### 29. 
1. 进程/线程间事件通知
* pipe 单向
* socketpair 双向
* eventfd 高效

* 条件变量 仅线程

2. wakeup() 一个线程唤醒另一个线程

3. RunInLoop()

4. 设计: 让 IO 线程也能执行一些计算任务. 否则当没有 IO 任务是线程就一直阻塞着

### 30. EventLoopThread 实现
1. muduo 并发模型为 one loop per thread + threadpool. 则 EventLoopThread 就是 one loop per thread 的实现, 主要用于处理 IO 事件. 而 threadpool 用于计算任务

2. 主要做法:
* 创建一个线程
* 在线程中运行一个EventLoop


### 31. CSocket 类封装
1. htobe32() 等不是Posix标准函数, 不可移植. 只存在与 glibc 中. 而 htons/htonl 是可以移植的.
2. valgrand : 内存泄漏检测工具
3. readv 可以接收数据到多个缓冲区
4. TCP_NODELAY 选项: 编写低延迟网络服务很有必要; TCP_KEEPALIVE 选项: 定期检测链接是否存在, 如果应用层有心跳可以不用设置.
5. __attribute__ ((deprecated)) 告诉编译器, 调用该函数要发出"过时"的警告.
6. 自连接现象, 及其原因.

### 32. Acceptor
1. muduo库的回调使用过程要很清晰
2. 这里用到了 EMPFILE 的处理 --- 准备多一个空闲描述符做管理

### 33. TcpServer 封装


### 34. TcpConnection的生存期管理
1. 主要完善了连接断开的处理

### 35. muduo库如何支持多线程: EventLoopThreadPool
1. IO线程池类的功能是开启若干个IO县城, 并让这些IO线程处于事件循环状态.


### 36. Buffer 应用层缓冲区实现
1. 为什么必须要有应用层缓冲区 -> write()并不一定马上发送, 而是放在内核缓冲区. 这就要网络库设置应用层缓冲区, 使得发送大量数据可以被处理. 另外上层应用如果要关闭连接, 也不能立即关闭连接, 要把缓冲区数据发完.
2. 接收缓冲区:  
粘包问题: 接收到数据, 存至input buffer, 通知上层应用程序进行判断, 若不是完整包, 就不会取走. 直到消息完整再取走
3. muduo的buffer不是最高性能的. **更高效的做法** 见37节

4. ET模式效率未必会高于LT模式. 开源库都使用LT模式. LT模式也可以采用尽可能把数据读完的方式, 减少通知次数.

### 37. 其他缓冲区实现
1. buffer不设置锁, 因为缓冲区是每个IO线程私有的, 依赖于RunInLoop()会把相关的操作放到对应的线程中进行处理. 不需要加锁, 提升了效率.
2. 更高效的缓冲区: a. 环形队列, b. mbuf 指针结构体
3. 服务端主动断开与客户端连接, 这是客户端 read() 返回0, 然后会 close(conn); 这样服务端收到的事件其实是 `POLLHUP | POLLIN`

### 38. 完善TcpConnection
1. 对于大流量应用程序:
如果对等方接收不及时, 受到通告窗口限制, 内核发送缓冲区不足, 这是用户添加的数据会到应用层发送缓冲区, 可能会撑爆output buffer.    
解决方法就是: 关注WriteCompleteCallback(), 所有数据都发送完, WriteCompleteCallback回调了, 再继续发送.
2. boost::any 可变类型解决方案
* void\*: 这种方法不安全
* boost::any 安全存储和取回任意类型, 也可用于 vector\<boost::any\>

### 39. muduo库对编写Tcp客户端的实现: Connector 和 TcpClient
1. TcpClient与Connector的关系, 和 TcpServer 与 TcpConnection 的关系类似


### 40. 后面的是关于 http 模块, 以及一些用例
#

### 42. muduo 使用示例:
1. 网络编程主要处理3个半事件
* 连接建立
* 连接断开
* 消息接收
* 消息发送完成(对于低流量程序不需要关注)

2. 使用muduo库实现应用程序, 只需创建一个类, 包含CTcpServer对象, 再向该对象注册OnConnection(), OnMessage(), OnWriteCompleteCallback()即可

### Poll处理错误汇总
1. EBADF 文件描述符错误. 当使用一个本端已被关闭的文件描述符进行读写操作时, 会报这种错误.
2. Broken Pipe: 原因: peer close对端关闭
3. 僵尸进程与孤儿进程
