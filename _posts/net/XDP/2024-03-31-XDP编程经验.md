---
layout: post
title: XDP编程经验
category: net
typora-root-url: ../../..
---

## 概念

XDP全称为**eXpress Data Path**，它能够在网络包进入用户态直接对网络包进行过滤或者处理。XDP依赖eBPF技术。

### iptables/netfilter的问题

iptables规则逐渐增加，遍历iptables效率变得很低，一个表现就是kube-proxy，他是Kubernetes的一个组件，[容器](https://cloud.tencent.com/product/tke?from=3346)要使用iptables和-j DNAT规则为服务提供[负载均衡](https://cloud.tencent.com/product/clb?from=3346)。随着服务增加，iptable的规则列表指数增长。随着服务数量的增长，网络延迟和性能严重下降。iptables的还有一个缺点，无法实现增量更新。每次添加新规则时，必须更新整个规则列表。一个例子：装配2万个Kubernetes服务产生16万条的iptables规则需要耗时5个小时。

在容器环境下还有一个问题：容器的生命周期可能很多，可能一个容器的生命周期只有几秒，意味着iptables规则需要被快速更新，这也使得依靠使用IP地址进行安全过滤的系统受到压力，因为集群中的所有节点都必须始终知道最新的IP到容器的映射。

针对这个问题，cilium就是用ebpf来进行解决。另外也有基于BPF构建的bpfilter

### XDP的优势

相对于DPDK，XDP具有以下优点

- 无需第三方代码库和许可
- 同时支持轮询式和中断式网络
- 无需分配大页
- 无需专用的CPU
- 无需定义新的安全网络模型
- 对网卡要求低很多，适合嵌入式场景

使用场景

- DDoS防御
- 防火墙：bpfilter，发展一般
- 基于XDP_TX的负载均衡：kortan、cilium
- 网络统计
- 复杂网络采样
- 高速交易平台
- SD-WAN支撑：主要是**基于AF_XDP**来提升软件交换机(ovs)性能

## 基础知识

### 网络钩子（hook）

在计算机网络中，Hook钩子在操作系统中用于在调用前或执行过程中拦截网络数据包。Linux内核中暴露了多个钩子，BPF程序可以连接到这些钩子上，实现数据收集和自定义事件处理。我们主要关注两个hook：

* xdp：可以处理RX方向靠近NIC的数据包（只存在RX路径上）。
* tc：可以处理TX方向靠近NIC的数据包

### XDP程序介绍

它只存在于RX路径上，允许在网络设备驱动内部网络堆栈中数据来源最早的地方进行数据包处理，在特定模式下可以在操作系统分配内存（**skb**）之前就已经完成处理。

XDP暴露了一个可以加载BPF程序的网络钩子，其用法：

**【XDP程序示例】**

```c
//PART1:
#include <linux/bpf.h>
#include <bpf/bpf_helpers.h>

//PART2:
/* LLVM maps __sync_fetch_and_add() as a built-in function to the BPF atomic add
 * instruction (that is BPF_STX | BPF_XADD | BPF_W for word sizes)
 */
#ifndef lock_xadd
#define lock_xadd(ptr, val)     ((void) __sync_fetch_and_add(ptr, val))
#endif

//PART3:
//统计XDP_PASS收包数，key=XDP_PASS(2)，value=收包数
struct bpf_map_def SEC("maps") xdp_stats_map = {
        .type        = BPF_MAP_TYPE_ARRAY,
        .key_size    = sizeof(__u32),
        .value_size  = sizeof(__u64),
        .max_entries = XDP_REDIRECT+1,
};

//PART4:
static __always_inline long add_recv_cnt() {
        __u32 key = XDP_PASS; /* XDP_PASS = 2 */
        __u64 *rec_ptr;

        //查找返回value的指针，对于BPF_MAP_TYPE_ARRAY类型，因为数组是静态创建好的，总是返回有效指针
        rec_ptr = bpf_map_lookup_elem(&xdp_stats_map, &key);
        if (!rec_ptr)
                return XDP_ABORTED;

        //多核可能同时访问rec_ptr，需要原子操作
        lock_xadd(rec_ptr, 1);
        return XDP_PASS;
}

//PART5:
SEC("xdp_stats")
int  xdp_stats_func(struct xdp_md *ctx) {
        return add_recv_cnt();
}

//PART6:
char _license[] SEC("license") = "GPL";
```

这个程序包含几个部分：

1. PART1，头文件引用，

   * `linux/bpf.h`头文件包括BPF程序使用到的所有结构和常量的定义；

   * `bpf/bpf_helpers.h`头文件则包括libbpf库提供的一些辅助函数，如：bpf_map_lookup_elem()等，同时修改了一些通用宏定义如：

     ```c
     #define SEC(name) __attribute__((section(name), used))
     #undef __always_inline
     #define __always_inline inline __attribute__((always_inline))
     #endif
     ```

2. PART2，宏定义BPF内置操作函数；由于eBPF内核验证器要求BPF程序除了内联函数和内置函数，无法调用其他函数，所以对于内存拷贝，原子操作等都使用内置操作函数（内置操作函数在内部永远都是编译为内联的）
3. PART3，定义数据存储的数据结构`xdp_stats_map`；map数据结构都需要添加`SEC(maps)`这个 **section annotations**，这样内核在装载BPF ELF文件时，能准确找出xdp程序部分、xdp map数据部分、以及BPF 程序的 license 信息部分；我们称之为**对象**；后续当通过`bpf()`系统调用进行操作对应对象时，内核就能准确找到要操作的部分，或通过 license 信息对所调用函数进行协议兼容性判断。值得注意的是，libbpf内部也用三个结构体来对应这些对象：
   * struct `bpf_object`
   * struct `bpf_program`
   * struct `bpf_map`
4. PART4，自定义的函数，需要使用`__always_inline`声明其永远被内联，否则无法通过eBPF内核验证器
5. PART5，通过`SEC("xdp_stats")`定义xdp程序部分，程序sec命名为`xdp_stats`
6. PART6，通过`SEC("license")`定义协议信息

**【XDP输入参数】**

XDP暴露的钩子使用单一`struct xdp_md`作为其输入参数，定义在bpf.h中：

```c
struct xdp_md {
  __u32 data;				// 数据包开始指针
  __u32 data_end;			// 数据包结尾指针
  __u32 data_meta;			// 空闲内存地址，用于与其他层交换数据包元数据
  /* Below access go through struct xdp_rxq_info */
  // 当访问这两个值时，BPF代码会在内核内部重写，以访问实际持有这些值的内核结构struct xdp_rxq_info。
  __u32 ingress_ifindex; /* 接收数据包的接口, 对应rxq->dev->ifindex */
  __u32 rx_queue_index;  /* RX队列的索引，对应rxq->queue_index  */
};
```

**【XDP输出参数】**

XDP钩子函数处理完数据包后，需要返回一个动作（Action）作为输出，定义在bpf.h中：

```c
enum xdp_action {
  XDP_ABORTED = 0, // Drop packet while raising an exception
  XDP_DROP, // Drop packet silently
  XDP_PASS, // Allow further processing by the kernel stack
  XDP_TX, // Transmit from the interface it came from
  XDP_REDIRECT, // Transmit packet from another interface
};
```

其中XDP_REDIRECT动作一般通过调用`action = bpf_redirect(ifindex, flags)`设置转发到的网卡，设置成功后将该函数返回的XDP_REDIRECT动作作为返回值即可。

**【XDP程序编译运行】**

XDP程序需要使用clang编译为BPF字节码，再通过加载器加载进内核。

编译使用如下指令：

```shell
#直接编译
clang -O2 -g -Wall -target bpf -c xstatic_kern.c -o xstatic_kern.o

#先创建文件依赖列表：
clang -MD -MF xstatic_kern.d -target bpf -I ~/linux/tools/lib/bpf -c xstatic_kern.c
#再将依赖列表所需文件移至./include，然后使用依赖列表的少数文件进行编译
clang -target bpf -Wall -O2 -emit-llvm -g -I./include -c xstatic_kern.c -o - | \
llc -march=bpf -mcpu=probe -filetype=obj -o xstatic_kern.o
```

XDP 总共支持三种工作模式（operation mode）：

* native

   **native XDP**（原生 XDP）, 意味着 BPF 程序**直接在驱动的接收路径上运行**，理论上这是软件层最早可以处理包的位置（the earliest possible point）。这是**常规/传统的 XDP 模式，需要驱动实现对 XDP 的支持**，目前 Linux 内核中**主流的 10G/40G 网卡都已经支持**。

* generic

   **generic XDP**（通用 XDP），用于给那些还没有原生支持 XDP 的驱动进行试验性测试。generic XDP hook 位于内核协议栈的主接收路径（main receive path）上，接受的是 `skb` 格式的包，但由于 **这些 hook 位于 ingress 路径的很后面**（a much later point），因此与 native XDP 相比性能有明显下降。因此，`generic` 大部分情况下只能用于试验目的，很少用于生产环境。

* offload

  最后，一些智能网卡（例如支持 Netronome’s nfp 驱动的网卡）实现了 `offload` 模式 ，允许将整个 BPF/XDP 程序 offload 到硬件，因此程序在网卡收到包时就直接在网卡进行 处理。这提供了比 native XDP 更高的性能，虽然在这种模式中某些 BPF map 类型 和 BPF 辅助函数是不能用的。BPF 校验器检测到这种情况时会直 接报错，告诉用户哪些东西是不支持的。除了这些不支持的 BPF 特性之外，其他方面与 native XDP 都是一样的。

将`xstatic_kern.o`加载进内核则有多种方式：

1. 使用iproute2指令进行加载：

   ```shell
   #以lo网卡举例，使用xdp则智能选择XDP程序工作模式，xdpdrv、xdpgeneric、xdpoffload则是手动指定
   #不指定sec，则使用地一个
   ip link set dev lo xdp obj prog.o sec foobar
   
   #加载并覆盖
   ip -force link set dev lo xdp obj prog.o
   
   #从接口删除XDP程序
   ip link set dev lo xdp off
   ```

2. 自己编写C装载程序，参考[xdp-tutorial](https://github.com/xdp-project/xdp-tutorial)；大致的处理流程如下：
   ![img](../../../assets/XDP%E7%BC%96%E7%A8%8B%E7%BB%8F%E9%AA%8C/e0a9e2a9972ae43db39605054304af47.png)

3. 利用C库，如[bcc](https://github.com/iovisor/bcc)，进行装载。

4. 利用特定语言提供的库进行加载，如golang的`cilium/ebpf`。这种方式非常简单方便，并且直接和golang语言绑定，方便使用go语言直接操作XDP程序和数据。详见下文：[配合golang开发](#配合golang开发)

**【XDP程序状态监控】**

linux系统有多种方式可以观察xdp程序的各项状态，整理如下：

1. 查看日志：xdp程序内部可以使用`bpf_printk()`（宏定义，调用bpf_trace_printk）打印日志；该日志输出到路径：`/sys/kernel/debug/tracing/trace_pipe`，直接`cat /sys/kernel/debug/tracing/trace_pipe`就可以查看实时的日志

2. 利用bpftool查看程序状态和map数据：通过`apt install bpftool`安装

   * 查看程序信息：

     ```shell
     #查看程序信息
     bpftool prog show
     bpftool prog show --json id 123
     
     #获取整个程序的代码信息，方便调试由编译器生成的 BPF 字节码
     bpftool prog dump xlated id 123
     
     #图形展示
     bpftool prog dump xlated id 817 visual &> output.out
     apt install graphviz
     dot -Tpng output.out -o visual-graph.png
     ```

   * 查看map数据：

     ```shell
     #查看正在使用的 BPF 映射
     bpftool map show
     
     #创建映射counter，并钉（pin）到 BPF 伪文件系统
     bpftool map create /sys/fs/bpf/counter name counter type array key 4 value 4 entries 5
     #更新映射，key和value要一个一个字节输入
     bpftool map update name counter key 1 0 0 0 value 0 0 0 0
     #查看映射信息
     bpftool map dump name counter
     ```

   * 其他：参考`bpftool help`

3. 查看 BPF 伪文件系统：map 可以被钉（pin）到 BPF 伪文件系统，进行查看和修改，文件系统位置在：`/sys/fs/bpf/`，如果未挂载，需要先执行挂载指令：`mount -t bpf bpf /sys/fs/bpf`

## 来自ebpf的特性

eBPF程序被“附加”到内核中指定的代码路径。当代码路径被遍历到时，任何附加的eBPF程序都会被执行。当前主要用途是：

* 网络：过滤流量（DDoS)、流量分类、高性能数据包处理、AF_XDP编程
* 限制一个进程可以使用的系统调用方法。这是用[seccomp BPF](https://lwn.net/Articles/656307/)实现的。
* 调试内核和执行性能分析:程序可以被附加到跟踪点、kprobes和perf事件。因为eBPF程序可以访问内核的数据结构，所以开发人员可以编写和测试新的调试代码，而不必重新编译内核。这对于在实时运行系统上调试问题的繁忙工程师来说，作用是显而易见的。

### eBPF内核验证器

允许用户空间代码在内核中运行，是存在固有的安全性和稳定性风险的。因此，在加载每个eBPF程序之前，都要执行一定数量的检查测试

工作原理：

1. 第一个测试是确保eBPF程序终止时，不包含任何可能导致内核锁定的循环逻辑，这点是通过对程序的控制流图(CFG)进行深度优先搜索来检查的。严禁使用不可达的指令；任何包含不可达指令的程序都将无法加载。
2. 第二阶段更为复杂，需要验证器模拟执行eBPF程序，每次一条指令。在执行每条指令之前和之后检查虚拟机状态，以确保寄存器和堆栈状态是有效的。禁止越界跳转，也禁止访问越界数据。
   验证器不需要遍历程序中的每条路径，因为它足够智能，知道当前程序的状态是已经被检查过程序的子集。因为之前的所有路径都必须是有效的(否则程序就已经加载失败了)，当前路径必须也是有效的。这允许验证器“裁剪”当前分支并跳过其模拟验证过程。
3. 验证器还有一个禁止指针运算的“安全模式”。当没有使用`CAP_SYS_ADMIN`特权选项加载eBPF程序时，就会启用安全模式。其思想是确保内核地址不会泄漏给没有特权的用户，并且指针不能写入内存。如果未启用安全模式，则允许指针运算，但必须在执行附加检查之后。例如，检查所有指针访问的类型、位置和边界违反情况。
4. 无法读取具有未初始化内容(那些从未被写入的内容)的寄存器;这么做会导致程序加载失败。寄存器R0-R5的内容在函数调用时会被标记为不可读，方法是存储一个特殊的值来捕获任何读取未初始化寄存器的操作。对读取栈上的变量也进行了类似的检查，以确保没有指令写入只读类型的帧指针寄存器。
5. 最后，验证器使用eBPF程序类型(后面将介绍)来限制可以从eBPF程序调用哪些内核函数以及可以访问哪些数据结构。例如，某些程序类型可以直接访问网络包数据。

影响：

1. 限制循环：
   * 内核5.3以前：不支持loop，如果需要循环，要靠编译选项`unroll`对循环进行展开，参考：https://docs.cilium.io/en/v1.10/bpf/#llvm
   * 5.3~5.13：Bounded Loop；由XDP内部支持受限的loop，但不能超过指令上限(BPF_COMPLEXITY_LIMIT_INSNS)和jmp数目上限(BPF_COMPLEXITY_LIMIT_JMP_SEQ)，否则需要修改内核源码，参考[XDP-Forwarding](https://github.com/gamemann/XDP-Forwarding/blob/master/patches)
   * 5.13之后：MAP Iterator；提供`BPF_MAP_GET_NEXT_KEY `和`BPF_MAP_LOOKUP_BATCH`指令来循环遍历内部map，参考：https://lwn.net/Articles/826058/
   * 5.17之后：bpf_loop；相当于把 loop 从普通的函数流程里抽取了出来，单独用一个 bpf 函数来实现。据说已经合并到5.17之后版本，参考：
     * https://stackoverflow.com/questions/56107380/are-loops-allowed-in-linuxs-bpf-programs
     * https://lwn.net/Articles/877062/

### ebpf调用

使用`bpf()`这个系统调用函数配合对应的命令来与ebpf程序进行交互、以及创建和修改eBPF maps数据结构。原型如下：

```c
// cmd: 操作的指令
// attr: union结构，允许在内核和用户空间之间传递数据；格式取决于cmd参数
// size: attr的字节大小
int bpf(int cmd, union bpf_attr *attr, unsigned int size);
```

注：libbpf库兼容旧版bpf，它还是通过`syscall(__NR_bpf, ...)`方式与bpf类程序进行交互的。

详细的用法可以查`man bpf`，这里简要列举：

* cmd命令分三类：使用eBPF程序的命令，使用eBPF maps的命令，或同时使用程序和maps的命令

* BPF_PROG_LOAD命令加载的程序，attr中包括了程序类型信息规定了四件事：程序可以附加在哪里，验证器允许调用内核中的哪些帮助函数，网络包的数据是否可以直接访问，作为第一个参数传递给程序的对象类型。

* eBPF程序使用的主要数据结构是eBPF map（键值对）数据结构，由BPF_MAP_XXXXXX类指令进行控制；map有多种类型，每种类型由如下结构体定义：

  ```c
  struct bpf_map_def SEC("maps") xdp_stats_map = {
  	.type        = BPF_MAP_TYPE_ARRAY,
  	.key_size    = sizeof(__u32),
  	.value_size  = sizeof(struct datarec),
  	.max_entries = XDP_ACTION_MAX,
  };
  ```

  

## 配合golang开发

使用golang开发，我们使用**[cilium/ebpf](https://github.com/cilium/ebpf)**，因为它由Cilium 和 Cloudflare 维护，更加稳定可用。基于该库可以很方便实现golang开发XDP程序：

1. 使用`//go:generate`调用xdp编译成ELF格式码，并嵌入到go代码中，便于进行代码分发。
2. golang通过调用cilium/ebpf库，可以快速实现XDP加载并与其map交互，实现xdp用户端程序。

最终这边会使用golang来开发XDP程序，详细用例参考：xnat。

## AF_XDP开发

> ref: 
>
> [xdp-tutorial](https://github.com/xdp-project/xdp-tutorial)/**advanced03-AF_XDP**
>
> [AF_XDP技术详解](https://rexrock.github.io/post/af_xdp1/) ：通俗易懂，并且直接教会如何编写AF_XDP程序

上文提到，通过XDP_REDIRECT我们可以将报文重定向到其他设备发送出去或者重定向到其他的CPU继续进行处理。而**AF_XDP则利用 bpf_redirect_map()函数，实现将报文重定向到用户态一块指定的内存中**，接下来我们看一下这到底是如何做到的。

我们使用普通的 socket() 系统调用创建一个AF_XDP套接字（XSK）。每个XSK都有两个ring：RX RING 和 TX RING。套接字可以在 RX RING 上接收数据包，并且可以在 TX RING 环上发送数据包。这些环分别通过 setockopts() 的 XDP_RX_RING 和 XDP_TX_RING 进行注册和调整大小。每个 socket 必须至少有一个这样的环。RX或TX描述符环指向存储区域（称为UMEM）中的数据缓冲区。RX和TX可以共享同一UMEM，因此不必在RX和TX之间复制数据包。

UMEM也有两个 ring：FILL RING 和 COMPLETION RING。应用程序使用 FILL RING 向内核发送可以承载报文的 addr (该 addr 指向UMEM中某个chunk)，以供内核填充RX数据包数据。每当收到数据包，对这些 chunks 的引用就会出现在RX环中。另一方面，COMPLETION RING包含内核已完全传输的 chunks 地址，可以由用户空间再次用于 TX 或 RX。

> 此外需要注意的事，AF_XDP socket不再通过 send()/recv()等函数实现报文收发，而实通过直接操作ring来实现报文收发。
>
> 1. FILL RING
>
> **fill_ring 的生产者是用户态程序，消费者是内核态中的XDP程序；**
>
> 用户态程序通过 fill_ring 将可以用来承载报文的 UMEM frames 传到内核，然后内核消耗 fill_ring 中的元素（后文统一称为 desc），并将报文拷贝到desc中指定地址（该地址即UMEM frame的地址）；
>
> 1. COMPLETION RING
>
> **completion_ring 的生产者是XDP程序，消费者是用户态程序；**
>
> 当内核完成XDP报文的发送，会通过 completion_ring 来通知用户态程序，哪些报文已经成功发送，然后用户态程序消耗 completion_ring 中 desc(只是更新consumer计数相当于确认)；
>
> 1. RX RING
>
> **rx_ring的生产者是XDP程序，消费者是用户态程序；**
>
> XDP程序消耗 fill_ring，获取可以承载报文的 desc并将报文拷贝到desc中指定的地址，然后将desc填充到 rx_ring 中，并通过socket IO机制通知用户态程序从 rx_ring 中接收报文；
>
> 1. TX RING
>
> **tx_ring的生产者是用户态程序，消费者是XDP程序；**
>
> 用户态程序将要发送的报文拷贝 tx_ring 中 desc指定的地址中，然后 XDP程序 消耗 tx_ring 中的desc，将报文发送出去，并通过 completion_ring 将成功发送的报文的desc告诉用户态程序；

### AF_XDP 的性能提升从何而来？

AF_XDP socket 非常快，在这个性能提升的背后隐藏了多少秘密呢？ AF_XDP 的 idea 背后的基础可以追溯到 [Van Jacobson](https://en.wikipedia.org/wiki/Van_Jacobson) 的关于 [network channels](https://lwn.net/Articles/169961/) 的报告中。在该报告中，描述了如何直接从驱动的 RX-queue （接收队列）去创建一个无锁的 [channel](https://lwn.net/Articles/169961/) 构建 AF_XDP socket。

（前面介绍 `AF_XDP` 的内容也提到了），AF_XDP 使用的队列是 Single-Producer/Single-Consumer (SPSC) 的描述符（descriptor）环形队列：

- **Single-Producer** (SP) 绑定到了某个特定的 RX **queue id** 上，通过 NAPI-softirq 确保在软中断（softirq）触发期间，只有一个 CPU 来处理一个 RX-queue id。

  TIP

  NAPI 是 Linux 上采用的一种提高网络处理效率的技术，它的核心概念就是不采用中断的方式读取数据，否则包太多了，不停触发中断。而代之以首先采用中断唤醒数据接收的服务程序，然后 POLL 的方法来轮询数据。

- **Single-Consumer** (SC) 则是一个应用，从环中读取指向 UMEM 区域的描述符（descriptor）。

因此不需要对每个包都分配一次内存。可以在事先为 UMEM 内存区域进行分配（因此 UMEM 是有界的）。UMEM 包含了一些大小相同的块，环中的指针会引用它们的地址来引用这些块。这个地址就是在整个 UMEM 区域中的偏移量。用户空间负责为 UMEM 分配内存，分配的方法很灵活，可以用 malloc、mmap、huge pages 等形式。这个内存空间通过在 `setsockopt()` 方法中设置 `XDP_UMEM_REG` 触发相应的系统调用，注册到内核中。**需要注意的是**：这样就意味着你需要负责及时地将 frame 返回给 UMEM，并且需要为你的应用提前分配足够的内存。

Van Jacobson 在报告中谈到的 [transport signature](http://www.lemis.com/grog/Documentation/vj/lca06vj.pdf)，在 XDP/eBPF 程序中体现为选择将 frame `XDP_REDIRECT` 到哪个 AF_XDP socket。

### zero-copy 模式

正如前面提过的 AF_XDP 依赖于驱动的 `XDP_REDIRECT` action 实现。对于所有实现了 `XDP_REDIRECT` action 的驱动，就都支持 “copy-mode” 下的 AF_XDP。“copy-mode” 非常快，只拷贝一次 frame（包括所有 XDP 相关的 meta-data）到 UMEM 区域。用户空间的 API 也是如此。

为了支持 AF_XDP 的 “zero-copy” 模式，驱动需要在 NIC RX-ring 结构中直接实现并暴露出注册和使用 UMEM 区域的 API，以便使用 DMA。针对你的应用场景，在支持 “zero-copy” 的驱动上使用 “copy-mode” 仍然可能是有意义的。如果出于某些原因，并不是 RX-queue 中所有的流量都是要发给 AF_XDP socket 的，XDP 程序在 `XDP_REDIRECT` 和 `XDP_PASS` 间交替，如上面的 Advance03 示例中的那样，那么 “copy-mode” 可能是更好的选择。因为在 “zero-copy” 模式下使用 XDP_PASS 的代价很高，涉及到了为 frame 分配内存和执行内存拷贝。

### 可能碰到的问题

- 首先这些 BPF 相关的 demo 都是需要 `sudo` 去跑的，需要管理员权限。

- 系统内核太旧了，本身不支持 `AF_XDP` socket。

- 最常见的错误：为什么我在 AF_XDP socket 上看不到任何流量？

  正如你在上面了解到的，AF_XDP socket 绑定到了一个 **single RX-queue id** （出于性能考量）。因此，用户空间的程序只会收到某个特定的 RX-queue id 下的 frames。然而事实上网卡会通过 RSS-Hashing，把流量散列到不同的 RX-queues 之间。因此，流量可能没有到达你所期望的那个队列。

  TIP

  RSS (Receive Side Scaling) Hashing 是一种能够在多处理器系统下使接收报文在多个CPU之间高效分发的网卡驱动技术。网卡对接收到的报文进行解析，获取IP地址、协议和端口五元组信息。网卡通过配置的 HASH 函数根据五元组信息计算出 HASH 值,也可以根据二、三或四元组进行计算。取HASH值的低几位（这个具体网卡可能不同）作为 RETA (redirection table) 的索引，根据 RETA 中存储的值分发到对应的 CPU。

  为了解决这个问题，你必须配置网卡，让流进入一个特定的 RX-queue，可以通过 ethtool 或 TC HW offloading filter 设置。下面的例子展示了如何配置网卡，将所有的 UDP ipv4 流量都导入 *RX-queue id* 42：

  

  ```
  ethtool -N <interface> flow-type udp4 action 42
  ```

  参数 *action* 指定了目标 *RX-queue*。一般来说，上面的这个流量转发的规则包含了匹配准则和 action。L2、L3 和 L4 header 值能被用来指定匹配准则。如果想要阅读更详细的文档，请查看 ethtool 的 man page （`man ethtool`）。它记载了 header 中所有能够用来作为匹配准则的值。

  其他替代的方案：

  1. 创建和 RX-queue 数量相同的 AF_XDP sockets，然后由用户空间使用 `poll/select` 等方法轮询这些 sockets。
  2. 出于测试目的，也可以把 RX-queue 的数量削减到 1，例如：使用命令 `ethtool -L <interface> combined 1`。

  但是在用 `testenv/testenv.sh` 脚本虚拟出来的网卡用不了 `ethtool` 的上面这些和 RX-queue 相关的命令。

### zero-copy 模式

正如前面提过的 AF_XDP 依赖于驱动的 `XDP_REDIRECT` action 实现。对于所有实现了 `XDP_REDIRECT` action 的驱动，就都支持 “copy-mode” 下的 AF_XDP。“copy-mode” 非常快，只拷贝一次 frame（包括所有 XDP 相关的 meta-data）到 UMEM 区域。用户空间的 API 也是如此。

为了支持 AF_XDP 的 “zero-copy” 模式，驱动需要在 NIC RX-ring 结构中直接实现并暴露出注册和使用 UMEM 区域的 API，以便使用 DMA。针对你的应用场景，在支持 “zero-copy” 的驱动上使用 “copy-mode” 仍然可能是有意义的。如果出于某些原因，并不是 RX-queue 中所有的流量都是要发给 AF_XDP socket 的，XDP 程序在 `XDP_REDIRECT` 和 `XDP_PASS` 间交替，如上面的 Advance03 示例中的那样，那么 “copy-mode” 可能是更好的选择。因为在 “zero-copy” 模式下使用 XDP_PASS 的代价很高，涉及到了为 frame 分配内存和执行内存拷贝。

### 在 STM32MP157A 开发板上跑这个 demo 碰到的问题

1. 板载系统没有开启 AF_XDP_SOCKET 支持（幸亏厂商提供了基于 5.4.31 内核的 Ubuntu 18.04，而且提供了他们构建开发板时的项目源码，只需要改下配置项，重新编译下内核，但凡他们搞个低版本的，闹不好我就寄了）。那么需要在内核源码目录下的`.config` 中重新编译一份 arm 架构的内核，将生成的 uImage 镜像和设备树文件拷贝到板子的 `/boot` 目录下。板子我是用的 sd 卡安装的 ubuntu，boot 目录没有自动挂载到，还要到 `/dev` 下找到它所在的分区（记录一下，我自己的板子是 block1p4），对应的 u-boot 的配置文件中如果启动的路径不对，可能也要修改。这里就庆幸自己是拿的 sd 卡装的，不然在只能进入到 u-boot 终端的情况下，只用 tftp 还处理不了 `boot` 目录下错误的路径配置。

2. 编译上面的例子时候，板子缺少 `libelf-dev` 包，会报错丢失 `<gelf.h>` 头文件。

3. 编译上面的例子时候，板子的`/usr/include/` 下没有 `asm` 文件夹，只有 `asm_generic`。有人博客里写，给 `asm_generic` 链接到 `asm` 就行了。亲测不是如此，二者包含的头文件并不相同。

   后来发现该目录下，还有一个 `arm `开头的文件夹，推测里面应该包含了板子 `arm` 架构下的相关头文件。打开后果然如此，有一个`asm`，那么只需要在`/usr/include` 下做一个软连接 `ln -s` 到它，命名成 `asm` 就行了。



## 问题解决经验

### XDP钩子可以同时挂接多个程序吗？

不可以的。同时只能有一个程序在XDP钩子运行；可以设计用其他方式支持一个钩子分发(dispatcher)消息到多个程序，参考：https://lpc.events/event/7/contributions/671/attachments/561/992/xdp-multiprog.pdf

### golang操作map，对于key为结构体类型提示`struct doesn't marshal to 8 bytes`

golang通过Put和Lookup方法可以直接操作map，但是key的类型必须与bpf c文件中的key类型保持内存布局一致；通过查看`cilium/ebpf`源码的marshalBytes()操作，加入适当日志就会发现其实是内核结构体使用自动对齐达到了8Bytes，而golang中的只有7Bytes。修改内核结构体取消自动对齐即可。

### 使用veth网卡测试REDIRECT包时，不通

`xdp-tutorial/packet03-redirecting`中提到，如果需要redirect到veth的某个端口，则redirect涉及到的所有设备都需要挂载XDP程序，才能实现正常的REDIRECT。如果使用物理设备，比如同主机下一个网卡redirect到另一个网卡，也是需要两个网卡都挂载XDP程序（一般出口网卡挂载XDP_PASS即可）。另外，generic模式则无此限制。

这个是XDP的设计缺陷，后续版本应该会修复，前期需要这么做。

### 数据包部分字段改变了，需要修改校验和

修改校验和并不需要完全重新计算，如果只是部分包头改变了，比如16bit以内，可以使用增量算法（见[rfc1141](https://www.rfc-editor.org/rfc/rfc1141)），在XDP程序中可以调用`bpf_csum_diff()`。



## 参考资源

基础参考：

* Cilium：BPF 和 XDP 参考指南：[BPF and XDP Reference Guide](https://docs.cilium.io/en/stable/bpf/#bpf-and-xdp-reference-guide)，[译文](https://arthurchiao.art/blog/cilium-bpf-xdp-reference-guide-zh/)

* [xdp-tutorial](https://github.com/xdp-project/xdp-tutorial)

  * `basic02`：讲解了 libbpf 怎么加载 bpf 代码的。让读者自己实现一个简化的加载过程。用户实现的函数，使用 `_` 前缀与教程中 xdp 团队提供的 api 相区分。相应的 api 是没有 `_` 前缀的，位于 `common` 目录下。例如，`common/common_user_bpf_xdp.c` 下的`load_bpf_and_xdp_attach()` 函数。
  * `basic03`：讲解了 bpf map 的使用。
  * `basic04`：讲解了跨应用共享 bpf map，使用的是 **pinning maps 技术**。
  * `tracing01` 到 `tracing04` 是做 tracing 方面的应用。
  * `packet01` 到 `packet03` 是从包的层面上做了 parsing、rewriting、redirecting。

  * `advance01` 是 xdp 和 tc 交互的例子。

  * `advance03` 很有趣，是一个比较完整的例子，展示了如何通过 xdp 在用户空间解析 IPV6 ICMP 报文，并发送回复。是用了一种新型的 socket 地址类型，`AF_XDP`，可以在 kernel 的文档中找到[ AF_XDP 的介绍](https://www.kernel.org/doc/html/latest/networking/af_xdp.html)。
  * 答案分布：`advance` 和 `tracing` 部分的答案就是在代码里的。`basic` 和 `packet` 部分的是在 `basic-solutions` 和`packet-solutions` 目录下。

* 内核源码示例：${内核源码树}/samples/bpf/

进阶：

* [cilium](https://github.com/cilium/cilium)：k8s中使用的网络插件，管理整个容器网络。Cilium方案中大量使用了XDP、TC等网络相关的BPF hook，以实现高性能的网络RX和TX
* **[katran](https://github.com/facebookincubator/katran)**：facebook开发的一个L4负载均衡器
* [bcc](https://github.com/iovisor/bcc)：基于ebpf的Linux性能分析和追踪工具，列入[linux performance](https://www.brendangregg.com/linuxperf.html)

