---
layout: post
title: cilium转发原理源码分析
category: net
typora-root-url: ../../..
---

## 部署demo

我们部署简单的curl、nginx这两个应用，观察cilium如何进行数据转发：

```yaml
#filename: curl_nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: curl
  name: curl
spec:
  containers:
  - image: hub-mirror.c.163.com/curlimages/curl
    name: curl
    command: ["sleep", "365d"]
  nodeName: node1
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  containers:
  - image: hub-mirror.c.163.com/nginx:alpine
    name: nginx
  nodeName: node2
```

部署：`kubectl apply -f curl_nginx.yaml`

## 跟踪过程

首先，放上一个[别人整理](https://rexrock.github.io/post/cilium2/)好的流程图，这个图非常清晰的描绘出了cilium内部网络包转发过程，我们基于这个图进行跟踪验证：

![cilium datapath](../../../assets/cilium%E8%BD%AC%E5%8F%91%E5%8E%9F%E7%90%86%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/1614297388691.png)

**所以流量劫持从prog类型上可以分为：**

- 内核层面基于sock的流量劫持，主要用于lb（k8s-proxy）;
- 基于端口流量劫持，实现整个datapath的替换；

cilium-agent是daemonset方式部署，在宿主机上会创建如下网卡：

* cilium_vxlan：cilium开启tunnel模式用到（默认模式）。我们主要需要关注这个网卡
* cilium_net和cilium_host：只在处理ipvlan模式需要用到
* cilium_health：只在做健康检查用到
* 以及与容器网络命名空间的以太接口的隧道对端 lxcxxxx

首先确认下我们的curl和nginx的地址：

```shell
$ kubectl get pod -A -owide
...
kube-system   curl   ...   10.244.0.108
kube-system   nginx  ...   10.244.2.171
...
```

* curl这个pod地址：10.244.0.108
* nginx这个pod地址：10.244.2.171

### 从curl这个pod发出数据包：

```shell
#查找到10.244.2.171的路由，发现需要经过网关10.244.0.150
$ kubectl -n kube-system exec curl -- ip r get 10.244.2.171
...
74: eth0@if75 ....
...

#查看10.244.0.150使用的mac，会发现这个mac地址是配在宿主机上的lxca7eadf015921上的
$ kubectl -n kube-system exec curl -- arp -n | grep 10.244.0.150

#查看lxca7eadf015921网卡，curl上的cilium-agent是cilium-9jlhx
$ kubectl -n kube-system exec cilium-9jlhx -- ip l show lxca7eadf015921
...
75: lxca7eadf015921@if74 ...
...
```

这其实是因为curl和宿主机使用veth相连，这个从curl上的eth0的网卡名和接口编号都能看出来。

### 进入lxc网卡

上节curl的eth0 egress方向的包，因为veth特性，会变成lxca7eadf015921 ingress方向的包。lxc网卡上有hook ebpf程序：

```shell
$ kubectl -n kube-system exec -ti cilium-9jlhx -- bpftool net show dev lxca7eadf015921
xdp:

tc:
lxca7eadf015921(75) clsact/ingress bpf_lxc.o:[from-container] id 7322

flow_dissector:


# 查看挂载的程序详细信息
$ kubectl -n kube-system exec -ti cilium-9jlhx -- bpftool prog show id 7322
7322: sched_cls  name handle_xgress  ...
...
```

可以看到lxc网卡上只有ingress方向加载了 BPF 程序 `bpf_lxc.o` 的 `from-container` 部分。到 Cilium 的源码 [bpf_lxc.c](https://github.com/cilium/cilium/blob/1c466d26ff0edfb5021d024f755d4d00bc744792/bpf/bpf_lxc.c#L1320) 的 `__section("from-container")` 部分，程序名 `handle_xgress`：

```shell
handle_xgress 
  ep_tail_call(ctx, CILIUM_CALL_IPV4_FROM_LXC) => tail_handle_ipv4
    __tail_handle_ipv4
      lb4_lookup_service #1
      lb4_local
      tail_ipv4_ct_egress
      	ct_lookup4 #2
      	tail_handle_ipv4_cont 
          handle_ipv4_from_lxc
            lookup_ip4_remote_endpoint => ipcache_lookup4 #3
            lb4_ctx_restore_state #4
            policy_can_access4 #5
            ct_create4 #6
            if TUNNEL_MODE
              encap_and_redirect_lxc #7
                ctx_redirect(ctx, ENCAP_IFINDEX, 0)
            if ENABLE_ROUTING
              ipv4_l3
            return CTX_ACT_OK;
```

(1)：对包进行验证，并提取出+L3+proto（协议类型）

(2)：所有 IPv4 的网络包都交由 `tail_handle_ipv4` 来处理。`handle_xgress`是如何跳转到 `tail_handle_ipv4` ，这里用到了 [Tails Call](https://docs.cilium.io/en/stable/bpf/#tail-calls) 。Tails call 允许我们配置在某个 BPF 程序执行完成并满足某个条件时执行指定的另一个程序，且无需返回原程序。

要理清`handle_xgress`跳转到 `tail_handle_ipv4`的细节，其实会涉及到比较多的代码阅读。我们在[cilium核心（cilium-daemon）实现源码分析]()中有做详细介绍

(3)：核心的逻辑都在 `__tail_handle_ipv4`。完成：

1. 首先查找**Service 负载均衡**，即从 Service 的后端 Pod 中选择一个合适的，这里假设选择 node2 上的 nginx。

2. 先使用ct_lookup4()查找CT，判断是否redirect_proxy。

3. 查找cilium_ipcache这个map的隧道信息（tunnel endpoint、vxlanID）；下面安全策略验证会用到。可以使用cilium检查：

   ```shell
   $ kubectl -n kube-system exec -ti cilium-9jlhx -- cilium map get cilium_ipcache
   ...
   10.244.2.171/32 ... tunnelendpoint=192.168.56.106 ...
   ...
   ```

4. 如果1有查到service映射变化，这里**执行 DNAT**，将包的 `dst_ip` 由 ServiceIP 改成 `POD_IP`。我们这里没有变化。

5. 进行容器出向（egress）**安全策略验证**，这里假设没有安全策略，因此能通过验证。

6. 创建或更新**连接跟踪**（CT）记录。（修改cilium_ct4_global或ciilum_ct_any4_global）注意这里使用`&ct_state_new`将有变化的信息写入ct

7. 之后的处理会有两种模式：

   * 直接路由：交由内核网络栈进行处理，或者 underlaying SDN 的支持。
   * 隧道：会将网络包再次封装，通过隧道传输，比如 vxlan。

   这里我们使用的也是隧道模式。网络包交给 `encap_and_redirect_lxc` 隧道包封装处理，使用 tunnel endpoint 作为隧道对端。最终转发给 `ENCAP_IFINDEX`（这个值是接口的索引值，由 cilium-agent 启动时获取的），就是以太网接口 `cilium_vxlan`

### 从cilium_vxlan发出

数据包到达cilium_vxlan网卡egress方向，先看下该网卡上加载的ebpf程序：

```shell
$ kubectl -n kube-system exec -ti cilium-9jlhx -- bpftool net show dev cilium_vxlan
xdp:

tc:
cilium_vxlan(5) clsact/ingress bpf_overlay.o:[from-overlay] id 2699
cilium_vxlan(5) clsact/egress bpf_overlay.o:[to-overlay] id 2707

flow_dissector:
```

egress方向对应程序`to-overlay`位于 [`bpf_overlay.c`](https://github.com/cilium/cilium/blob/1c466d26ff0edfb5021d024f755d4d00bc744792/bpf/bpf_overlay.c#L528) 中，这个程序的处理很简单，如果是 IPv6 协议会将封包使用 IPv6 的地址封装一次。这里是 IPv4 ，直接返回 `CTX_ACT_OK`。将网络包交给内核网络栈，进入 `eth0` 接口。

如果有使用nodeport，这里会调用`handle_nat_fwd()`进行SNAT。

### 从eth0发出

数据包到达cilium_vxlan网卡egress方向，先看下该网卡上加载的ebpf程序：

```shell
$ kubectl -n kube-system exec -ti cilium-9jlhx -- bpftool net show dev cilium_vxlan
xdp:

tc:
eth0(2) clsact/ingress bpf_netdev_eth0.o:[from-netdev] id 2823
eth0(2) clsact/egress bpf_netdev_eth0.o:[to-netdev] id 2832

flow_dissector:
```

egress 程序 `to-netdev` 位于 [`bpf_host.c`](https://github.com/cilium/cilium/blob/1c466d26ff0edfb5021d024f755d4d00bc744792/bpf/bpf_host.c#L1081)。实际上没做重要的处理，只是返回 `CTX_ACT_OK` 交给内核网络栈继续处理：将网络包发送到 vxlan 隧道发送到对端，也就是节点 `192.168.1.13` 。中间数据的传输，实际上用的还是 underlaying 网络，从主机的 `eth0` 接口经过 underlaying 网络到达目标主机的 `eth0` 接口。

如果有使用nodeport，且没有MARK_MAGIC_SNAT_DONE，这里会调用`handle_nat_fwd()`进行SNAT。

此后数据包进入网络中，最终发到node2

### node2从eth0收到vxlan包

node2上的cilium-agent是cilium-gc8d8，我们通过它来看下node2上以太网卡挂载的ebpf程序

```shell
$ kubectl -n kube-system exec -ti cilium-gc8d8 -- bpftool net show dev eth0
xdp:

tc:
eth0(2) clsact/ingress bpf_netdev_eth0.o:[from-netdev] id 4556
eth0(2) clsact/egress bpf_netdev_eth0.o:[to-netdev] id 4565

flow_dissector:
```

这里是ingress方向，触发的是 `from-netdev`，位于 [bpf_host.c](https://github.com/cilium/cilium/blob/1c466d26ff0edfb5021d024f755d4d00bc744792/bpf/bpf_host.c#L1040) 中。

```shell
from_netdev
  if vlan
    allow_vlan
    return CTX_ACT_OK
```

对 vxlan tunnel 模式来说，这里的逻辑很简单。当判断网络包是 vxlan 的并确认允许 vlan 后，直接返回 `CTX_ACT_OK` 将处理交给内核网络栈。

### node2的cilium_vxlan收到包

数据包到达cilium_vxlan网卡ingress方向，先看下该网卡上加载的ebpf程序：

```shell
$ kubectl -n kube-system exec -ti cilium-gc8d8 -- bpftool net show dev cilium_vxlan
xdp:

tc:
cilium_vxlan(5) clsact/ingress bpf_overlay.o:[from-overlay] id 4468
cilium_vxlan(5) clsact/egress bpf_overlay.o:[to-overlay] id 4476

flow_dissector:
```

ingress方向对应程序`from-overlay`位于 [`bpf_overlay.c`](https://github.com/cilium/cilium/blob/1c466d26ff0edfb5021d024f755d4d00bc744792/bpf/bpf_overlay.c#L528) 中

```shell
from_overlay
  validate_ethertype
  ep_tail_call(ctx, CILIUM_CALL_IPV4_FROM_OVERLAY) => tail_handle_ipv4
    handle_ipv4 #1
      ctx_get_tunnel_key #2
      lookup_ip4_endpoint #3
        map_lookup_elem
      ipv4_local_delivery #4
        ipv4_l3
        tail_call_dynamic #5
```

(1)：通过尾调用到达handle_ipv4

(2)：先解压，获得overlay包，获得bpf_tunnel_key

(3)：`lookup_ip4_endpoint` 会在 eBPF map `cilium_lxc` 中检查目标地址是否在当前节点中（这个 map 只保存了当前节点中的 endpoint）。

```shell
$ kubectl -n kube-system exec -ti cilium-gc8d8 -- cilium map get cilium_lxc | grep 10.42.0.51
10.244.2.171:0    id=2826  flags=0x0000 ifindex=29  mac=96:86:44:A6:37:EC nodemac=D2:AD:65:4D:D0:7B   sync
```

这里查到目标 endpoint 的信息：id、以太网口索引、mac 地址。在 NODE2 的节点上，查看接口信息发现，这个网口是虚拟以太网设备 `lxc65015af813d1`，正好是 pod `nginx` 接口 `eth0` 的对端。

(4)：`ipv4_local_delivery` 的逻辑位于 [`l3.h`](https://github.com/cilium/cilium/blob/1c466d26ff0edfb5021d024f755d4d00bc744792/bpf/lib/l3.h#L114) 中，先调用ipv4_l3()使用ep信息构建二层包。

(5)：然后这里会 tail-call 通过 endpoint 的 LXC ID（`29`）定位的 BPF 程序。

### node2的lxc egress方向发包

`lxc65015af813d1`上并没有加载`to-container`，结合上文，这里是直接由 vxlan 尾调用到 `to-container`，该程序可以在 [`bpf-lxc.c`](https://github.com/cilium/cilium/blob/1c466d26ff0edfb5021d024f755d4d00bc744792/bpf/bpf_lxc.c#L2131) 中找到。

```shell
handle_to_container
  ep_tail_call(ctx, CILIUM_CALL_IPV4_CT_INGRESS) 
  => tail_ipv4_ct_ingress
    ct_lookup4
    tail_ipv4_to_endpoint
      ipv4_policy #1
        policy_can_access_ingress
        redirect_ep
          ctx_redirect
```

(1)：`ipv4_policy` 会执行配置的策略。如果策略通过，会调用 `redirect_ep` 将网络包发送到虚拟以太接口 `lxc65015af813d1`，进入到 veth 后会直达与其相连的容器 `eth0` 接口。