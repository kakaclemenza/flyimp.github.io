---
layout: post
title: mptcp应用场景分析
category: system
---

<br>
### 一. 测试场景列举
#### 场景一: A->B 直连
假设客户端有 A-1 和 A-2 网卡, 服务端有 B 网卡. 切对于 `MPTCP_SCHEDULER` 设置为 **default**, 对于 `MPTCP_PATH_MANAGER` 设置为 **fullmesh**. 交互场景:
```
A --- B
```
1. 客户端要和服务端建立连接, 首先会找到第一块网卡 A-1 做连接, 如果 A-1 与 B 连接不成功, 直接报错, 不会去尝试 A-2.   
2. 网卡 A-1 与 B 三次握手成功, 如果之后没有发任何数据包, 是不会添加 subflow 的.   
3.  A-1 发送第一个数据包, 此时就会开始遍历所有网卡, 向目标 B 的**所有网卡**发起添加 subflow(这是 fullmesh 的作用). 发送的类型为 MP\_JOIN, 通过类似三次握手的过程, 就把有效的 subflow 添加了进来, 这里就是添加了 A-2 与 B 的连接  
4. 由于 `MPTCP_SCHEDULER` 设置为 default, 所以后续的发包正常情况下都是走 A-1 到 B 的连接  
5. 此时在 B 的防火墙拒绝接收 A-1 的包, 以模拟 A-1 到 B 断线. 此时客户端再发包, 就会按照 default 调度策略, 隔一小段时间之后, 自动选择了使用 A-2 到 B 的连接.  
6. 当然, 你也可以在客户端 A 处动态的增加网卡, 如可以先把 A-1, A-2 都禁用掉, 然后添加 A-3 网, 再去访问 B. 由于 fullmesh 设置, 内核会自动把 A-3 到 B 的subflow添加进来, 由于 default 策略, 一小段时间后就能使用 A-3 到 B 的链接正常进行收发包而不会中断.

**优点解析**   
1. 内核帮你按策略管理好了多条线路, 但是对外体现为一个 MPTCP 链接
2. 可以通过任意一个可用的 subflow 把内容传给服务端, 使得连接保持, 而不是重连

#### 场景二: A->Proxy->B
这里设定和"场景一"是一样的, 中间加了 Proxy 主要就是为了解决服务端 B 不支持具备 MPTCP 的内核的情况, 这时 Proxy 就要负责把收到的 MPTCP 包转化成普通的 TCP 包再发给 B. 交互场景如下:   
```
A-1 ---
	   |
	 Proxy --> B
	   |
A-2 ---
```
这里的 Proxy 只要是在 传输层 及以上层做代理的, 都能满足要求. 实验测试 haproxy 和 nginx 配置都能充当这里的 Proxy.   
另外, 用 nginx 做负载均衡也是没有问题的.

<br>
#### 场景三: A-1 -> Proxy1 -> B, A-2 -> Proxy2 -> B
Proxy1 和 Proxy2 是两个不同的代理服, 要求 A, B 支持 MPTCP 协议, Proxy1 和 Proxy2 可以不必支持 MPTCP 协议. 交互场景如下:
```
A-1 --- Proxy-1 ---
				   |
				   B
				   |
A-2 --- Proxy-1 ---
```
这种情况是可以实现的, 但实际上不是原生内核 MPTCP 提供的功能, 而是在利用 MPTCP 协议的基础上做了自定义改造, 限制如下:   
* 如果在应用层上实现, A 和 B 都需要自行组包和解包, 并自己对 subflows 做调度管理. 这个在本机已经做了实现证明, 在 [mptcp-scapy](https://github.com/nimai/mptcp-scapy) 这个开源代码的基础上修改测试得到实现.  
* 如果在内核层实现, 这里并没有做探究, 但是预计可以在已有 MPTCP 基础上做一些修改, 使得其对应用层暴露接口允许进行手动添加 subflow, 就能达到相同效果.

<br>
### 二. 总结
#### 局限性分析
1. 原生内核提供的功能, 需要多网卡的支持, 移动也就两个网卡, 端游客户端基本不会有双网卡的情况.
2. 依赖于客户端和服务端的内核支持: IOS11 支持, linux 4.14.24 支持, 其他都不支持
3. 潜在安全问题未知

#### 个人感想
MPTCP的思想是很好的, 如果能普遍应用, 对于线路稳定性提升应该很明显. 但是推进要求很高, 改动代价预估会很大. 作为技术储备是挺不错的, 如果要真正使用的话, 应该还会面临许多问题.

#### 展望
1. 可否修改内核, 实现客户端单网卡情况下, 通过多端口与多代理建立连接, 然后让 MPTCP 来管理这些的连接. ==> 问题就是各端对于改造内核的支持性问题
2. 应用场景不局限于移动端, 服务端到服务端的连接也是能用上的.
3. 也提供了一个思路, 可以在应用层实现对多条线路的智能管理, 实现对线路稳定性的控制. 参考 [mptcp-scapy](https://github.com/nimai/mptcp-scapy)

<br>
### 三. 编程指引
* 在[官网](https://multipath-tcp.org/pmwiki.php/Users/ConfigureMPTCP)上有提供如何设置内核 MPTCP 的不同调度策略和线路管理模式, 并且有详细的解释, 实现过程中必看. sysctl 或 setsockopt() 两种方法选一种即可
* 在 python 中也可以不加改造的使用 setsockopt() 进行设置, 只是由于没有头文件支持, 不能使用到如 `MPTCP_ENABLED` 这样的定义, 可以通过查找 `/usr/src/linux-headers-4.x.x.mptcp/include/linux/tcp.h` 这个文件对应的定义, 直接使用相应的数字即可.

### 四. 实际测试记录

### 原理与结论

##### 结论:

在 传输层 及以上层做代理的, 应该都可以成为 MPTCP 代理.

##### 原理:

从抓到的包的顺序来看, haproxy或类似的代理(nginx经测试也是可以的), 应该是在两个已经建立好的tcp连接中做数据包透传.

例如下面的这个: A -> Proxy -> B
连接①: A 与 Proxy 有一个 TCP 连接, 由于两边都支持 MPTCP, 所以这个就是一个 MPTCP 连接.
连接②: Proxy 与 B 有一个 TCP 连接, 由于 B 不支持 MPTCP, 所以这是一个 TCP 连接.

1. A 通过 ① 发一个数据包, haproxy 取出数据, 立即响应 A 已收到数据
2. 之后, haproxy 再把这个数据通过 ② 发给 B. B 收到后响应 haproxy. haproxy 确认 B 确实收到数据, 就继续按配置维持这条连接
   0 0 1-7 * 6 xxx.sh

### HAProxy 配置测试

1. 安装. 默认安装好会自动启动

```
sudo apt-get install haproxy
# 如果没启动, 自行启动
sudo /etc/init.d/haproxy start
```

1. 配置文件/etc/haproxy/haproxy.cfg:

```
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

	stats socket /var/lib/haproxy/stats

defaults
	log	global
	mode	tcp
	option tcplog
	option	dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000
	errorfile 400 /etc/haproxy/errors/400.http
	errorfile 403 /etc/haproxy/errors/403.http
	errorfile 408 /etc/haproxy/errors/408.http
	errorfile 500 /etc/haproxy/errors/500.http
	errorfile 502 /etc/haproxy/errors/502.http
	errorfile 503 /etc/haproxy/errors/503.http
	errorfile 504 /etc/haproxy/errors/504.http

listen test1
	bind :7000
	mode tcp
	server t1 192.168.140.104:7000

```

1. 配置完毕后重新载入配置即可:

```
sudo /etc/init.d/haproxy reload
```

### nginx 安装配置:

1. 编译安装:
   因为要配置 --with-stream 支持对 tcp 的代理, 只能通过源码编译安装

```
sudo apt-get install openssl libpcre3 libpcre3-dev
wget http://nginx.org/download/nginx-1.9.4.tar.gz
tar zxvf nginx-1.9.4.tar.gz
cd nginx-1.9.4
./configure  --prefix=/usr/local/nginx --user=www --group=www --with-http_stub_status_module --with-stream --with-http_ssl_module --with-http_gzip_static_module --with-http_realip_module --with-http_flv_module --with-http_mp4_module --with-pcre-jit
make
sudo make install
```

1. 配置 nginx : `sudo vi /usr/local/nginx/conf/nginx.conf`

```
user www-data;
worker_processes 4;
pid /var/run/nginx.pid;

error_log  logs/error.log;
error_log  logs/error.log  notice;
error_log  logs/error.log  info;


events {
    worker_connections  1024;
}

stream {
	upstream testtcp {
		#hash $remote_addr consistent;
		server 192.168.140.104:7000 weight=1 max_fails=3 fail_timeout=10s;
		server 192.168.140.102:7000 weight=1 max_fails=3 fail_timeout=10s;
		server 192.168.140.105:7000 weight=1 max_fails=3 fail_timeout=10s;
	}
	server {
		listen 7000;
		proxy_pass testtcp;
	}
}
```

1. 启动 nginx

```
sudo /usr/local/nginx/sbin/nginx
```

就完成了对 192.168.140.104:7000 的代理

### 通过网关配置的测试

通过网关配置被证实是不可行的, 以下总结一下学到的网关配置知识:

1. 对于特定网卡 eth0 特定地址 192.168.140.103 的所有包都指定其网关为 192.168.140.101:

```
route add -net 192.168.140.0/24 gw 192.168.140.101
```

1. 配置完成后, 可以通过 traceroute 测试一下是不是经过 192.168.140.101

```
traceroute 192.168.140.104
```

1. 实验完成后, 复原:

```
route del -net 192.168.140.0/24 gw 192.168.140.101
```

1. 当然也可以设置默认网关:

```
# 增
route add default gw 192.168.56.101
# 删
route del default gw 192.168.140.101
```

更详细可参考[链接](