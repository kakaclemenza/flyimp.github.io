---
layout: post
title: xxmysdn总体设计
category: net
tag: sdn
typora-root-url: ../../..
---



### 一. xxmysdn总体结构

![xxmysdn组件](/img/sdn/xxmysdn_structure.png)

xxmysdn包含如下组件:

* xxmysdn_cli: 使用go语言编写的简单xxmysdn接入客户端. 完成如下功能:
  * 找controller获取接入ap列表
  * 探测ap列表, 获得最快返回的ap
  * 使用该ap和用户传入的需要走xxmysdn的网段, 设置本地路由规则
  * 开启AP保活测试. 测试失败会重走接入流程
* ap: xxmysdn网络接入点, 兼xxmysdn网络交换节点
* controller: xxmysdn网络控制器

组件间交互流程为:

1. ap1~ap3构建gre隧道两两互连, 搭建overlay网络
2. ap1~ap3均与controller建立连接, controller会下发流表规则来控制ap1~ap3的转发路径
3. client调用xxmysdn_cli来指定接入xxmysdn网络的ip段
4. xxmysdn_cli会先找controller获取接入ap列表, 并接入测速最优的ap1
5. client后续对该ip段的新连接流量, 就会走xxmysdn网络, 最终由同机房的ap2转发给server



### 二. 转发细节

为了更生动直观的说明转发的细节, 我们将自己当作是一个udp数据包pkt, 说明从client开始通过xxmysdn网络到达server的完整过程. 我们假设:

* client的ip为: 10.1.0.2/16
* server的ip为: 10.2.0.2/16
* ap1的ip为: 10.1.0.1/16, overlay ip为: 240.0.0.1/16, overlay mac为: 00:00:00:00:00:01
* ap2的ip为: 10.2.0.1/16, overlay ip为: 240.0.0.2/16, overlay mac为: 00:00:00:00:00:02
* ap3的ip为: 10.3.0.1/16, overlay ip为: 240.0.0.3/16, overlay mac为: 00:00:00:00:00:03
* pkt的源ip和目标ip为: 10.1.0.2 -> 10.2.0.2

netfilter对数据包的处理流程参考:

![](/img/kernel/Netfilter-packet-flow.svg)

#### (1) client发出了pkt

client在调用xxmysdn_cli成功接入ap1后, client本地的iptables规则如下:

```shell
*mangle
-A PREROUTING -m state --state NEW -j MARK --set-mark 100
-A PREROUTING -m state --state NEW -j CONNMARK --set-mark 100
-A PREROUTING -m connmark --mark 100 -j CONNMARK --restore-mark
-A INPUT -m state --state NEW -m connmark --mark 100 -j CONNMARK --set-mark 0
-A OUTPUT -m state --state NEW -j MARK --set-mark 100
-A OUTPUT -m state --state NEW -j CONNMARK --set-mark 100
-A OUTPUT -m connmark --mark 100 -j CONNMARK --restore-mark
```

这里有一点要说明下: **mangle表中的动作MARK和CONNMARK, 在处理完后, 会继续处理同一条链的其余规则, 而不像大多数动作一样处理完后直接跳到下一跳链.** 所以在PREROUTING链中能同时对数据包和连接打上标记100 

client本地的路由规则如下:

```shell
# ip rule show
...
from all fwmark 0x64 lookup 100
...
# ip r l t 100
10.2.0.0/16 via 10.1.0.1 dev eth0
10.3.0.0/16 via 10.1.0.1 dev eth0
```

client发出pkt, 在到达OUTPUT链时, 判断到该连接为新连接, 会给pkt以及该连接都打上标记100. 过了OUTPUT链后, 会进行重路由, 此时策略路由匹配到路由表100中的这条路由:

> 10.2.0.0/16 via 10.1.0.1 dev eth0

于是pkt会发到ap1, 由ap1进行转发.



#### (2) ap1转发pkt

ap1初始化后, 本地iptables规则如下:

```shell
*nat
-A POSTROUTING -j MASQUERADE

*mangle
-A PREROUTING -p tcp -m state --state NEW -j MARK --set-mark 100
-A PREROUTING -p tcp -m state --state NEW -j CONNMARK --set-mark 100
-A PREROUTING -p tcp -m connmark --mark 100 -j CONNMARK --restore-mark
-A INPUT -p tcp -m state --state NEW -m connmark --mark 100 -j CONNMARK --set-mark 0
-A PREROUTING -p udp -m state --state NEW -j MARK --set-mark 100
-A PREROUTING -p udp -m state --state NEW -j CONNMARK --set-mark 100
-A PREROUTING -p udp -m connmark --mark 100 -j CONNMARK --restore-mark
-A INPUT -p udp -m state --state NEW -m connmark --mark 100 -j CONNMARK --set-mark 0
-A POSTROUTING -m mark --mark 100 -j MARK --set-mark 0
```

ap1上的路由规则如下:

```shell
# ip rule show
...
from all fwmark 0x64 lookup 100
...
# ip r l t 100
10.2.0.0/16 via 240.0.0.2 dev br0
10.3.0.0/16 via 240.0.0.3 dev br0
```

pkt到达ap1后, 在PREROUTING链会判断为新连接, 所以会给pkt以及该连接都打上标记100. 之后进行路由判断, 此时策略路由匹配到路由表100中的这条路由:

> 10.2.0.0/16 via 240.0.0.2 dev br0

于是pkt会经过FORWARD链直接到达POSTROUTING链, 在POSTROUTING链中会对pkt的标记做清理并进行MASQUERADE, 然后pkt就会通过br0网卡发送出去. 此时pkt的ip信息和mac信息如下(这里忽略了arp请求的过程):

```shell
# ip头信息
240.0.0.1 -> 10.2.0.2
# mac头信息
00:00:00:00:00:01 -> 00:00:00:00:00:02
```

br0网卡连接在ap1上ovs创建的虚拟网桥的LOCAL端口, 从br0上发出的数据包, 会从LOCAL端口进入虚拟网桥, 再由虚拟网桥的流表规则决定如何处理数据包. 目前ap1上虚拟网桥的流表如下:

```shell
..., priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER
..., priority=65535,arp,in_port=LOCAL,dl_dst=ff:ff:ff:ff:ff:ff actions=ALL
..., priority=1,arp,dl_dst=ff:ff:ff:ff:ff:ff actions=LOCAL
..., priority=1,dl_src=00:00:00:00:00:01,dl_src=00:00:00:00:00:02 action=output:2
..., priority=1,dl_src=00:00:00:00:00:01,dl_src=00:00:00:00:00:03 action=output:3
..., priority=1,dl_src=00:00:00:00:00:02,dl_src=00:00:00:00:00:01 action=LOCAL
..., priority=1,dl_src=00:00:00:00:00:03,dl_src=00:00:00:00:00:01 action=LOCAL
```

pkt匹配到如下流表:

> ..., priority=1,dl_src=00:00:00:00:00:01,dl_src=00:00:00:00:00:02 action=output:2

于是从虚拟网桥的2号端口发出, 此端口在ap1初始化是已经通过gre隧道协议连接着ap2的1号端口. 于是pkt被加上gre头部, 并使用标准ip层发包流程(ip_local_out)发送封装好的ip包. 我们将此时的数据包记作gre-pkt. gre-pkt的ip头信息如下:

```shell
# gre-pkt的ip头信息
10.1.0.1 -> 10.2.0.1
# pkt的ip头信息
240.0.0.1 -> 10.2.0.2
```

此时的gre-pkt会先进行路由判断, 由于我们在前面的**POSTROUTING链已经将gre-pkt的标记100清除掉了**, 所以这里不会匹配路由表100, 而会直接匹配系统main路由表. 这时gre-pkt会从eth0网卡发送出去



#### (3) ap2转发pkt

ap2初始化后, 本地iptables规则如下:

```shell
*nat
-A POSTROUTING -j MASQUERADE

*mangle
-A PREROUTING -p tcp -m state --state NEW -j MARK --set-mark 100
-A PREROUTING -p tcp -m state --state NEW -j CONNMARK --set-mark 100
-A PREROUTING -p tcp -m connmark --mark 100 -j CONNMARK --restore-mark
-A INPUT -p tcp -m state --state NEW -m connmark --mark 100 -j CONNMARK --set-mark 0
-A PREROUTING -p udp -m state --state NEW -j MARK --set-mark 100
-A PREROUTING -p udp -m state --state NEW -j CONNMARK --set-mark 100
-A PREROUTING -p udp -m connmark --mark 100 -j CONNMARK --restore-mark
-A INPUT -p udp -m state --state NEW -m connmark --mark 100 -j CONNMARK --set-mark 0
-A POSTROUTING -m mark --mark 100 -j MARK --set-mark 0
```

ap2上的路由规则如下:

```shell
# ip rule show
...
from all fwmark 0x64 lookup 100
...
# ip r l t 100
10.1.0.0/16 via 240.0.0.1 dev br0
10.3.0.0/16 via 240.0.0.3 dev br0
```

gre-pkt到达ap2后, 首先在PREROUTING链会判断为新连接, 所以会给gre-pkt以及该连接都打上标记100. 之后进行路由判断, 此时策略路由匹配到的会是系统local路由表, gre-pkt会直接经过INPUT链, 在INPUT链这里, 会断该连接清除标记100.

之后gre-pkt由内核的gre协议模块处理, 去除gre头部, 还原出pkt, 并重新调用链路层接收函数接收pkt. pkt会先经过ovs虚拟网桥来匹配流表. ap2上虚拟网桥的流表如下:

```shell
..., priority=65535,dl_dst=01:80:c2:00:00:0e,dl_type=0x88cc actions=CONTROLLER
..., priority=65535,arp,in_port=LOCAL,dl_dst=ff:ff:ff:ff:ff:ff actions=ALL
..., priority=1,arp,dl_dst=ff:ff:ff:ff:ff:ff actions=LOCAL
..., priority=1,dl_src=00:00:00:00:00:02,dl_src=00:00:00:00:00:01 action=output:1
..., priority=1,dl_src=00:00:00:00:00:02,dl_src=00:00:00:00:00:03 action=output:3
..., priority=1,dl_src=00:00:00:00:00:01,dl_src=00:00:00:00:00:02 action=LOCAL
..., priority=1,dl_src=00:00:00:00:00:03,dl_src=00:00:00:00:00:02 action=LOCAL
```

pkt匹配到如下流表:

> ..., priority=1,dl_src=00:00:00:00:00:01,dl_src=00:00:00:00:00:02 action=LOCAL

于是pkt会通过ap2上的br0网卡的接收函数输入到netfilter框架中. 在PREROUTING链会判断为新连接, 所以会给pkt以及该连接都打上标记100. 之后进行路由判断, 这时, 在路由表100中无法匹配到路由, 于是会继续匹配系统mian路由表. 由于ap2已经和server在同一个网段, 这时就会匹配到这样一条路由:

> 10.2.0.0/16 dev eth0 proto kernel scope link src 10.2.0.1

于是pkt会经过FORWARD链直接到达POSTROUTING链, 在POSTROUTING链中会对pkt的标记做清理并进行MASQUERADE, 然后pkt就会通过eth0网卡发送出去. 此时pkt的ip信息如下:

```shell
# ip头信息
10.2.0.1 -> 10.2.0.2
```



#### (4) 后续流程

server收到pkt后, 会进行处理. 假设会进行响应, 我们记响应包为repkt. repkt会通过如下过程回到client:

```shell
server -> ap2 -> ap1 -> client
```

后续的流程和前面分析的基本相同, 唯一不同的地方在于: ap2和ap1在收到repkt时, 由于此时的连接已经不是NEW状态, 所以会匹配到这条iptables规则:

> -A PREROUTING -p tcp -m connmark --mark 100 -j CONNMARK --restore-mark

于是同样会给repkt数据包打上标记100, 来进入路由表100进行路由匹配.



### 三. 部署细节

#### (1) conntroller部署

游戏维护平台脚本

#### (2) ap部署

游戏维护平台脚本



### 四. 接入细节

下载[xxmysdn_cli](http://dl.ops.2980.com/xxmygwvpn/linux/vpn_debug/xxmysdn_cli)命令行工具, 执行如下命令接入即可接入xxmysdn:

```shell
sudo ./xxmysdn_cli -r 10.2.0.2/32,10.2.0.3/32 1>/dev/null 2>&1 &
```

命令日志输出和执行参数参考:

```shell
# ./xxmysdn_cli -h
Usage of ./xxmysdn_cli example: sudo ./xxmysdn_cli -r 10.10.10.0/24
  -h	show help info
  -n int
    	max retry count while failed (default -1)
  -o string
    	the output dir of log (default "/tmp/xxmysdn_cli")
  -r string
    	the ip/net info want to route to xxmysdn
```

要取消接入xxmysdn, 直接终止进程即可, 内部会自动清理路由和iptables规则:

```shell
sudo pkill xxmysdn_cli
```



### 五. 网络拓补设计

xxmysdn的网络拓补图为强连通图, 选路算法使用遗传算法中的随机法来高效得到近似最优解. 当前本地虚机能实现200个节点0.38s左右的时间内得出结果(c++实现)

在该xxmysdn网络中, 所有节点依赖gre隧道组成大二层虚拟网络. 网络的设置遵循如下规范:

* 网络中每个交换节点同时也是接入节点
* 每个交换节点的LOCAL端口绑定到本地虚拟网卡br0
* 每个交换节点绑定唯一的dpid, dpid会从1开始增加, 最多容纳65535个dpid
* 虚拟网卡br0的ip地址, mac地址都按照dpid来生成, 生成规则如下:
  * ip: `240.0.0.0 + dpid`
  * ip_mask: `/8`
  * mac: `00:00:00:00:00:00 + dpid`
* 某个交换节点A与其他交换节点B的gre连接端口号, 设置为B的bsid

xxmysdn网络中可能存在多ip的交换节点. 在构建gre隧道时就需要选择两端使用哪个ip, ip的选择顺序如下:

1. 如果两个ip不在GFW同一侧, 则不建立连接
2. 如果两个ip具有同运营商, 使用同运营商ip相连
3. 如果两个ip不具备同运营商, 使用第一个ip相连 => **TODO: 待讨论**

基于如上的规则, xxmygw最终的网络拓补如下:

![xxmysdn组件](/img/sdn/xxmysdn_topo.png)

我们在配置文件的`topo`项中定义网络拓补, 如下:

```json
...
"topo": {
    "//": "格式为: dpid: ip列表",
    "1": ["1.1.1.1"],
    "2": ["2.2.2.1", "2.2.2.2"]
}
...
```

附注: 配置文件中的key, 如果是数字含义的字符串, 在json反序列化后都会立即转为整形.



### 六. 流表设计

xxmysdn的流表按功能分成如下几大块:

| 功能说明     | priority | match                                                        | action         |
| ------------ | -------- | ------------------------------------------------------------ | -------------- |
| lldp包统计   | 65535    | "dl_dst": "01:80:c2:00:00:0e", "dl_type": 35020              | 发给控制器     |
| 支持arp广播  | 65535    | "in_port": "LOCAL", "dl_dst": "ff:ff:ff:ff:ff:ff", "dl_type": 2054 | 广播到其他端口 |
|              | 65534    | "dl_dst": "ff:ff:ff:ff:ff:ff", "dl_type": 2054               | 发给LOCAL端口  |
| 支持vrrp组播 | 65535    | "in_port": "LOCAL", "dl_dst": "01:00:5E:00:00:12", "ip_proto": 112 | 发到组表0      |
|              | 65534    | "dl_dst": "01:00:5E:00:00:12", "ip_proto": 112               | 发给LOCAL端口  |
| 选路流表     | 1        | ...                                                          | ...            |

ap节点会自动按同机房信息分组得到组信息, 同组的ap之间互为主备, 以防止单点故障. 主备切换用到了vrrp协议组播, vrrp组播会用到组表, **组表会按组信息自动进行配置**. 

组表0配置如下:

```shell
# 如果vrrp组播组中只有一个成员
group_id=0,type=all,bucket=actions=drop

# 如果vrrp组播组中有大于一个成员
group_id=0,type=all,bucket=actions=output:2,...
```



### 路由设计

xxmysdn中的ap节点会按同机房性质组成一个**路由组**, 每个路由组唯一对应一个**路由组id**, 记作`rtGid`. 路由组中的每个ap节点都会运行如下命令:

```
sudo vrrpd -D -n -v 1 -i br0 -p 100 240.1.0.1
```

该命令将同一路由组的所有ap组合起来, 共同维护一个高可用的虚拟ip: `240.1.0.1`, 我们称之为**路由组ip**, 记作`rtGip`, 生成规则是

> rtGip = 240.1.0.0 + rtGid

在配置文件的`route`项配置路由组信息, 如下:

```json
...
"route": {
    "//": "格式为: rtGid: [dpid列表, 这个组能转发的同机房网段列表]",
    "1": [[1,2], ["1.1.1.0/24"]],
    "2": [[3,4,5], ["2.2.1.0/24","2.2.2.0/24"]]
}
...
```

根据上面的配置信息, 就可以为每个ap节点确定其100路由表如何配置了. 规则如下:

* 对某个ap节点, 要配置到所有其他路由组的所有网段的路由
* 路由的网关就是该路由组的 rtGip

那么按照上面的`route`配置项, 我们在各个ap节点的路由配置就会是:

```shell
# dpid:1的路由表100配置
2.2.1.0/24 via 240.1.0.2 dev br0
2.2.2.0/24 via 240.1.0.2 dev br0

# dpid:2的路由表100配置
2.2.1.0/24 via 240.1.0.2 dev br0
2.2.2.0/24 via 240.1.0.2 dev br0

# dpid:3的路由表100配置
1.1.1.0/24 via 240.1.0.1 dev br0

# dpid:4的路由表100配置
1.1.1.0/24 via 240.1.0.1 dev br0

# dpid:5的路由表100配置
1.1.1.0/24 via 240.1.0.1 dev br0
```

