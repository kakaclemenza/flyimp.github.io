---
typora-root-url: ../../..
---



## 前言

写这篇文章, 是因为和同事探讨xxmysdn_yw性能提升方案时, 一些设置调整导致性能变化较大, 无法获得合理解释, 所以只能深挖到内核实现原理来分析. 首先我们需要了解两种功能的实现原理: 隧道和网桥

### 隧道(gre)

这里隧道就以最通用的gre为例, gre隧道实现源码我们之前也有略微分析过, 这里阅读内核代码, 更主要就是了解清楚它的发包和收包两个方向的行为. 其他类型的隧道如vxlan都大同小异:

* 发包:  协议栈发出数据包, 路由判断应该从gre虚拟网卡发出, 最终数据包会发到网卡上, 调用网卡的`ndo_start_xmit()`函数发包; 这里就是`net/ipv4/ip_gre.c::ipgre_netdev_ops`注册的`ipgre_tunnel_xmit()`, 该函数对数据包做gre协议封装, 最终调用`ip_local_out()`发出数据包, 此时数据包按路由会从underlay网卡发出

  ```shell
  发包 -> gre0网卡发包函数 -> ipgre_tunnel_xmit() -> ip_local_out() -> eth0发包函数 -> 网络
  ```

* 收包: 协议栈收到数据包, 正常解析到ip层`ip_local_deliver_finish()`, 发现是gre协议, 于是通过协议注册信息找到了`net/ipv4/gre.c::gre_rcv()`进行处理, 按gre版本找到`net/ipv4/ip_gre.c::ipgre_rcv()`处理cisco gre类型隧道包, 解隧道封装还原出overlay包, 设置**接收设备为gre虚拟网卡**, 然后调用`netif_rx()`正常接收数据包, 相当于模拟gre0网卡收到数据包

  ```shell
  收包 -> eth0网卡收包函数 -> ip_local_deliver_finish() -> gre_rcv() -> ipgre_rcv() -> netif_rx()
  ```



### 网桥(bridge)

#### 网桥设备的创建和从设备添加

创建网桥, 实际调用`net/bridge/br_if.c::br_add_bridge()`, 添加设备时调用`net/bridge/br_device.c::br_dev_setup()`, 这里注册了一个`br_netdev_ops`, 将网桥的发包函数指定为`br_dev_xmit()`



#### 网桥设备下添加从设备

网桥设备创建完成后需要在其下面添加从设备才能使网桥设备正常工作起来, 添加从设备的主要任务在`net/bridge/br_if.c::br_add_if()`函数中完成, 其中最重要的两个任务是:

* 调用`dev_set_promiscuity()`函数设置从设备为混杂模式
* 调用`netdev_rx_handler_register`函数将从设备的`dev->rx_handler`赋值为`br_handle_frame()`函数, 这使得从设备接受到的所有数据包实际接收到了网桥



#### 网桥收包

链路层接收数据包处理函数`net/core/dev.c::__netif_receive_skb()`中, 有许多我们熟知的关键操作都是在这个函数处理的, 如:

* 处理XDP逻辑(较新的内核才有)
* 处理 ptype_all 上所有的 packet_type->func(), 典型的有使用PF_PACKET协议的tcpdump抓包
* 处理tc流控逻辑
* 处理vlan标记
* 处理rx_handler函数, 网桥收包就是在这里处理
* 处理ptype_base上所有的 packet_type->func(), 数据包传递给上层协议层处理, 例如ip_rcv函数

对于所有加入网桥中的网卡, 网卡收包都会通过`rx_handler()`函数进一步调用`net/bridge/br_input.c::br_handle_frame()`, `br_handle_frame()`主要数据包的合法性, 处理特殊特殊的mac地址, 正常就调用`br_handle_frame_finish()`决定数据包的前送或者上传给内核; 

* 如果是本地数据包则调用`br_pass_frame_up`函数回到`__netif_receive_skb`函数中继续处理
* 如果是前送数据包则调用`br_forward`函数, 最终在`br_dev_queue_push_xmit`函数中调用`dev_queue_xmit`函数下发给指定端口将数据包发送出去

#### 网桥发包

网桥设备对于内核来说就是一个普通的网卡, 当有数据需要发送时内核调用函数指针`ndo_start_xmit()`, 即调用网桥设备向内核注册的`br_dev_xmit()`函数中. 在此函数中首先检查数据包是否是广播/多播数据包:

* 是, 则将数据包下发给所有端口
* 不是, 则调用`__br_fdb_get()`函数查找需要下发的端口, 如果找到则使用`br_deliver()`下发给指定端口, 最终通过调用`dev_queue_xmit()`将数据从该端口发送出去: 
* 如果`__br_fdb_get()`函数没有找到, 则下发给所有端口



## ovs网桥转发包分析

### ovs封装underlay包

![](/img/sdn/xxmysdn_yw_packet_encap_realize.png)

具体流程: 

1. 数据包进入eth0, eth0收包
2. 网桥`br_handle_frame()`判断数据包使用`br_forward`转发到vx0 
3. vx0网卡发出数据包, 调用vxlan封装函数
4. 封装好的数据包从p0网卡发出, 调用网桥`br_dev_xmit()`函数发包, 判断数据包需要下发到gre0端口发出
5. gre0网卡发出数据包, 调用gre封装函数
6. 封装好的数据包再从br0网卡发出, 调用网桥`br_dev_xmit()`函数发包, 判断数据包需要下发到eth0
7. eth0 网卡发出数据包, 数据包进入网络

### ovs接收overlay包

![](/img/sdn/xxmysdn_yw_packet_decap_realize.png)

具体流程: 

1. 数据包进入eth0, eth0收包
2. 网桥`br_handle_frame()`判断数据包为本地数据包, 将数据包接收设备设置为网桥自己, 并调用`br_pass_frame_up()`继续协议栈收包
3. 调用gre解封函数`ipgre_rcv()`, 调用`netif_rx()`接收解封数据包
4. 网桥`br_handle_frame()`判断数据包为本地数据包, 将数据包接收设备设置为网桥自己, 并调用`br_pass_frame_up()`继续协议栈收包
5. 调用vxlan解封函数, 调用`netif_rx()`接收解封数据包
6. 网桥`br_handle_frame()`判断数据包为前送数据包, 调用`br_forward()`将数据包下发到指定端口
7. eth0 网卡发出数据包, 数据包进入网络



