---
layout: post
title: xxxsdn应用方案
category: net
typora-root-url: ../../..
---

## 一. 私网转发

### dyp2p trans转发

方案如下:

1. trans服初始化: 向AS服获得最近的AP(一般都会使同机房), 将自己到其他trans服的**下一跳路由配置为AP**
2. trans1服收到转发请求, 正常发给对应的trans2服. 此时数据包会根据路由规则来到AP1, AP1通过sdn网络将数据包路由到trans2服同机房的AP2, AP2再将数据包转给trans2服即可



讨论的问题:

* [ ] 客户端接入的ap如果故障了, 如何切换? : 设计sdk命令行工具, 自行保活与切换, 用户不用关心
* [x] xxmysdn内部是否实现了最优选路? : 已实现
* [x] xxmysdn内部是否能自动负载均衡 : 目前能基于丢包和延迟每5s决策切换线路. 多路负载均衡依赖于前k条最短路径的选路算法, 需要进一步探索
* [x] 完善的线路监控: 目前已经实现对于线路的延迟, 丢包, 流量每1s监控

### xxmygw转发

xxmygw的PS节点目前自然组成一个sdn网络, 可以直接添加其他机房私网网段的路由, 让数据包进入sdn网络即可. 方便灰度使xxmysdn转发.

也可以同样将**下一跳路由配置为AP**, 保证xxmysdn和xxmygw相互独立.

### 语音服转发

方案同"dyp2p trans转发"



### ...

<br>

## 二. 客户端接入

### vnet方案实现

详见vnet概要设计



### ...



<br>

## 三. 其他设计问题

### 监控设计

1. sdn网络内部以实现实时LLDP包探测, 网络内部各个link, path都有每秒监控. 路径决策算法会每5s进行一波决策, 可以调节
2. 监控数据可以推送到大数据, 进一步统计分析



### 容灾设计

1. AP节点目前都是单点, 大流量下单点性能没有评测过, 理论上是走内核转发, 目前的场景不需要担心. 

   解决: 若单点路由性能不行, 则需要用多点配置策略路由, 进行负载均衡

2. AP节点如果故障, 即使是多点负载均衡, 也会出现部分转发异常. 

   解决: 可以使用vrrp漂移ip

3. CC节点目前是单点, 若CC节点故障, ovs上流表不会改变, 仍能正常工作, 但是无法实时调整转发路径.

   解决: CC多点需要进行设计, 主要是解决监控数据的同步问题.



### 性能设计

1. AP节点及中间交换节点的转发都在内核, 理论上能接近(千兆)网卡线速. 

   解决: 如果需要更大的流量, 可以引入dpdk等技术, 在用户态直接控制网卡; 用p4编写交换节点也是一个方向.

2. CC节点使用的是python语言的ryu控制器, 如果连接的AP过多, 在实时计算转发路径时可能会存在瓶颈. 

   解决: 可以用更高效的语言自研sdn控制器, 比如用go来实现openflow协议, 监控和计算逻辑



<br>

## 四. 推进计划

### 可用阶段[01-16 ~ 02-07]

* 晓风:
  * [x] 调整AP部署脚本, 重新规范路由配置流程
  * [x] 完善CC日志监控和流表下发逻辑
  * [x] 正式环境AP节点调整: 由于会操作路由和流流表, 需申请新的节点单独部署, 避免影响到xxmygw
  * [x] 尝试将xxmygw部分PS私网转发路由进xxmysdn网络, 测试下效果.
  * [x] 总结好接入流程文档
  * [x] 通知引擎dyp2p安排测试和接入体验
* 苏度, 周一:
  * [x] 入门sdn网络, 掌握xxmysdn实现原理
  * [x] 多做记录和总结, 完善sdn网络入门流程文档
  * [x] 多做相关技术讨论, 加深团队在sdn上的技术积累.

### 稳定阶段

* [x] 监控数据完善, 统计
* [x] 按需重构CC各模块逻辑
* [x] 探究vrrp应用在AP节点的容灾方案.
* [x] 实现CC多点改造, 或将计算任务分布式

### 高效阶段

* [ ] AP节点上容器云
* [ ] AP节点使用dpdk提高单点转发效率
* [ ] CC使用go提升单点性能

### 应用场景推广阶段

再做讨论





### xxmysdn接管运维网络

