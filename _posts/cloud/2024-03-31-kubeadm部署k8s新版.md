---
layout: post
title: kubeadm部署k8s新版
category: cloud
typora-root-url: ../..
---

## 〇、集群规划

当前使用的是k8s最新稳定版: v1.24.3

本地环境我们预计使用3台2核2G虚拟机，搭建一个1master-2node集群，各虚机节点地址如下：

```shell
192.168.56.109 master
192.168.56.105 node1
192.168.56.106 node2
```

为了方便操作，我们先使用一台debian11.3虚机（已部署好docker、使用samba共享目录）来作为master安装配置好环境，另外2台node节点则可以直接使用virtualbox的“复制-链接复制”模式快速创建，最小化3台虚机所需的重复配置操作！

## 一、初始系统配置

由于 k8s 集群中的每个 node 的 hostname 不能一样，所以需要确保虚拟机的 hostname 不同，首先设置新主机名：

```shell
hostnamectl set-hostname master
```

然后修改`/etc/hosts`，确保域名解析正确：

```shell
127.0.0.1       localhost
192.168.56.109 master
192.168.56.105 node1
192.168.56.106 node2
...
```

> 注意：如果没有修改/etc/hosts，使用`sudo`指令会由于解析不到master而超时报错

接着是一系列系统配置：

```bash
#加载默认的内核模块
cat << EOF > /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter

#正确设置系统参数
cat << EOF > /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
user.max_user_namespaces=28633
vm.swappiness=0
EOF
sysctl -p

#关闭swap
swapoff -a
sed -ri "/swap/s|^(.*)|#\1|g" /etc/fstab
#确认swap已经关闭
free -m
```

然后，我们需要配置containerd，它是容器运行时，在v1.24.0之后作为k8s默认的容器运行时（不再支持docker-cli）。kubelet启动容器的发展历程摘抄如下：

> 早期: kubelet --> docker-manager --> docker
> 中期: kubelet -CRI-> docker-shim --> docker --> containerd --> runc
> 中期: kubelet -CRI-> cri-containerd --> containerd --> runc
> 当前: kubelet -CRI-> containerd(CRI plugin) --> runc
> 当前: kubelet -CRI-> cri-o --> runc

在安装docker-ce时，安装的依赖`container.io`就包含它，所以不需要再进行安装，直接配置：

```shell
#生成containerd的配置文件
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml
```

根据文档[Container runtimes](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) 中的内容，对于使用systemd作为init system的Linux的发行版，使用systemd作为容器的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定，因此这里配置各个节点上containerd的cgroup driver为systemd。

修改前面生成的配置文件`/etc/containerd/config.toml`，配置sandbox_image调整pause地址（使用公司地址），并配置SystemdCgroup使用systemd作为容器的cgroup driver

```shell
[plugins."io.containerd.grpc.v1.cri"]
  ...
  sandbox_image = "hub.2980.com/gcr/google_containers/pause:3.7"
  ...
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
      ...
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
        SystemdCgroup = true
```

然后重启并测试：

```shell
systemctl daemon-reload
systemctl restart containerd
# 测试，确保可以打印出版本信息并且没有错误信息输出
crictl version
#检查containerd日志
systemctl status containerd.service --no-pager -l

#解决crictl命令执行报错`crictl error while dialing dial unix /var/run/dockershim.sock`
crictl config runtime-endpoint unix:///var/run/containerd/containerd.sock
```

crictl可以平替docker（实际是docker-cli）这个指令，大多数参数用法是一样的，参考：[使用 crictl 对 Kubernetes 节点进行调试](https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/crictl/)

## 二、部署kubernetes

安装：(使用公司地址)

```shell
echo "deb [trusted=yes] http://mvn-apt.2980.com/repository/aliyun/kubernetes/apt kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
curl -s http://mvn-apt.2980.com/repository/aliyun/kubernetes/apt/doc/apt-key.gpg | apt-key add -
apt update
apt install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl #  标记软件包不被自动更新
```

安装完成后，使用`kubeadm config print init-defaults --component-configs KubeletConfiguration > kubeadm.yml`可以打印集群初始化默认的使用的配置到kubeadm.yml文件，调整该文件如下：

```shell
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  # 修改成master的IP
  advertiseAddress: 192.168.56.109
  bindPort: 6443
nodeRegistration:
  # 设置了容器运行时为containerd
  criSocket: unix:///run/containerd/containerd.sock
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.24.3
# 定制了imageRepository为公司的registry
imageRepository: hub.2980.com/gcr/google_containers
networking:
  podSubnet: 10.244.0.0/16
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
# 设置kubelet的cgroupDriver为systemd
cgroupDriver: systemd
failSwapOn: false
```

在开始初始化集群之前可以使用预先拉取所k8s需要的容器镜像：

```shell
#拉取镜像
kubeadm config images pull --config kubeadm.yml
#检查拉取的镜像
crictl images
```

最后，由于我们虚机的多张网卡，k8s节点间通信使用的host-only网卡，这里需要显式让kubelet使用host-only网卡IP：

```shell
echo 'KUBELET_EXTRA_ARGS="--node-ip=192.168.56.109"' > /etc/default/kubelet
#如果直接修改/etc/systemd/system/下的配置，需要重载入配置
#systemctl daemon-reload
#重启服务
systemctl restart kubelet
#检查kubelet进程是否加入--node-ip参数
ps aux | grep kubelet | grep --color node-ip
```

## 三、克隆工作节点

到这一步，需要对所有节点进行安装或配置的工作就完成了，接下来就可以复制master节点生成node1、node2节点。

首相，我们写一个脚本，方便快速对node1、node2的必要配置做更改：

```shell
#!/bin/bash
#filename: init_node.sh

if [ "$(whoami)" = "root" ]; then
    echo "need to run as root"
    exit 1
fi
if [ $# -lt 1 ]; then
    echo "usage: init_node.sh <node_name>"
    exit 1
fi

NODENAME=$1
#调整hostname
hostnamectl set-hostname $NODENAME
sed -i "/^127/ s|master|${NODENAME}" /etc/hosts
#调整kubelet通信ip
nodeIp=$(ip a | grep inet | grep -v 127.0.0.1 | grep -v inet6 | awk '{print $2}' | cut -d / -f1 | grep -e '^192.168')
sed -i "s/192.168.56.109/${nodeIp}" /etc/default/kubelet
```

然后，我们克隆出node1、node2：

![](C:\Users\admin\fun\assets\2022-08-17-09-59-06-image.png)

![](C:\Users\admin\fun\assets\2022-08-17-10-00-34-image.png)

![](C:\Users\admin\fun\assets\2022-08-17-10-01-20-image.png)

## 四、配置集群

master节点上执行：

```shell
kubeadm init --config kubeadm.yml
```

等待其初始化完成，如果命令行打印报错，检查kubelet.service运行日志：

```shell
journalctl -xeu kubelet --no-pager
```

排查问题后，重置kubeadm并重新进行初始化：

```shell
kubeadm reset
#重试
kubeadm init --config kubeadm.yml
```

默认init了之后，会提示将kubectl的配置文件拷贝到用户目录，以及打印出来将工作节点加入集群的指令

```shell
 # 拷贝kubectl配置文件
 mkdir -p $HOME/.kube/
 cp /etc/kubernetes/admin.conf $HOME/.kube/config
```

后续也可以打印工作节点添加命令：token有效期默认是24小时，所以每次使用create新建一个token来打印才是确保有效的。

```shell
kubeadm token create --print-join-command
```

在node1，node2上分别运行以上命令输出的join命令，即可加入k8s集群。然后在master上查看相关信息：

```shell
kubectl get nodes -o wide --all-namespaces
kubectl get pod -o wide --all-namespaces
```

如果要更新节点yaml配置，有以下方式：

```shell
# 如果修改的资源有配置灰度更新，直接apply之后由kubenetes进行自动更新
kubectl apply -f xxx.yaml

# 需要手动更新pod的，可以删除pod，由k8s重启启动一个
kubectl delete -n <namespace> pod <pod_name>

# 可以强制刷新配置
kubectl replace --force -f xxx.yaml
```

其他可用的维护指令：

```shell
# 将现有资源导出为yaml, 如deployment（缩写为：deploy）
kubectl get deploy <dp-name> -o yaml

# 查看service（缩写为：svc）限制
kubectl get svc -A
```





## 五、dashboard安装

下载dashboard配置文件: https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.0/aio/deploy/recommended.yaml

修改该文件: 其

```shell
...
---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort	#新增: 以端口映射方式暴露服务
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30009	#新增: 暴露30009端口
  selector:
    k8s-app: kubernetes-dashboard

---
...
          image: hub.2980.com/dockerhub/kubernetesui/dashboard:v2.5.0 # 修改docker pull镜像位置
...
          image: hub.2980.com/dockerhub/kubernetesui/metrics-scraper:v1.0.7 # 修改docker pull镜像位置
...
```

然后执行安装:

```shell
kubectl apply -f ./recommended.yaml
```

然后编写`dashboard-adminuser.yaml`, 创建sa和ClusterRoleBinding, 用于后续生成token登录. (注: `cluster-admin`这个ClusterRole已经在recommended.yaml中创建好了)

```shell
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
```

然后执行sa创建, 并生成token

```shell
kubectl apply -f dashboard-adminuser.yaml
kubectl -n kubernetes-dashboard create token admin-user
#注: v1.24.0之后, 默认不会再自动生成secret, 也不会绑定到sa; 所以每次要登录, 只能使用create token生成新的token, 这样也更安全
```



## 六、常见错误：

1. Cannot detect current cgroup on cgroup v2
   containerd未运行正常，查看其日志，发现是pause镜像拉取失败，镜像源无法访问

2. `"Error getting node" err="node \"master\" not found"`
   
   containerd未运行正常，导致etcd这个pod没有启动，k8s就无法正常监听配置

3. `Container runtime network not ready`
   未配置网络插件导致，下一节会进行配置

4. 安装dashboard启动报错：CrashLoopBackOff
   
   ```shell
   #查看报错的pod
   kubectl get pods -n kube-system | grep -v Running
   #查看pod存在的问题，发现镜像拉取等没问题，是pod自身运行后异常
   kubectl describe pod <pod-name> -n kube-system
   #查看异常pod的日志，即可定位异常
   kubectl logs <pod-name> -n kube-system
   ```

5. dashboard日志报错：`Metric client health check failed: the server is currently unable to handle the request`
   原因是：指定了使用nodePort，但是没有让dashboard-metrics-scraper使用hostNetwork
   解决方法：

   * 配置dashboard-metrics-scraper，添加`hostNetwork: true`

     ```yaml
     kind: Deployment
     apiVersion: apps/v1
     metadata:
       labels:
         k8s-app: dashboard-metrics-scraper
       name: dashboard-metrics-scraper
       namespace: kubernetes-dashboard
     spec:
       ...
       template:
       ...
         spec:
         ...
           hostNetwork: true
     ```

   * 加载配置，并更新：由k8s内部策略自动更新

     ```shell
     kubectl apply -f ./recommended.yaml
     ```

     

6. dashboard打开后无数据：

   * 需要切换到`kube-system`这个命名空间

   * 可以看下右上角错误提示定位具体错误问题：
     
     * （1）没有ClusterRole：编辑dash_account.yaml加入对具体ClusterRole的创建
     
     * （2）提示没有pods的read权限：修改ClusterRole创建时的resources和verbs配置：
       
       ```shell
       apiVersion: rbac.authorization.k8s.io/v1
       kind: ClusterRole
       metadata:
         # "namespace" 被忽略，因为 ClusterRoles 不受名字空间限制
         name: secret-reader
       rules:
       #使用"*"代表所有资源组：
       - apiGroups: ["*"]
         # 使用"*"代表所有资源
         resources: ["*"]
         verbs: ["get", "watch", "list","create"]
       ```

7. 错误`error execution phase preflight: couldn’t validate the identity of the API Server`: 这是因为使用旧的已过期的token和hash去添加节点. 需要重新生成新的token和hash

8. 默认配置下Kubernetes不会将Pod调度到Master节点, 使用的是Taint标记. 如果希望Master节点也能作为Node使用,

   ```shell
   # 则使用如下命令去除Taint标记, 加'-'就意味着移除该键
   kubectl taint nodes k8s-master node-role.kubernetes.io/master-
   # 要恢复Master Only状态, 则使用'=""'添加该键:
   kubectl taint nodes k8s-master node-role.kubernetes.io/master=NoSchedule
   ```
   
9. 解决重启时containerd-shim服务卡住重启90s问题：
   `vi /etc/systemd/system.conf`调小服务终止等待超时

   ```shell
   DefaultTimeoutStopSec=5s
   ```

10. 私有仓库拉取报错“access denied... authorization failed”
    通过配置secret，指定私仓的用户名密码，注意放到对应的namespace下面：

    ```shell
    kubectl create secret docker-registry <my-secret> -n <my-ns> --docker-server=<my-hub-hostname> --docker-username=<username> --docker-password=<password>
    ```

    然后在需要拉取私仓的资源配置中加入：

    ```yaml
    spec:
      ...
      imagePullSecrets:
      - name: <my-secret>
    ```

    





## 八、其他资料

k8s概念了解：[概念 | Kubernetes](https://kubernetes.io/zh-cn/docs/concepts/)

k8s官方教程：[教程 | Kubernetes](https://kubernetes.io/zh-cn/docs/tutorials/)

kubectl指令参考：[kubectl 备忘单 | Kubernetes](https://kubernetes.io/zh-cn/docs/reference/kubectl/cheatsheet/)





## 九、维护经验

1. kube-proxy组件在没有配置conntrack-max时, 默认会根据系统内存设置conntrack-max, 这个功能会导致系统的设置不知不觉被改掉. 详见源码 Kubernetes](https://kubernetes.io/zh-cn/docs/tutorials/)

kubectl指令参考：[kubectl 备忘单 | Kubernetes](https://kubernetes.io/zh-cn/docs/reference/kubectl/cheatsheet/)





## 九、维护经验

1. kube-proxy组件在没有配置conntrack-max时, 默认会根据系统内存设置conntrack-max, 这个功能会导致系统的设置不知不觉被改掉. 详见源码