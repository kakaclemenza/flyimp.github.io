---
layout: post
title: redis
category: app
typora-root-url: ../../..
---



## 集群部署

集群部署统一采用docker容器+docker-compose简单编排. 这里依照redis最小集群规模, 启用3主+3备共六个节点

首先编写配置文件模板`redis-cluster.tmpl`:

```shell
port ${PORT}
# 保护模式，默认值 yes，即开启。开启保护模式以后，需配置 bind ip 或者设置访问密码；关闭保护模式，外部网络可以直接访问
protected-mode no
# 是否以守护线程的方式启动。docker模式下必须设置为no，否则容器没有前台进程会自动退出
daemonize no

### REDIS CLUSTER CONFIG ###

# 开启集群模式，redis会多监听一个加 10000 的端口，用于集群间通信
cluster-enabled yes
cluster-config-file nodes-${PORT}.conf
cluster-node-timeout 15000
# 添加访问认证
#requirepass
# 如果主节点开启了访问认证，从节点访问主节点需要认证
#masterauth
```

编写脚本生成每个节点的配置文件目录和数据存储目录`init.sh`:

```shell
for port in `seq 6371 6376`; do \
  mkdir -p redis-${port}/conf \
  && PORT=${port} envsubst < redis-cluster.tmpl > redis-${port}/conf/redis.conf \
  && mkdir -p redis-${port}/data;\
done
```

编写docker-compose编排脚本:

```yml
version: "2.0"

# 定义服务，可以多个
services:
  redis-cluster-init:
    image: redis
    container_name: redis-cluster-init # 容器名称
    command: >
      sh -c "
        redis-cli --cluster create 172.50.0.11:6371 172.50.0.12:6372 172.50.0.13:6373 --cluster-replicas 0 --cluster-yes &&
        redis-cli --cluster add-node 172.50.0.14:6374 172.50.0.11:6371 --cluster-slave &&
        redis-cli --cluster add-node 172.50.0.15:6375 172.50.0.12:6372 --cluster-slave &&
        redis-cli --cluster add-node 172.50.0.16:6376 172.50.0.13:6373 --cluster-slave &&
        redis-cli --cluster check 172.50.0.11:6371
        "
    networks: #网络
      redis-net:
        ipv4_address: 172.50.0.10
    depends_on:
      - redis-6371
      - redis-6372
      - redis-6373
      - redis-6374
      - redis-6375
      - redis-6376

  redis-6371: # 服务名称
    image: redis # 创建容器时所需的镜像
    container_name: redis-6371 # 容器名称
    restart: always # 容器总是重新启动
    volumes: # 数据卷，目录挂载
      - ./redis-6371/conf/redis.conf:/usr/local/etc/redis/redis.conf
      - ./redis-6371/data:/data
    command: redis-server /usr/local/etc/redis/redis.conf # 覆盖容器启动后默认执行的命令
    networks: #网络
      redis-net:
        ipv4_address: 172.50.0.11
    ports: # nat 网络模式
      - 6371:6371
      - 16371:16371

  redis-6372:
    image: redis
    container_name: redis-6372
    volumes:
      - ./redis-6372/conf/redis.conf:/usr/local/etc/redis/redis.conf
      - ./redis-6372/data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks: #网络
      redis-net:
        ipv4_address: 172.50.0.12
    ports:
      - 6372:6372
      - 16372:16372

  redis-6373:
    image: redis
    container_name: redis-6373
    volumes:
      - ./redis-6373/conf/redis.conf:/usr/local/etc/redis/redis.conf
      - ./redis-6373/data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks: #网络
      redis-net:
        ipv4_address: 172.50.0.13
    ports:
      - 6373:6373
      - 16373:16373

  redis-6374:
    image: redis
    container_name: redis-6374
    volumes:
      - ./redis-6374/conf/redis.conf:/usr/local/etc/redis/redis.conf
      - ./redis-6374/data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks: #网络
      redis-net:
        ipv4_address: 172.50.0.14
    ports:
      - 6374:6374
      - 16374:16374

  redis-6375:
    image: redis
    container_name: redis-6375
    volumes:
      - ./redis-6375/conf/redis.conf:/usr/local/etc/redis/redis.conf
      - ./redis-6375/data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks: #网络
      redis-net:
        ipv4_address: 172.50.0.15
    ports:
      - 6375:6375
      - 16375:16375

  redis-6376:
    image: redis
    container_name: redis-6376
    volumes:
      - ./redis-6376/conf/redis.conf:/usr/local/etc/redis/redis.conf
      - ./redis-6376/data:/data
    command: redis-server /usr/local/etc/redis/redis.conf
    networks: #网络
      redis-net:
        ipv4_address: 172.50.0.16
    ports:
      - 6376:6376
      - 16376:16376

networks:
  redis-net: #网段名
     driver: bridge #连接方式
     ipam:
       driver: default #ip
       config:
         - subnet: 172.50.0.0/16 #定义网段
```

以上准备好之后, 我们就可以启动集群了:

```shell
bash ./init.sh
docker compose up -d
```

注意，集群内部我们使用`redis-cluster-init`对集群的主备状态进行初始化，这里有两种方式，我们使用方式二:

* (一) 让redis自动进行初始化: 这种方式redis会内部随机选择主节点和从节点, 哪个是主节点只有初始化后才能知道
  
  ```shell
  docker exec redis-6371 redis-cli --cluster create 172.50.0.11:6371 172.50.0.12:6372 172.50.0.13:6373 172.50.0.14:6374 172.50.0.15:6375 172.50.0.16:6376 --cluster-replicas 1 --cluster-yes
  ```

* (二)手动配置主从节点: 首次启动的节点和被分配槽的节点都是主节点, 从节点负责复制主节点槽信息和相关的数据. 我们需要先创建主节点集群, 6371~6373为主节点, 会自动分配哈希槽:
  
  ```shell
  docker exec redis-6371 redis-cli --cluster create 172.50.0.11:6371 172.50.0.12:6372 172.50.0.13:6373 --cluster-replicas 0 --cluster-yes
  ```
  
  然后我们配置从节点, 将当前节点设置为 node_id 指定的节点的从节点. 如:
  
  ```shell
  # 手动配置6371~6373的从节点
  redis-cli --cluster add-node 172.50.0.14:6374 172.50.0.11:6371 --cluster-slave
  redis-cli --cluster add-node 172.50.0.15:6375 172.50.0.12:6372 --cluster-slave
  redis-cli --cluster add-node 172.50.0.16:6376 172.50.0.13:6373 --cluster-slave
  
  # 配好后检查集群状态
  docker exec -ti redis-6371 redis-cli --cluster check 172.50.0.11:6371
  ```

集群搭建完毕，后续如果有需要调整配置，比如redis改密码，步骤是：

1. 修改配置文件
2. 依次重启各个节点：redis内部做主备切换，不会影响数据正确性

## redis命令行操作

### 基本操作

启动redis命令行客户端:

```shell
# 指定密码, 连接的ip:port
redis-cli -a <password> -h 127.0.0.1 -p 6381
# 以集群模式连接
# redis-cli -c 启动，就会自动进行各种底层的重定向的操作
redis-cli -a <password> -h 127.0.0.1 -p 6381 -c

# 查看集群所有key及其分布, 其他指令类似
redis-cli -a <password> --cluster call 127.0.0.1:6381 keys \*
```

redis客户端交互指令

```shell
> keys *
# 查看`a`的类型
> type a

## hash类型操作
> hlen key #获取 key 键的字段数量
> hgetall key #返回 key 键的所有字段及其值
> hkeys key #获取 key 键中所有字段的名字
> hvals key #获取 key 键中所有字段的值
```

### 基于stream的消息队列

```shell
## stream类型操作
# 添加长度为5的消息队列`s`，并添加一条消息
> xadd s maxlen 5 * <key1> <value1> <key2> <value2>
# 读取ID>0的所有消息，一般消息redis自动添加的ID是"时间戳-0"
> xread streams s 0
# 读取指定范围1条消息
> xrange s - + count 1
# 读取最后一条消息
> xrevrange s + -  count 1
# 删除最旧的一条消息
> xdel s 0
# 获取消息队列长度
> xlen s
# 删除整个消息队列
> del s
# 查看stream信息
> xinfo stream s
# 查看stream完整信息
> xinfo stream s full
# 消费者组：消费者组中的各个消费者可以竞争消息，达到并行处理
#创建消费者组`sg`，并从队列头开始消费（将`0-0`改为`$`则从队列尾部开始）
> xgroup create s sg 0-0
#从消费者组中读取1条消息，会将消费者`c`自动加入消费者组
# `>`号表示从当前消费组的last_delivered_id后面开始读，
# 每当消费者读取一条消息，last_delivered_id变量就会前进
> xreadgroup GROUP sg c count 1 streams s >
#查看读取后，未确认的消息
> xpending s sg - + 10
#告知消息处理完成(消息ID: "1676529253137-0")
> xack s sg "1676529253137-0"
```

其中主要是stream消息队列的处理最为重要，使用也最频繁。基本概念：

stream消息队列相对于其他方式的优势：

* 基于list实现：
  * 无法避免消息丢失
  * 只支持单消费者
* 基于pub/sub实现：
  * 不支持数据持久化
  * 无法避免消息丢失
  * 消息堆积有上限，超出时数据丢失
  * 广播消息

stream实现消息队列的分类：

* 类型一：xread读取
  * 消息可回溯，读取不会删除
  * 一个消息可以被多个消费者读取
  * 可以阻塞读取
  * 有消息**漏读的风险**
* 类型二：xreadgroup消费者组方式读取
  * 消息可回溯
  * 可以多消费者争抢消息，加快消费速度
  * 可以阻塞读取
  * **没有消息漏读**的风险
  * 有消息确认机制，保证消息至少被消费一次

其他内部技术：

* 避免服务器时间错误带来的问题（比如宕机重启）：
  Redis的每个Stream类型数据都维护一个latest_generated_id属性，用于记录最后一个消息的ID。**若发现当前时间戳退后（小于latest_generated_id所记录的），则采用时间戳不变而序号递增的方案来作为新消息ID**（这也是序号为什么使用int64的原因，保证有足够多的的序号），从而保证ID的单调递增性质。

* 消费者崩溃带来的消息丢失问题：
  STREAM 设计了 **Pending 列表**，用于记录读取但并未处理完毕的消息。等待消费者再次上线后，可以读取该Pending列表，就可以继续处理该消息了，保证消息的有序和不丢失。

* 消费者彻底宕机后如何转移给其它消费者处理？
  使用`xclaim`指令做处理；注意每次需要指定消息ID和转移时间IDLE；转移后IDLE会被重置，保证不会进行重复转移，防止过期消息同时转给多个消费者。

* 损坏的消息问题处理：
  如果某个消息，不能被消费者处理，也就是不能被XACK，这是要长时间处于Pending列表中，即使被反复的转移给各个消费者也是如此。此时该消息的delivery counter就会累加（上一节的例子可以看到），当累加到某个我们预设的临界值时，我们就认为是坏消息（也叫死信，DeadLetter，无法投递的消息）

  如果判定是坏消息，删除方式如下：

  ```shell
  # 删除队列中的消息
  127.0.0.1:6379> XDEL s 1553585533795-1
  (integer) 1
  # 查看队列中再无此消息
  127.0.0.1:6379> XRANGE s - +
  1) 1) "1553585533795-0"
     2) 1) "msg"
        2) "1"
  2) 1) "1553585533795-2"
     2) 1) "msg"
        2) "3"
  # 告知Pending该消息已处理完毕
  127.0.0.1:6379> XACK s sg 1553585533795-1
  (integer) 1
  ```

  



## 进阶知识

### 缓存穿透、击穿、雪崩

#### 缓存穿透

**问题来源**

缓存穿透是指**缓存和数据库中都没有的数据**，而用户不断发起请求。由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。

在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。

如发起为id为“-1”的数据或id为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大。

**解决方案**

1. 增加校验：接口层增加校验，如用户鉴权校验，id做基础校验，id<=0的直接拦截；
2. 缓存空值：从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击
3. 布隆过滤器：下文有介绍。可以快速判断一个key是否**不存在**于某容器，**对缓解缓存击穿效果很明显**。

#### 缓存击穿

缓存击穿是指**缓存中没有但数据库中有的数据**（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。

**解决方案**

1. 加锁排队：此处理方式和缓存雪崩加锁排队的方法类似，都是在查询数据库时加锁排队，缓冲操作请求以此来减少服务器的运行压力。

2. 设置永不过期：对于某些热点缓存，我们可以设置永不过期，这样就能保证缓存的稳定性，但需要注意在数据更改之后，要及时更新此热点缓存，不然就会造成查询结果的误差。
3. 接口限流与熔断，降级。重要的接口一定要做好限流策略，防止用户恶意刷接口，同时要降级准备，当接口中的某些 服务 不可用时候，进行熔断，失败快速返回机制。

#### 缓存雪崩

**问题来源**

缓存雪崩是指缓存中**数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机**。和缓存击穿不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。大量的缓存穿透，就会导致缓存雪崩！

**解决方案**

1. 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。
2. 如果缓存数据库是分布式部署，将热点数据均匀分布在不同的缓存数据库中。
3. 设置热点数据永远不过期。
4. 开启rdb和aof持久化



### 一致性hash 与 哈希槽

在对缓存增删节点时，其实就是增删哈希槽，如果使用普通的hash算法，比较导致hash大规模失效，引起大量的缓存穿透，造成雪崩。

一致性hash的设计，就是为了解决这个问题，主要想法是：

1. 增加节点：
2. 删除节点：会把当前节点所有数据加到它的下一个节点上，容易引起下一个节点压力增加，引发**"缓存抖动"** 



### 容灾: RDB和AOF

什么场景下需要持久化：

* 只读缓存，但是请求量巨大，重新刷数据会导致缓存雪崩。
* 有写缓存，用于将多次结果累加后一段时间写入存储层。

#### rdb

主要做法：将数据库”快照“存盘；

指令：

* save：阻塞存盘，手动触发。
* bgsave：子进程存盘，不阻塞；手动触发或配置文件设置自动触发，在以下4种情况时会自动触发
  - redis.conf中配置`save m n`，即在m秒内有n次修改时，自动触发bgsave生成rdb文件；
  - 主从复制时，从节点要从主节点进行全量复制时也会触发bgsave操作，生成当时的快照发送到从节点；
  - 执行debug reload命令重新加载redis时也会触发bgsave操作；
  - 默认情况下执行shutdown命令时，如果没有开启aof持久化，那么也会触发bgsave操作；

衍生：

* bgsave：使用fork()子进程存盘，利用内核”copy-on-write“特性，尽量不影响主进程。使用配置文件**自动触发**进行的rdb，都是使用bgsave方式存盘。
* bgsave会先存盘到一个新文件，等到完成了，再通过原子move操作替换旧文件；如果bgsave过程中崩溃了，不会影响到旧文件
* 极端情况下，bgsave也会有性能问题，主要是：
  * 频率太高，导致多个快照进程竞争有限的磁盘带宽；
  * fork()虽然不复制内存，但是在复制内存页表时也会阻塞主线程；如果主线程内存过大也会存在较明显阻塞时长。

优缺点总结：

- **优点**
  - RDB文件是某个时间节点的快照，默认使用LZF算法进行压缩，压缩后的文件体积远远小于内存大小，适用于备份、全量复制等场景；
  - Redis加载RDB文件恢复数据要远远快于AOF方式；
- **缺点**
  - RDB方式实时性不够，**无法做到秒级的持久化**；
  - 每次调用bgsave都需要fork子进程，fork子进程属于重量级操作，频繁执行成本较高；
  - RDB文件是二进制的，没有可读性，AOF文件在了解其结构的情况下可以手动修改或者补全；
  - 不同版本RDB文件存在兼容问题；

针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决

#### aof

AOF日志采用写后日志，即**先写内存，后写日志**，不像mysql的binlog。采用写日志有两方面好处：

- **避免额外的检查开销**：Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。
- 不会阻塞当前的写操作

但这种方式存在潜在风险：

- 如果命令执行完成，写日志之前宕机了，会丢失数据。
- 主线程写磁盘压力大，导致写盘慢，阻塞后续操作

AOF日志写入过程：AOF日志记录Redis的每个写命令，步骤分为：命令追加（append）、文件写入（write）和文件同步（sync）。

- **命令追加** 当AOF持久化功能打开了，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器的 aof_buf 缓冲区。
- **文件写入和同步** 关于何时将 aof_buf 缓冲区的内容写入AOF文件中，Redis提供了三种写回策略
  - Always：每个写命令执行完，立马同步地将日志写回磁盘
  - Everysec：每秒写回，先把日志写到AOF文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘
  - No：系统控制写回，先把日志写到AOF文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘

AOF重写：AOF重写是为了**解决AOF文件体积随时间膨胀的问题**；重写的原理是：将多条命令执行后的结果，用一条相同效果的命令来替代。配置为`auto-aof-rewrite-min-size`和`auto-aof-rewrite-percentage`。AOF重写的过程：

- 主线程fork出子进程重写aof日志（COW特性，也可能卡主线程）
- 重写过程中，主线程就会将期间新写入的命令日志记录到aof日志缓冲；等子进程重写日志完成后，主线程追加aof日志缓冲到重写日志文件后面。（追加时，阻塞主线程）
- 原子替换日志文件

#### rdb混合aof方式

redis 4.0开始，通过`aof-use-rdb-preamble`配置即可开启，本质是同时开启rdb和aof方式，并且更进一步：将两者记录到同个`xxx.aof`文件中

redis 5.0开始，这个配置是默认开启的，rdb和aof参数也使用了默认值。如果是主服处理请求，不备份数据，需要配置关闭：

```shell
#关闭aof、rdb+aof混合持久化
appendonly no
#关闭rdb
save ""
```

注意：主服master关闭所有持久化时，**必须配置k8s重启策略为不自动重启**，否则如果主服自动重启后本地没有持久化数据，而**slave来不及接替master**，会出现master使用空数据集运行并将slave备份的数据也清理掉的清空！

#### 备份数据恢复

4.0版本前，先检查有没有aof文件，有的话先从aof恢复，没有再从rdb。

4.0以后，也是先恢复aof文件，不过如果aof中有rdb内容，先读取rdb内容，再读取aof执行。

#### 备份最佳实践

- 如果Redis中的数据并不是特别敏感或者可以通过其它方式重写生成数据，可以**关闭持久化**，如果丢失数据可以通过其它途径补回；
- 自己制定策略定期检查Redis的情况，然后可以**手动触发**备份、重写数据；
- 单机如果部署多个实例，要防止多个机器同时运行持久化、重写操作，防止出现内存、CPU、IO资源竞争，让持久化变为串行；
- 可以**加入主从**机器，一台用于响应主业务，一台用于数据持久化，这样就可能让 Redis 更加高效的运行。
- RDB持久化与AOF持久化可以同时存在，配合使用。

### 布隆过滤器

bloomfilter用于快速判断一个key是否**不存在**于某容器，对缓解**缓存穿透**效果很明显。它的优点是空间效率和查询时间都比一般的算法要好的多，缺点是有一定的误识别率和删除困难。
原理：

* 数据初始化时，创建一个位数组
* 每次写键值数据前，先通过几个无偏哈希函数算出它的几个位置，在位数组中对该位置1
* 每次读键值数据，就可以通过同样的哈希函数算出该键值的各个位置是否为1，**都为1则说明该数据可能存在**，因为可能有别的数据也哈希后设置了同样的键值；**若有一个不为1，则说明该数据必定不存在**
* 布隆过滤器准确率会随时间降低、或者占用空间会不断变大，因为哈希位被占用太多；这时需要重置：即清空位数组，然后重新将已有数据都计算哈希填入布隆过滤器。

### redis事务、lua脚本

数据库事务需要满足ACID特性：

- Atomicity: 原子性，事务是不可分割的单元。事务中的命令要么全部一起执行，要么就都不执行（如果有一个命令不能执行，那所有命令不能执行）。
- Consistency: 一致性，包括
  - 提交成功的事务产生的效果要能被后续的所有事务读取到
  - 不能破坏数据库约束
  - 提交成功的事务里，所有操作都要成功执行
- Isolation: 隔离性，多个并发事务不能互相影响
- Durability: 持久性，提交成功的事务不能丢失

redis事务：

* 使用 `WATCH-MULTI-EXEC` 实现：本质上是多条指令打包执行，不使用WATCH无法保证原子性。
* 缺点：事务中某个指令执行失败了，后续指令仍继续执行。【**不满足一致性、原子性**（原子性要看定义，语法错误是满足的，运行错误不满足）】
* 缺点：WATCH很难定义
* 缺点：多条指令要分多次发到服务端，效率低

lua脚本：

* 使用lua脚本，提交到redis，可以复用。
* 优点：高效，多条命令打包成一个脚本一次执行
* 优点：原子性好
* 缺点：事务中某个指令执行失败了，后续指令不执行。【**不满足一致性、原子性**】

## redis应用



### redis-cluster负载均衡

#### hash槽位定位算法

Redis 集群总共的槽位数是 16384 个，每一个主节点负责维护一部分槽以及槽所映射的键值数据，Redis 集群默认会对要存储的 key 值使用 CRC16 算法进行 hash 得到一个整数值，然后用这个整数值对 16384 进行取模来得到具体槽位，公式为：

> slot = CRC16(key) % 16383

如果出现不均衡，可以使用`rebalance`命令手动重新分配各个节点负责的槽数量，但不推荐，因为它可能认为没有必要进行分配时会直接退出，无法解决bigkey问题。

bigkey问题：每个key用于hash的字段设置不合理，导致分配倾斜，需要重新设计key中需要被hash的范围来解决。

#### 请求重定向问题

redis-cluster模式下，如果一个请求被发到任意一个与该key所在节点不同的节点，该节点会根据key来计算出其所在的节点，并将请求重定向到目标节点。

频繁的请求重定向会降低整个系统性能，需要减少请求重定向

#### 集群请求的负载均衡

业界：使用redis proxy代理模式，比如：twemproxy、codis等。这些是代理负责数据分片，redis集群不关心数据分片，通常适合单实例或哨兵模式

智能客户端：推荐使用redis智能客户端来解决重定向问题。智能客户端做法是：

1. 当发送key操作命令后，redis-cluster返回moved重定向或ask重定向，则向redis集群更新分片信息
2. 发起请求前，自行计算该key对应的节点
3. 直接向对应节点发起请求

智能客户端还可以处理事务操作（redis不支持事务，智能在客户端实现）

### redis-cluster主从选举

#### **故障发现**

故障发现里面有两个重要的概念：疑似下线（PFAIL-Possibly Fail）和确定下线（Fail）。

集群中的健康监测是通过定期向集群中的其他节点发送 PING 信息来确认的，如果发送 PING 消息的节点在规定时间内，没有收到返回的 PONG 消息，那么对方节点就会被标记为疑似下线。

一个节点发现某个节点疑似下线，它会将这条信息向整个集群广播，其它节点就会收到这个消息，并且通过 PING 的方式监测某节点是否真的下线了。如果一个节点收到某个节点疑似下线的数量超过集群数量的一半以上，就可以标记该节点为确定下线状态，然后向整个集群广播，强迫其它节点也接收该节点已经下线的事实，并立即对该失联节点进行主从切换。

这就是疑似下线和确认下线的概念，这个概念和哨兵模式里面的主观下线和客观下线的概念比较类似。

#### **故障转移**

当一个节点被集群标识为确认下线之后就可以执行故障转移了，故障转移的执行流程如下：

1. 从下线的主节点的所有从节点中，选择一个从节点（选择的方法详见下面“新主节点选举原则”部分）；
2. 从节点会执行 SLAVEOF NO ONE 命令，关闭这个从节点的复制功能，并从从节点转变回主节点，原来同步所得的数据集不会被丢弃；
3. 新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己；
4. 新的主节点向集群广播一条 PONG 消息，这条 PONG 消息是让集群中的其他节点知道此节点已经由从节点变成了主节点，并且这个主节点已经接管了原本由已下线节点负责处理的槽位信息；
5. 新的主节点开始处理相关的命令请求，此故障转移过程完成。

#### **新主节点选举原则**

新主节点选举的方法是这样的：

1. 集群的纪元（epoch）是一个自增计数器，初始值为0；
2. 而每个主节点都有一次投票的机会，主节点会把这一票投给第一个要求投票的从节点；
3. 当从节点发现自己正在复制的主节点确认下线之后，就会向集群广播一条消息，要求所有有投票权的主节点给此从节点投票；
4. 如果有投票权的主节点还没有给其他人投票的情况下，它会向第一个要求投票的从节点发送一条消息，表示把这一票投给这个从节点；
5. 当从节点收到投票数量大于集群数量的半数以上时，这个从节点就会当选为新的主节点。

到这里整个新主节点的选择就完成了。

### 消息队列

见上文redis命令行操作

### 分布式锁的设计

完整做法：

1. 加锁：`SET $lock_key $unique_id EX $expire_time NX`

2. 操作共享资源

3. 起一个goroutine定时为锁续期：lua脚本，判断锁是否还是自己的，再续期

4. 释放锁：Lua 脚本，先 GET 判断锁是否归属自己，再 DEL 释放锁

   ```lua
   // 判断锁是自己的，才释放
   if redis.call("GET",KEYS[1]) == ARGV[1]
   then
       return redis.call("DEL",KEYS[1])
   else
       return 0
   end
   ```

解决的问题：

1. 加锁后节点异常无法释放锁：加锁同时设置过期时间
2. 加速和设置过期时间无法原子化：使用redis2.6.12之后的附带过期设置的指令
3. 如何原子化释放锁：使用lua脚本，判断锁是自己的，再进行释放
4. 进程处理时间过长导致持有了过期的锁：定时续期

未解决的问题：

1. 主从切换时，set指令没有同步到新的节点。解决：
   * redlock：较复杂
   * 自定义：定时续期检查，可以查出自己已经不再持有锁
2. 进程卡住，导致锁过期了，进程恢复后存在一段时间和其他进程一起持有锁的情况：所有分布式锁都无法避免！



## 其他问题

### redis与mysql双写一致性如何保证（如何更新存储层和缓存层的数据）

节选最最常用的**Cache Aside Pattern**, 总结来说就是

- **读的时候**，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。
- **更新的时候**，先更新数据库，然后再删除缓存。

其具体逻辑如下：

- **失效**：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。
- **命中**：应用程序从cache中取数据，取到后返回。
- **更新**：先把数据存到数据库中，成功后，再让缓存失效。

问题：

是不是Cache Aside这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。

但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。

### NUMA对redis的影响

NUMA架构，就是CPU硬件技术，将内存按插槽绑定到具体的CPU物理核上，这样就导致CPU插槽上的核心范围本核心内存速度加快，如果要访问其他CPU插槽内存则需要特殊配置并且速度变慢。

Linux识别NUMA架构后，会执行如下操作：默认配置下，即`vm.zone_reclaim_mode = 1`，NUMA为了高效，会仅仅只从你的当前node里分配内存，只要当前node里用光了（即使其它node还有），也仍然会启用硬盘swap。这会导致访存变得很慢。

调优方法：

* `vm.zone_reclaim_mode = 0`，使得内存不足时可以去其他CPU插槽申请内存，而不是访问swap
* `numactl --interleave=all`，随机去不同CPU插槽申请内存，防止挤占同一根inter-connect通道导致速度变慢

另外NUMA还会有其他坑：

*  在os层numa关闭时,打开bios层的numa会影响性能，QPS会下降15-30%
* 当发现numa_miss数值比较高时，说明需要对分配策略进行调整。例如将指定进程关联绑定到指定的CPU上，从而提高内存命中率。如redis-server需要显式绑定到某个物理CPU核心上运行，否则由于它经常会睡眠，容易上下文切换到其他物理核心上。



## 最佳实践

