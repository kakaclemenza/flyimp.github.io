---
layout: post
title: kcp源码理解
category: app
typora-root-url: ../../..
---

## 前言

kcp协议的设计还是很精妙的，总的来说，这个协议是**在应用层继续做传输层的工作**；基于这点理解，这边总结kcp协议带来的关于网络优化的改变：

* 可定制可靠传输：用户态可定制的滑动窗口、快速重传、选择性重传、拥塞控制等传统tcp协议所需的机制
* 可扩展冗余：可以轻易增加fec前向纠错机制
* 清晰的分层：kcp协议层只负责数据包的可靠传输，并不负责底层协议（如UDP）的收发。这样底层收发细节可以轻易的在运行时被替换，这使得线路切换非常方便

下面将使用常规的数据包流跟踪的方式来分析kcp协议源码，分：“接受流程”和“发送流程”两部分；在解析过程中大家注意体会kcp是如何实现对以上的特性的。这边以kcp-go为基准代码来分析。



## kcp协议

我们先来看 KCP 的报文段结构. 首先, KCP 的有四种报文段, 或者说四个 Command:

- 数据报文 `IKCP_CMD_PUSH`
- 确认报文 `IKCP_CMD_ACK`
- 窗口探测报文 `IKCP_CMD_WASK`, 询问对端剩余接收窗口的大小.
- 窗口通知报文 `IKCP_CMD_WINS`, 通知对端剩余接收窗口的大小.

无论是那种报文段, 其结构都是这样的:

```
0               4   5   6       8 (BYTE)
+---------------+---+---+-------+
|     conv      |cmd|frg|  wnd  |
+---------------+---+---+-------+   8
|     ts        |     sn        |
+---------------+---------------+  16
|     una       |     len       |
+---------------+---------------+  24
|                               |
|        DATA (optional)        |
|                               |
+-------------------------------+
```

可以看到有这么几个字段:

- `conv` 4 字节: 连接标识, 前面已经讨论过了.
- `cmd` 1 字节: Command.
- `frg` 1 字节: 分片数量. 表示随后还有多少个报文属于同一个包.
- `wnd` 2 字节: 发送方剩余接收窗口的大小.
- `ts` 4 字节: 时间戳.
- `sn` 4 字节: 报文编号.
- `una` 4 字节: 发送方的接收缓冲区中最小还未收到的报文段的编号. 也就是说, 编号比它小的报文段都已全部接收.
- `len` 4 字节: 数据段长度.
- `data`: 数据段. 只有数据报文会有这个字段.

首先, 每个数据报文和 ACK 都会带上 sn, 唯一标识一个报文; 发送方发送一个数据报文, 接收方收到后回一个 ACK, 发送方收到 ACK 后根据 sn 将对应的报文标记为已送达; 同时, 每个报文都会带上 una, 发送方也会根据 una 将相应的报文标记已送达.

ts 可以用来估算 RTT (Round-Trip Time, 往返时间), 从而计算出 RTO (Retransmission TimeOut, 重传超时时间). 我们会根据 RTO 确定每个报文的超时时间, 如果报文在超时时间内未被标记为已送达, 就会被重传.

数据包的大小可能会超过一个 MSS (Maximum Segment Size, 最大报文段大小). 这个时候需要进行分片, frg 表示随后的分片数量, 即随后还有多少个报文属于同一个包.

每个报文都会带上 wnd, 告诉对端发送方剩余接收窗口的大小, 这有助于对端控制发送速率



**注意**：

1. 如果使用kcp-go，默认会开启fec，则使用wireshark解析KCP协议时，使用的插件比如[kcp_dissector.lua](https://raw.githubusercontent.com/cfadmin-cn/kcp_dissector/master/kcp_dissector.lua)，需要减去fec头部大小(8Bytes)才能正常解析，如果开启crypto头部同理：

   ```lua
   ...
     -- KCP dissect packet
     function KCP.dissector (Buffer, Menu, T)
   ...
       -- Calculate the data offset value
       -- 减去fec头部大小8Bytes
       local offset  = 8
   ...
   ```

2. kcp-go实现支持捎带确认：ACK包后面可以携带PUSH包，组成一个包一起发送

## kcp-go文件结构

kcp-go包使用go语言实现了kcp协议，除此之外，还扩展了fec纠错、crypto加密等功能，并补充了底层udp协议收发包实现。关键的文件及功能解析如下：

* `kcp.go`：对于kcp协议的go语言完整实现
* `sess.go`：封装kcp.go中的接口，并添加
  * 收发包实现：`readloop.go`、`tx.go`
  
  * fec纠错：`fec.go`
  
  * crypto加密：`crypt.go`
  
  * 可选的`writeDelay`功能：`timedsched.go`
    c版实现的kcp代码，每次都需要主动调用`ikcp_update()`驱动数据包发送，在`ikcp_flush()`函数中会先判断是否已经调用：
  
    ```c
    void ikcp_flush(ikcpcb *kcp)
    {
        ...
        // 'ikcp_update' haven't been called. 
    	if (kcp->updated == 0) return;
        ...
    ```
  
    这点对于使用上还是比较蛋疼的，定期调用也会证件数据包收发的平均耗时；**kcp-go中则把这个功能做成可选的，默认有数据立即触发发送**，可选`writeDelay`参数来设置定期调用`update`发送数据。

## 连接建立

### 客户端：

```shell
sess.go::DialWithOptions()
  conn， err := net.ListenUDP()  # 创建udp socket
  newUDPSession(..., conn, ...)
    sess.conn = conn
    sess.kcp = NewKCP()   # 初始化kcp
    go sess.readLoop() => readloop_linux.go::UDPSession.readLoop() # 发起读goroutine
      sess.go::UDPSession.packetInput()
        sess.go::UDPSession.kcpInput()
          s.kcp.Input()  # 调用kcp实例解kcp包
    SystemTimedSched.Put(sess.update, time.Now())  # 添加写数据包的调度
```

这里需要说明下`sess.update()`的调度，默认会每隔sess.interval的时间间隔调度执行sess.update()，检查未发送的数据包进行发送；但如果是上层直接调用``Write()`接口的数据，则是直接发送，详见下文发包流程；

### 服务端：

```shell
sess.go::ListenWithOptions()
  conn， err := net.ListenUDP()  # 创建udp socket
  serveConn(..., conn, ...)
    l.conn = conn
    go l.monitor() => readloop_linux.go::Listener.monitor() #监控进入的数据包
      sess.go::Listener.packetInput()
        s.kcpInput()	# 如果连接(conv)已存在，直接将包交付该连接
        ------
        s := newUDPSession()     # 否则创建该客户端连接，newUDPSession()调用流程和客户端是一样的！
        s.kcpInput()	# 收包，会把fec、crypto等头部解掉
          s.kcp.Input()  # 调用kcp实例解kcp包
          s.uncork()     # 将s.txqueue数据发出
```

服务端收包，其实是通过`l.monitor()`协程进行监控，当有数据包到来，先判断判断是否是新的客户端连接还是旧的连接，然后调用s.kcpInput()收包

总结下，kcp-go的设计思想：

* 服务端比客户端多做了一步处理，即需要判断是否是新建的客户端连接；所以服务端使用sess.Listener类来实现，最终交互的连接都是sess.UDPSession类
* sess.go负责kcp协议之外的工作处理，而kcp.go负责kcp协议的处理
* 创建客户端或服务端之后，都使用goroutine监听数据包的接收，并最终都是用`s.kcp.Input()`接收数据包

## 收包流程

接上节，kcp协议的收包是在`s.kcp.Input()`，我们从这个调用开始：

```shell
kcp.go::KCP.Input()
  ikcp_decode8u(...)  # 解包
  ...
  KCP.parse_ack()     # 如果是IKCP_CMD_ACK, 解析ack
  KCP.parse_fastack() # 并解析fastack类型包
  ------
  kcp.ack_push(sn, ts) # 如果是IKCP_CMD_PUSH，先回复ack
  KCP.parse_data()     # 然后解析普通的数据包
    kcp.rcv_buf = append(kcp.rcv_buf, newseg) #移入kcp.rcv_buf
    kcp.rcv_queue = append(kcp.rcv_queue, kcp.rcv_buf[:count]...) #从kcp.rcv_buf移入kcp.rcv_queue
  ...
  if kcp.nocwnd == 0   # 处理拥塞控制逻辑
  ...
  kcp.flush()  # 如果窗口变动，或者开启ackNoDelay选项等，才会立即flush()发送ack数据包或其他类型数据包
```

这里接收到的数据包最终是存入rcv_queue，然后用户读取使用：

```shell
sess.go::UDPSession.Read()
  kcp.go::KCP.Recv() # 实际从rcv_queue读出数据包，会先进行分片重组
```

注意的点：

* KCP.parse_ack()中使用了选择确认机制，从实现中可以看出：

  ```go
  func (kcp *KCP) parse_ack(sn uint32) {
      ...
      for k := range kcp.snd_buf {
  		seg := &kcp.snd_buf[k]
  		if sn == seg.sn {
              ...
              break
          }
      }
      ...
  }
  ```

* 这里`parse_data()`先将数据包放入rcv_buf，再统一移动到rcv_queue，这样设计的目的是**解决丢包、乱序问题**。先将乱序、丢包等情况下收到的包存入rcv_buf，统一判断接收到完整序列的包后再移动至rcv_queue

* 最后`flush()`需要在特定情况下才会立即执行，否则如果上层没有调用发送数据接口，则需要由SystemTimedSched来调度执行。这导致我们在服务端看到的srtt数值会维持在大于链路延迟的数值！开启ackNoDelay选项可以避免这个问题。

## 发包流程

发包流程分为两种情况，一种是kcp内部调用调度发送数据：

```shell
timedsched.go::TimedSched.sched()
  task.execute() => UDPSession.update() #调度执行到update()
    KCP.flush()
    UDPSession.uncork()
```

另一种是上层触发发送

```shell
sess.go::UDPSession.Write()
  DPSession.WriteBuffers()
    kcp.go::KCP.Send()    #将待发送数据吸入snd_queue
      kcp.snd_queue = append(kcp.snd_queue, seg)
    KCP.flush()
    UDPSession.uncork()
```

可以看到，最终发包都需要先调用`flush()`，将数据包从snd_queue移到snd_buf中，然后在调用UDPSession中的回调函数将snd_buf的包移动到txqueue中。该函数比较重要，主要负责处理发包的重传、流量控制、拥塞控制等问题

```go
func (kcp *KCP) flush(ackOnly bool) uint32 {
    ...
    cwnd := _imin_(kcp.snd_wnd, kcp.rmt_wnd) //kcp.rmt_wnd用于流量控制
    if kcp.nocwnd == 0 {
		cwnd = _imin_(kcp.cwnd, cwnd) //kcp.cwnd用于拥塞控制
	}
    ...
    for k := range kcp.snd_queue {} // 将数据包从snd_queue移到snd_buf中
    ...
    ref := kcp.snd_buf[:len(kcp.snd_buf)] 
    for k := range ref {} //处理重传的包
    ...
    flushBuffer() => KCP.output()  // 将snd_buf的包移动到UDPSession.txqueue中
    ...
    if kcp.nocwnd == 0 {}  //调整拥塞窗口；分慢开始、快速恢复两种
```

**重传**数据段逻辑：

1. 如果这个段数据首次发送，则直接发送数据。 

2. 如果这个段数据的当前时间大于它自身重发的时间，也就是RTO，则重传消息。 

3. 如果这个段数据的ack丢失累计超过resent次数，则重传，也就是快速重传机制。这个resent参数由`resend`参数决定。 

4. 如果这个段数据的ack有丢失且没有新的数据段，则触发ER，ER相关信息ER

**流量控制**：

* 流量控制是点对点的通信量的控制，是一个端到端的问题。总结起来，就是发送方的速度要匹配接收方接收（处理）数据的速度。发送方要抑制自身的发送速率，以便使接收端来得及接收。
* KCP的发送机制采用TCP的滑动窗口方式，可以非常容易的控制流量。KCP的头中包含wnd，即接收方目前可以接收的大小。能够发送的数据即为snd_una与snd_una+wnd之间的数据。**接收方每次都会告诉发送方我还能接收多少，发送方就控制下，确保自己发送的数据不多于接收端可以接收的大小。**
* KCP默认为32，即可以接收最大为32*MTU=43.75kB。KCP采用update的方式，更新间隔为10ms，那么KCP限定了你最大传输速率为4375kB/s，在高网速传输大内容的情况下需要调用ikcp_wndsize调整接收与发送窗口。
* KCP的主要特色在于实时性高，对于实时性高的应用，如果发生数据堆积会造成延迟的持续增大。建议从应用侧更好的控制发送流量与网络速度持平，避免缓存堆积延迟。

**拥塞控制（KCP可关闭）**

* KCP的优势在于可以完全关闭拥塞控制，非常自私的进行发送。KCP采用的拥塞控制策略为TCP最古老的策略，无任何优势。完全关闭拥塞控制，也不是一个最优策略，它全是会造成更为拥堵的情况。
  网络中链路的带宽，与整条网络中的交换节点（路由器、交换机、基站等）有关。如果，所有使用该链路的流量超出了，该链路所能提供的能力，就会发生拥塞。车多路窄，就会堵车，车越多堵的越厉害。因此，TCP作为一个大公无私的协议，当网络上发送拥堵的时候会降低自身发送数据的速度。拥塞控制是整个网络的事情，流量控制是发送和接收两方的事情。

* 当发送方没有按时接收到确认包，就认为网络发生了拥堵行为。TCP拥塞控制的方式，归结为慢开始、拥塞避免，快速恢复

* KCP发生丢包的情况下的拥塞控制策略与TCP Tahoe版本的策略一致。TCP Reno版本已经使用快恢复策略。**因此，丢包的情况下，其实KCP拥塞控制策略比TCP更为苛刻。**

  KCP在发生快速重传，数据包乱序时，采用的是TCP快恢复的策略。控制窗口调整为已经发送没有接收到ack的数据包数目的一半+resent。

  注：目前kernel 3.2以上的linux，默认采用google改进的拥塞控制算法，Proportional Rate Reduction for TCP

实际发送是在调用KCP.flush()之后，再调用UDPSession.uncork()

```shell
sess.go::UDPSession.uncork()
  s.tx() => tx_linux.go::UDPSession.tx()
    s.xconn.WriteBatch() #调用底层接口发送数据包
```



## 连接销毁

```shell
sess.go::UDPSession.Close()
  close(s.die) #通知其他协程退出
  s.kcp.flush(false) #将剩余数据包发出
  s.uncork()
  s.kcp.ReleaseTX()  #销毁snd_queue中的数据包（已发出未确认）
  s.l.closeSession() #如果是服务端，则关闭服务端连接
  ------
  s.conn.Close()	# 如果是客户端则关闭客户端连接
```



## 思考总结

1. Q：为什么不推荐使用kcp协议拥塞控制算法？
   A：kcp协议的拥塞控制算法使用的是最原始的TCP reno算法，该算法并没有什么优势。
2. Q：如何更快传输？
   A：做好流控，其实就是需要不断配合调整一些参数的，作者有解答这个问题，针对不同的应用场景，优化方向也不太相同，参见：https://github.com/skywind3000/kcp/wiki/Flow-Control-for-Users