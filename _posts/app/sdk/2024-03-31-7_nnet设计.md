---
layout: post
title: 7_nnet设计
category: app
typora-root-url: ../../..
---

## nnet的工作流

### 读工作流

创建服务端`CreateServer()`：

* `SidMap()`注册                                       <font color=#ff0000>=> 【TODO】</font>不再注册sidmap，而是直接注册私网ip！
  
  * 回调完成
  
* `waitForReady()`等待注册完成             <font color=#ff0000>【✓】</font>参考networknext做阻塞/非阻塞处理的优化：只支持非阻塞模式

  ​                                                             <font color=#ff0000>【✓】</font>出错自动重试逻辑

* `kcp.ListenWithOptions()`创建kcp服务端
  
  * `go l.monitor()`监听udp连接		<font color=#ff0000>【✓】</font>异常包处理
  * `go l.tcpMonitor()`监听tcp连接	<font color=#ff0000>【✓】</font>异常包处理
  
* `go server.processNewClient()`监听新连接                            <font color=#ff0000>=> 【TODO】</font>服务端重启后，对于客户端发来的旧连接包，需要识别并丢包处理！
  
  * `server.listener.AcceptKCP()`接收连接
    * 读取 `<-l.chAccepts` 通道
  * `server.msgCb()`通告新连接建立。
  * `go conn.processRecvPacket()`对客户端连接监听收包。  <font color=#ff0000>=> 【TODO】</font>对比epoll、prefork、workerpool三种方式（https://colobu.com/2019/02/27/1m-go-tcp-connection-2/）

创建客户端`CreateClient()`

* `Login()`获取代理列表，回调craeteClientCb()    <font color=#ff0000>=> 【TODO】</font>不再通过proid、bsid获取，而是直接指定私网ip

  * `conn.FindFastestLine()`快速探测（最长4s）找到能连上的最有线路
  * `kcp.NewConn()`基于提供的底层链接，创建kcp连接
    * `go sess.readLoop()`
    * 如果初始连接是tcp连接，额外：`go sess.tcpReadLoop()`

* `waitForReady()`等待获取代理完成                     <font color=#ff0000>【✓】</font>出错自动重试逻辑

* `go conn.processNewClient()`处理客户端连接

  * `go conn.processRecvPacket()`对客户端连接监听收包

    * `bufio.NewReader(conn.sess).Read()`   <font color=#ff0000>【✓】</font>保持当前bufio读取的方式

      ​															  <font color=#ff0000>【✓】</font>如果短暂读出错了，如何恢复？

      ​															  <font color=#ff0000>【✓】</font>异常大包读取是否正常？

      * `conn.sess.Read()`：先读取kcp.rcv_queue，如果没有则等待`s.chReadEvent`管道消息。

    * conn.msgCb()通告读取到消息。

  * `go conn.switchMonitor()`对客户端连接做监控和自动切换

### 写工作流

客户端、服务端都是往具体连接中写，调用`conn.SendPacket()`

* conn.sess.Write()写入   						<font color=#ff0000>【✓】</font>写出错，下次调用仍可以继续写。

  ​														  	<font color=#ff0000>【✓】</font>大包写入：在sess.Write()中限定最大65535B。后续改为可配置。

  * sess.kcp.Send()写入snd_queue，最终刷写到sess.txqueue中。
  * sess.uncork()将sess.txqueue内容调用sess.tx()发送

### 关闭

关闭场景有两种：

1. 上层调用接口`NNET_Destroy()`或`NNET_FiniSdk()`，会阻塞调用`Destroy(conn.id)`
2. 内部处理错误，由`conn.msgCb()`通告错误消息；上层调用`Process()`，处理到错误消息时，会阻塞调用`Destroy(conn.id)`关闭自身连接。

`Destroy(conn.id)`调用：会终止连接，并等待所有资源释放。   <font color=#ff0000>【✓】</font>读写退出逻辑优化

* 读写锁锁定对于conn.sess的读写   		                        	 	<font color=#ff0000>=> 【TODO】</font>读写锁使用情况整理
* conn.sess.Close()触发sess读写退出
* close(conn.stopChan)通知switchMonitor()退出
* conn.wg.Wait()等待所有任务退出，确保不再使用conn.sess
* conn.sess = nil 最后置空

### 服务端Kick客户端

1. 收到服务端IKCP_CMD_KICK协议；
2. 调用回调函数`s.Kick()`                                <font color=#ff0000>=> 【TODO】</font>需要优化！不使用回调函数方式，直接关闭sess来触发失败。
3. `s.Kick()`中调用`go Destroy(conn.id)`启动一个新协程来关闭自身连接；
4. 协程中会终止连接，并等待所有资源释放。  <font color=#ff0000>=> 【TODO】</font>要回复一个IKCP_CMD_FIN协议，告知服务端快速释放客户端资源。

### 连接重试超限：

1. CreateClient() -> processRecvPacket()等待收包；switchMonitor()监听到切换。
2. switchMonitor()监听到需要切换，并且重试/login超过10次无果。
3. switchMonitor()调用`go Destroy(conn.id)`启动一个新协程来关闭自身连接。
4. 协程中会终止连接，并等待所有资源释放。





## 关于真实IP透传问题

我们参考haproxy设计的`proxy protocol`，他其实很简单，就是在代理服转发后端目标服的**每一个数据包上，加上PROXY protocol协议头**，如同http协议固定加上`X-Forwarded-For`协议头一样。后端再以相同的协议进行解析。

但是这样有一个小问题：每个数据包增加了协议头部，可用传输数据的容量进一步减少；对于ipv4的ip是4Bytes，ipv6是16Bytes；

其实这个头部也不需要每次进行传输，只需要在连接刚开始建立的时候传输，**保证后端目标服收到请求的那一刻立即能拿到客户端真实IP即可**。所以这边基于kcp协议，可以制定这样的策略：

1. 增加一个新的cmd：IKCP_CMD_NEW = 80 (或其他)，并在KCP头部之后留出 20Bytes大小空间，全部置为零，用于存储客户端真实IP和Port信息；如下：

   ```shell
   
   0   1   2       4               8 (BYTE)
   +---+---+-------+---------------+
   |ver|len| port  |               |
   +---------------+---------------+   8
   |         v4/v6 address         |
   +---------------+---------------+  16
   |               |
   +---------------+  20
   ```

   * ver、len：这两个字段保留，用作其他用途
   * port：代理服看到的客户端端口
   * v4/v6 address：ipv4/ipv6地址信息，注意，如果是ipv4地址，也统一按ipv6兼容格式来填写，如127.0.0.1，则填入：`::ffff:127.0.0.1`

2. 客户端在remote变动时（包括连接建立和连接切换），发出的 IKCP_CMD_PUSH 包都转为 IKCP_CMD_NEW 包；客户端需要持续发送  IKCP_CMD_NEW 包，直到收到了第一个ack，再恢复为发送 IKCP_CMD_PUSH 包

3. 之后 IKCP_CMD_NEW 包到达xxxgw代理点，xxxgw代理点通过一个内核模块识别到这种类型的包，直接在空白的20Bytes中填上客户端的真实IP的信息；

4. 服务端接收到 IKCP_CMD_NEW 则将客户端真实IP更新到连接中，使得上层可查；然后回复ack包。

## 负载均衡与故障排除

**【负载均衡设计】**

目前业界的负载均衡方式列举如下：

* nginx、haproxy等七层代理
* ipvs四层负载均衡
* iptables + statistic模块，使用nth模式（round robin轮询）
* ebpf编程

综合分析当前场景，适用于nnet获取代理的方式有：

1. nginx代理tcp、http、https，上层自己加密：优点是普遍支持，缺点是多次握手代价高。不适合
2. nginx代理udp，上层跑kcp：**可行**
3. ipvs四层代理udp，上层跑kcp：ipvs需要vip概念，对网络要求高，不适合
4. iptables转发udp，上层跑kcp：**可行**
5. ebpf编程：需要开发工作量，不适合。

如果要支持真实IP获取，使用http协议，可以利用协议头；使用kcp协议，安装上一节提到的内核模块即可。

**【故障排除设计】**

基于上文“负载均衡设计”，我们来设计可行的两种方案如何进行故障排除：

1. nginx代理udp，上层跑kcp：使用nginx upstream模块配置故障排除，参考：[UDP Health Checks](https://docs.nginx.com/nginx/admin-guide/load-balancer/udp-health-check/)
2. iptables转发udp，上层跑kcp：iptables无法做负载均衡，虽然网上说可以通过`condition `模块判断一个标志文件来进行调整，但是检测后端服务器可用性还是要另外实现。因此，需要配合应用层监控后端服务器可用性，并设计配置中心配合发布-订阅模式进行更新。

**【总结】**

基于当前场景，我们选用nginx代理udp方式，会更加简单直接。