---
layout: post
title: 服务器维护经验总结
category: system
typora-root-url: ../../
---

### 网络性能分析

* ifstat: 主要用来监测主机网口的网络流量
* 

### 系统资源检查总命令

10条命令在一分钟内检查服务器性能：

- uptime：查负载。负载数据表示等待 CPU 资源的进程和阻塞在不可中断 IO 进程（进程状态为 D）的数量
- dmesg | tail：查内核日志
- vmstat 1：查系统核心指标
- mpstat -P ALL 1：查每核利用率
- pidstat 1：输出进程的 CPU 占用率
- iostat -xz 1：查看机器磁盘 IO 情况
- free -m
- sar -n DEV 1：查网络设备的吞吐率
- sar -n TCP,ETCP 1：查tcp连接、收发包状态
- top

ref: https://www.infoq.cn/article/2015/12/linux-performance/

### CPU与负载

**CPU利用率：**显示的是程序在运行期间实时占用的CPU百分比

**CPU负载：**显示的是*一段时间内*正在使用和等待使用CPU的平均任务数。CPU利用率高，并不意味着负载就一定大。举例来说：如果我有一个程序它需要一直使用CPU的运算功能，那么此时CPU的使用率可能达到100%，但是CPU的工作负载则是趋近于“1”，因为CPU仅负责一个工作嘛！如果同时执行这样的程序两个呢？CPU的使用率还是100%，但是工作负载则变成2了。所以也就是说，当CPU的工作负载越大，代表CPU必须要在不同的工作之间进行频繁的**工作切换**。

问题排查: 如果某服务器CPU利用率不高, 但是负载却很高.
原因: 等待磁盘I/O完成的進程過多, 導致進程隊列長度過大，但是cpu運行的進程卻很少，這樣就體現到負載過大了，cpu使用率低.
体现: 使用vmstat看, 发现"cpu"类中"wa"列占用CPU时间较高, "procs"类中"r"和"b"的数值都比较多. 这可以体现出等待IO导致负载高. 
另外如果是持续读写磁盘的业务, 发现"io"类中"bo"或"bi"不均匀, 这就说明是硬件有问题.

题外话: 
​	只有进程处于**运行态(running)和不可中断状态(interruptible)**才会被加入到负载等待进程中, 也就是下面这两种情况的进程才会表现为负载的值

- 即便需要立即使用CPU，也還需等待其他進程用完CPU
- 即便需要繼續處理，也必須等待磁盤輸入輸出完成才能進行

ref: https://www.itread01.com/articles/1499096303.html



### top中si或cs占用较高

这种问题一般top中看到的si不会太高, 但是查看单核时可以发现某个核的软中断处理占CPU时间基本占满, 这时可以使用`mpstat -P ALL`定位到具体的某个核. 这种情况**说明CPU一个核的处理能力达到了瓶颈**, 解决方法:

1. 如果你只是单核的CPU, 或者CPU实在太辣鸡, 那只能更换CPU了. `cat /proc/cpuinfo`检查CPU型号, 找到更高级版本的CPU.

2. 通过`cat /proc/interrupts`查看网卡中断队列数, 如果网卡是多队列网卡, 即支持硬件RSS特性, 则可以直接设置网卡队列中断的CPU亲缘性, 将网卡的多队列中断处理绑定到不同的CPU上, 达到多CPU负载均衡. 参考: [set_irq_affinity.sh](https://code.google.com/p/ntzc/source/browse/trunk/zc/ixgbe/set_irq_affinity.sh)脚本

   另外附上一个完整的检查网卡并进行设置的教程: [网卡多队列及中断绑定](https://blog.csdn.net/wyaibyn/article/details/14109325)

3. 否则, 可以使用RPS/RFS, 在软件层面实现网卡中断的多CPU间负载均衡, 注意开启RFS提高cpu cache命中率, 会提升处理效率

   ```shell
   # 开启rps, 这里是对eth0网卡的1号接收队列对所有CPU开启rps
   # CPU核心数少于16个会自动调整这个数值的.
   echo ff > /sys/class/net/eth0/queues/rx-0/rps_cpus
   #开启RFS(内存大的机器可以设置大于4096), 
   # 1. 设置rps_flow_cnt
   echo 4096 > /sys/class/net/eth0/queues/rx-0/rps_flow_cnt
   # 2. 4096*N (N为网卡队列数cat /proc/interrupts | grep eth0)
   echo 32768 > /proc/sys/net/core/rps_sock_flow_entries
   ```
```
   
   原理参考: [网络RPS/RFS/GSO/GRO等功能释义](http://chengqian90.com/%E6%9D%82%E8%B0%88/%E7%BD%91%E7%BB%9CRPS-RFS-GSO-GRO%E7%AD%89%E5%8A%9F%E8%83%BD%E9%87%8A%E4%B9%89.html)



### top中%st占比高

> %st(Steal time) 是当 hypervisor 服务另一个虚拟处理器的时候，虚拟 CPU 等待实际 CPU 的时间的百分比。

%st占比高, 只会出现在虚拟机中. 这时一般是宿主机CPU被同主机其他虚机所抢占导致.



### 如何发现linux中引起高io等待的进程

I/O wait之所以难以排查是因为默认有太多的工具告诉你系统I/O阻塞，但没那么多工具可以帮你缩小范围以便确定出是哪个或哪些进程引起的问题. 这里总结下一般步骤:

1. top命令: 回答是不是I/O引起系统卡顿, 从CPU(s) 这行你可以看出当前CPU I/O Wait的情况；越高的wa表示越多的cpu资源在等待I/O
2. `iostat -x 2 5`: 通过 %utilized 栏确定哪个磁盘存在IO问题. 该数值表示一秒中有百分之多少的时间用于 I/O 操作，或者说一秒中有多少时间 I/O 队列是非空的. 如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈
3. `iotop`: 直接确定哪个进程引起高IO占用. (缺点: iotop要单独安装, 安装命令: `sudo apt-get install iotop`)

**使能RPS后为什么会导致QPS下降？**

这个是使用RPS/RFS方式网上的相关问题, 主要是表明RPS的原理和局限. 当业务进程处理能力受限于用户空间处理时, 使用RPS反而会导致QPS下降, 因为启用QPS之后总的用户空间处理所需CPU时间被增加的软中断处理时间所挤压, 这是核间分配带来的开销.

1. 收到网卡中断的CPU会向其他CPU发IPI中断，这体现在CPU的%irq上
2. 需要处理packet的cpu会收到NET_RX_SOFTIRQ软中断，这体现再CPU的%soft上。请注意，RPS并不会减少第一个CPU的软中断次数，但是会额外给其他的CPU增加软中断。他减少的是第一个CPU的软中断的执行时间，即，软中断里不再需要那么多的时间去走协议栈做包解析，把这个时间给均摊到其他的CPU上去了。



### DNS 与 CDN



* DNS(Domain Name System, 域名系统), 作用就是解析"域名->ip". 它是一个分布式数据库, 是**全局负载均衡(GSLB)**中的一种实现方式.

  常用DNS：

  - 8.8.8.8——Google提供的免费DNS服务器的IP地址
  - 114.114.114.114——国内用户上网常用的DNS
  - 223.5.5.5——阿里 AliDNS。阿里公共DNS是阿里巴巴集团推出的DNS递归解析系统，目标是成为国内互联网基础设施的组成部分，面向互联网用户提供“快速”、“稳定”、“智能”的免费DNS递归解析服务

* A记录和CNAME

  A记录是解析域名到IP，Cname是解析域名到另外一个域名。

* CDN主要是通过**接管DNS实现**, 对于用户而言, 只需修改自己的DNS解析, 设置一个CNAME指向CDN服务商即可. 这样最终解析出来的CDN缓存服务器IP, 用户通过该IP请求到CDN缓存服务器, CDN缓存服务器根据http请求提供的要访问的域名, 通过Cache内部专用DNS解析得到此域名的实际IP地址, 向实际IP发请求, 收到数据后一份返回给客户端, 同时本地保存一份.

### DDos攻击与防范

发生DDos攻击, 查找原因时, 首先应当确认攻击流量类型. 区分是属于TCP/UDP/ICMP. TCP类型的攻击主要是利用协议漏洞, 通过堆积无效连接占用系统资源, 可以通过调整超时即syncookie设置起到防护效果.

如果是icmp/udp, 一是控制肉鸡伪造数据包直接攻击, 二是利用往共用服务发源地址为攻击目标的伪造echo包实现反射攻击. 这两种方式一般需要攻击者付出与被攻击者相当的流量代价. 另外有一种流量放大的做法, 目前还不太了解, 参考: https://blog.netlab.360.com/what-we-know-about-memcache-udp-reflection-ddos/

**AWS分享的DDos攻击总结**

攻击分类

* 网络层: 反射型攻击(IP,UDP). 利用CDN, DNS, SNMP, SSDP, memcached等协议, 进行放大攻击
* 传输层: 利用协议漏洞(SYN/ACK)
* 应用层: HTTP GET泛洪. DNS泛洪. 不过受限于TCP连接数, 攻击效果不明显

一般DDos攻击只是分散注意力的方式, 往往有其他类型的渗透攻击在其中.

应对DDos, 只能"缓解", 用更大的带宽(黑洞也算这个范围吧), 更强的计算性能来对抗.

**关于流量清洗, 流量黑洞**

了解DDos攻击下流量清洗, 流量黑洞的具体含义. 简单说, 流量清洗是通过将流量导向特殊的清洗设备, 该设备过滤掉有攻击特征的包, 其他包再发给目标服; 流量黑洞, 则是在运营商路由器上, 设置某路由的流量阈值, 当路由流量超阈值时, 将该路由置为"null route"; 流量清洗是为了在攻击流量不大时给目标服减少收包压力, 流量黑洞则直接掐断数据包进入网络路径. 参考: <https://help.aliyun.com/knowledge_detail/40038.html?spm=5176.13910061.0.0.545d4062p9tWrk&aly_as=hsgDWKgY>





### limit 计算方法

/proc/sys/fs/file-max
决定了当前**内核计算建议**的可以打开的最大的文件句柄数
file-max一般为内存大小（KB）的10%来计算

### ip_conntrack: table full, dropping packet.

ct数目满了, 所以无法再建立新的连接. 这个日志可以从dmesg中看到. 这个时候就可以看到上面的信息.

##### 原因

nf_conntrack/ip_conntrack 跟 nat 有关，用来跟踪连接条目，它会使用一个哈希表来记录 established 的记录。nf_conntrack 在 2.6.15 被引入，而 ip_conntrack 在 2.6.22 被移除，如果该哈希表满了，就会出现：
*nf_conntrack: table full, dropping packet*

nf_conntrack 工作在 3 层，支持 IPv4 和 IPv6，而 ip_conntrack 只支持 IPv4。目前，大多的 ip_conntrack_已被 nf_conntrack_取代，很多 ip_conntrack_仅仅是个 alias，原先的 ip_conntrack 的 /proc/sys/net/ipv4/netfilter/ 依然存在，但是新的 nf_conntrack 在 /proc/sys/net/netfilter/ 中，这个应该是做个向下的兼容。

##### 解决方案

**查看占用conntrack过多的连接**

```
cat /proc/net/ip_conntrack | cut -d ' ' -f 10 | cut -d '=' -f 2 | sort | uniq -c | sort -nr | head -n 10
```

**解决方案1: 不使用nf_conntrack模块**

- 首先要移除 state 模块，因为使用该模块需要加载 nf_conntrack。确保 iptables 规则中没有出现类似 state 模块的规则，如果有的话将其移除：

```
-A INPUT -m state –state RELATED,ESTABLISHED -j ACCEPT
```

- 注释 /etc/sysconfig/iptables-config 中的：
  IPTABLES_MODULES=”ip_conntrack_netbios_ns”
- 移除 nf_conntrack 模块：

```
$ sudo modprobe -r xt_NOTRACK nf_conntrack_netbios_ns nf_conntrack_ipv4 xt_state
$ sudo modprobe -r nf_conntrack
```

现在 /proc/net/ 下面应该没有 nf_conntrack 了。

**解决方案2：调整 /proc/ 下面的参数**

可以增大 conntrack 的条目 CONNTRACK_MAX 或者增加存储 conntrack 条目哈希表的大小 HASHSIZE，默认情况下，CONNTRACK_MAX 和 HASHSIZE 会根据系统内存大小计算出一个比较合理的值：

- 对于 CONNTRACK_MAX，其计算公式：
  CONNTRACK_MAX = RAMSIZE (in bytes) / 16384 / (ARCH / 32)
  比如一个 64 位 48G 的机器可以同时处理 48*1024^3/16384/2 = 1572864 条 netfilter 连接。对于大于 1G 内存的系统，默认的 CONNTRACK_MAX 是 65535。
- 对于 HASHSIZE，默认的有这样的转换关系：
  CONNTRACK_MAX = HASHSIZE * 8
  这表示每个链接列表里面平均有 8 个 conntrack 条目。其真正的计算公式如下：
  HASHSIZE = CONNTRACK_MAX / 8 = RAMSIZE (in bytes) / 131072 / (ARCH / 32)
  比如一个 64 位 48G 的机器可以存储 48*1024^3/131072/2 = 196608 的buckets(连接列表)。对于大于 1G 内存的系统，默认的 HASHSIZE 是 8192。

可以通过 echo 直接修改目前系统 CONNTRACK_MAX 以及 HASHSIZE 的值：

```
## 修改配置值
sudo vim /etc/sysctl.conf(文件内增加net.nf_conntrack_max = 655360)
sudo sysctl -p
## 验证是否生效
nefs@xx-xx-x-xxx:~$ sudo cat /proc/sys/net/netfilter/nf_conntrack_max
655360
```

**解决方案3: 使用 raw 表，不跟踪连接**

iptables 中的 raw 表跟包的跟踪有关，基本就是用来干一件事，通过 NOTRACK 给不需要被连接跟踪的包打标记，也就是说，如果一个连接遇到了 -j NOTRACK，conntrack 就不会跟踪该连接，raw 的优先级大于 mangle, nat, filter，包含 PREROUTING 和 OUTPUT 链。
当执行 -t raw 时，系统会自动加载 iptable_raw 模块(需要该模块存在)。raw 在 2.4 以及 2.6 早期的内核中不存在，除非打了 patch，目前的系统应该都有支持:

```
$ sudo iptables -A FORWARD -m state --state UNTRACKED -j ACCEPT
$ sudo iptables -t raw -A PREROUTING -p tcp -m multiport --dport 80,81,82 -j NOTRACK
$ sudo iptables -t raw -A PREROUTING -p tcp -m multiport --sport 80,81,82 -j NOTRACK
```

上面三种方式，最有效的是 1 跟 3，第二种治标不治本。



### SNMP监控

发现机器流量异常:

1. 首先区分是转发流量还是本机应用流量, 如果是iptables转发流量, 本机CPU应该不会太受影响, 且SNMP统计不到具体的TCP/UDP数据包大量增长现象. 运维管理系统上可以直接看SNMP监控
2. 如果确定是本机流量, 则通过SNMP可以分清到底是UDP, 还是TCP, 再结合本机上具体应用(高CPU)定位受影响应用.



### swap使用问题

观察系统慢是不是因为swap被使用导致, 不应该直接看swap是否有显示used, 而是应该使用vmstat看看有没有实际的发生swap换入换出操作

​```shell
sudo vmstat 1 20
```

另外, 有一个swappiness配置项, 是用于配置需要将内存中**不常用的数据**移到swap中去的**紧迫程度**. 对于服务器来说，主要性能衡量标准是整体的处理能力，而不是具体某一次的响应速度，能把更多的内存用来做I/O cache可能效果更好，所以Ubuntu server建议保持60的默认值



### CPU性能指标

转自: https://sq.163yun.com/blog/article/199988838432768000

在进行服务端性能测试时，需要观察系统对CPU的使用情况，以此作为衡量整个系统性能的重要指标，对于Linux CPU主要的关注点在利用率，运行队列，负载，上下文切换等，因此了解这些指标的含义和常用的监控方法对性能测试又很大的帮助。



#### 1、CPU使用率

Linux CPU使用率主要是从以下几个维度进行统计：

- %usr：普通进程在用户模下下执行的时间；
- %sys：进程在内核模式下的执行时间；
- %nice：被提高优先级的进程在用户模式下的执行时间；
- %idle：空闲时间。
- %iowait：等待I/O完成的时间。
- %irp：处理硬中断请求花费的时间。
- %soft：处理软中断请求花费的时间。
- %steal：是衡量虚拟机CPU的指标，是指分配给本虚拟机的时间片被同一宿主机别的虚拟机占用，**一般%steal值较高时，说明宿主机的资源使用已达到瓶颈**；
- 

一般情况下，CPU大部分的时间片都是消耗在用户态和内核态上，sys和user间的比例是相互影响的，%sys比例高意味着被测服务频繁的进行用户态和系统态之间的切换，会带来一定的CPU开销，这样分配处理业务的时间片就会较少，造成系统性能的下降。对于IO密集型系统，无论是网络IO还是磁盘IO，一般都会产生大量的中断，从而导致%sys相对升高，其中磁盘IO密集型系统，对磁盘的读写需要占用大量的CPU，会导致%iowait的值一定比例的升高，所以当出现%iowait较高时，需排查是否存在大量的不合理的日志操作，或者频繁的数据载入等情况； 

CPU利用的详细情况可以通过top，vmstat命令进行查看 

- top：
- 

```
%Cpu(s): 0.9 us, 0.6 sy, 0.0 ni, 98.4 id, 0.1 wa, 0.0 hi, 0.0 si, 0.0 st
```

- vmstat：
- 

```
procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu----
r b swpd free buff cache si so bi bo in cs us sy id wa
1 0 1061592 15986 395216 91660 0 0 3 9 0 0 5 1 94 0
```

- mpstat：
- 

```
CPU %usr %nice %sys %iowait %irq %soft %steal %guest %idle
 all 0.92 0.00 0.50 0.17 0.00 0.00 0.00 4.14 94.27
```

####  

#### 2、运行队列：

当Linux内核要寻找一个新的进程在CPU上运行时，必须只考虑处于可运行状态的进程，（即在TASK_RUNNING状态的进程），因为扫描整个进程链表是相当低效的，所以引入了可运行状态进程的双向循环链表，也叫运行队列（runqueue）。每个CPU或者说每个核都会维持一个运行队列，队列中存放running和runnable两种状态的进程，CPU会不断的调度队列中的进程运行，因此队列中的进程数越多，每个进程分别到的时间片就越少，程序的时间也就越长，同时CPU会不断的处于运行状态，性能开销较大。可以通过观察一定时间内运行队列中的进程数量来判断CPU是否达到的瓶颈，这就有了负载的概念；



#### 3、负载：

CPU的负载（Load），是指在一段时间内 CPU正在处理以及等待 CPU处理的进程数之和的统计信息，也就是 CPU运行队列长度的统计信息。平均负载（Load Average）是指上一段时间内同时处于运行队列的平均进程数，Load Average是反应系统压力的重要指标，Load Average越高说明对于CPU资源的竞争越激烈，CPU资源比较短缺。对于资源的申请和维护其实也是需要很大的成本，所以在这种高Average Load的情况下CPU资源的长期“热竞争”也是对于硬件的一种损害。
举个例子，把CPU处理进程的过程看成火车站售票：

- 假如售票处有10个窗口，买票的人只有1个，毫无疑问，这个乘客随便找个窗口买完票就可以拍屁股走人了，而且还有9个售票人员可以喝茶看报纸，这时的平均负载是0.1，售票运作毫无压力；
- 如果买票的人有10个，那也还好，一个窗口一个人，所有窗口的售票人员都在工作，也可以快速的买到票，这时的平均负载是1，所有的售票窗口都在运作；
- 如果有20个人在买票，这就要出现排队的情况，售票人员要不停的卖票，直到排队的都买到票，这时平均负载是2，所有售票窗口都在运作，而且出现了排队情况；
- 

如果是100,200人或者更多的人买票，这时售票人员就要不停的工作，为了不把售票人员累坏可能就要考虑增加窗口，也就是扩容了。而实际CPU处理进程时并不是像售票一样卖完一个人下一个人，CPU处理每个进程时是有固定的时间片，如果在时间片内这个任务没有处理完，就要挂起这个任务处理下一个，这是就会产生中断，高负载的情况下会不断产生进程间的调用，从而产生大量中断，造成系统的开销，所以在做性能测试时，要重点关注负载情况，来判读CPU是否达到了瓶颈，依据经验分析，单核CPU负载<2时，系统性能是良好的，当单核CPU负载>5时，那么就表明这个机器存在严重的性能问题。
负载可以通过top，uptime、cat /proc/loadavg等命令查看1分钟，5分钟，15分钟的负载值：

- top

```
top - 10:45:06 up 406 days,  7:29,  5 users,  load average: 0.35, 0.63, 0.54
```

- uptime

```
10:45:23 up 406 days,  7:30,  5 users,  load average: 0.27, 0.60, 0.53
```

- cat /proc/loadavg

```
0.21 0.57 0.52 2/530 32089
```

####  

#### 4、上下文切换

一个标准的Linux内核可以支持运行50～50000个进程运行，对于普通的CPU，内核会调度和执行这些进程。每个进程都会分到CPU的时间片来运行，当一个进程用完时间片或者被更高优先级的进程抢占后，它会备份到CPU的运行队列中，同时其他进程在CPU上运行。这个进程切换的过程被称作上下文切换。Linux系统具有两个不同级别的运行模式，内核态和用户态，其中内核态的运行级别要大于用户态，这个级别可以进行任何操作，一般内核运行与内核态，而应用程序是运行在用户态。当发生上下文切换时，通过系统调用，处于用户态的应用程序就会进入内核空间，待调用完成之后重新返回值用户态运行，因此上下文切换存在系统开销，会一定程度上增加%sys的值。

当一个进程在执行时，CPU的所有寄存器中的值、进程的状态以及堆栈中的内容被称为该进程的上下文。当内核需要切换到另一个进程时，它需要保存当前进程的所有状态，即保存当前进程的上下文，以便在再次执行该进程时，能够必得到切换时的状态执行下去。在LINUX中，当前进程上下文均保存在进程的任务数据结构中。在发生中断时,内核就在被中断进程的上下文中，在内核态下执行中断服务例程。但同时会保留所有需要用到的资源，以便中断服务结束时能恢复被中断进程的执行。

引起上下文切换的原因有哪些？ 对于抢占式操作系统而言， 大体有几种：

1. 当前任务的时间片用完之后，系统CPU正常调度下一个任务；
2. 当前任务碰到IO阻塞，调度线程将挂起此任务，继续下一个任务；
3. 多个任务抢占锁资源，当前任务没有抢到，被调度器挂起，继续下一个任务；
4. 用户代码挂起当前任务，让出CPU时间；
5. 硬件中断； 
6. 

vmstat中的cs为中断数量

```
procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa
 0  0  54484 343748 687816 45583564    0    0     1     3    0    0  1  0 99  0
```

####  

#### 5、中断

中断是指CPU执行程序时，由于发生了某种随机的事件(外部或内部)，引起CPU暂时中断正在运行的程序，转去执行一段特殊的服务程序(称为中断服务程序或中断处理程序)，以处理该事件，该事件处理完后又返回被中断的程序继续执行。引入中断的原因有以下几点：

1. 提高数据传输率；
2. 避免了CPU不断检测外设状态的过程，提高了CPU的利用率。
3. 实现对特殊事件的实时响应。如多任务系统操作系统中缺页中断、设备中断、各类异常、实时钟等
4. 

中断根据中断源的不同可以分为硬中断和软中断：

- 硬中断： 硬中断又称外部中断，是由硬件产生，如键盘，鼠标，打印机等。每个设备或设备集都有它自己的中断请求（IRQ），基于IRQ，CPU可以将相应的请求分发到对应的硬件驱动上，处理中断的驱动是需要在CPU上运行的，因此中断产生时，CPU会中断当前正在运行的任务来处理中断，在多核的操作系统中，一个中断通常只能中断一颗CPU（核）。
- 软中断： 软中断又称内部中断，由软件系统本身发给操作系统内核的中断信号。通常是由硬中断处理程序或进程调度程序对操作系统内核的中断，也就是我们常说的系统调用(System Call)。一般情况下软中断是处理I/O请求时发生订单，这些请求会调用内核中的处理I/O的程序，对于某些设备，I/O请求需要被立即处理，而磁盘I/O请求通常可以排队并且可以稍后处理。根据I/O模型的不同，进程或许会被挂起直到I/O完成，此时内核调度器就会选择另一个进程去运行。I/O可以在进程之间产生并且调度过程通常和磁盘I/O的方式是相同。在I/O密集型系统中可能会出现大量的中断请求，中断会产生中断上下文，造成CPU开。
- 

可以通过top，vmstat等查看CPU的相关命令中监控中断情况

- vmstat中的in为中断数量

```
procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa
 0  0  54484 343748 687816 45583564    0    0     1     3    0    0  1  0 99  0
```

- mpstat中的 %irq %soft分别为硬中断和软中断所占CPU时间片比例

```
Linux 3.2.0-3-amd64 (app-66.photo.163.org)     12/14/16     _x86_64_    (16 CPU)

11:30:28     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle
11:30:28     all    0.63    0.00    0.32    0.06    0.00    0.11    0.00    0.00   98.88
```

####  

#### 6、负载与CPU利用率的关系

负载和CPU利用率都是衡量CPU性能的指标，那么CPU利用率高的话负载一定高吗？要回答这个问题首先要CPU资源在什么情况下会被消耗，CPU利用率其实是指为了处理进程抢占CPU时间片的比例，也就是只有处于运行状态的进程才会获得时间片，从而造成CPU利用率的上升，而所需CPU时间片的多少取决于进程中需要CPU运算逻辑的复杂程度。而负载是指运行队列中等待和运行的进程数的总和，在负载很高时，可能是出于等待运行的进程数较多，这些进行并不会占用过多的时间片，队列中进程消耗的CPU资源也有可能只是集中在某几个对CPU运算依赖较高的几个进程上。所以CPU利用率和负载之间并没有硬性的比例关联关系，衡量CPU性能时要同时关注两个性能指标，综合考虑。



#### 7、遇到CPU利用率高该如何排查

遇到CPU使用率高时，首先确认CPU是消耗在哪一块，如果是内核态占用CPU较高：

1. %iowait 高，这时要重点关注磁盘IO的相关操作，是否存在不合理的写日志操作，数据库操作等；
2. %soft或%cs 高，观察CPU负载是否较高、网卡流量是否较大，可不可以精简数据、代码在是否在多线程操作上存在不合适的中断操作等；
3. %steal 高，这种情况一般发生在虚拟机上，这时要查看宿主机是否资源超限； 
4. 

如果是用户态较高，且没有达到预期的性能，说明应用程序需要优化（java应用）：

1. 可以使用性能组脚本 show-busy-java-threads.sh（top和jstack命令的结合）抓去CPU利用率高的线程堆栈，查看相关操作；
2. 也可使用 jprofile 进行CPU热点的监控，操作步骤见wiki：<http://doc.hz.netease.com/pages/viewpage.action?pageId=36451057>



#### 8、性能指标总结

从网上看到一个按照经验给出的指标，可以参考：

1. 对于每一个CPU来说运行队列不要超过2，例如，如果是双核CPU就不要超过4；
2. 如果CPU在满负荷运行，应该符合下列分布: 
3. 

- User Time：65%～70%，如果高于此数值可以考虑对应用程序进行优化；
- System Time：30%～35%，高于此数值时，观察是否有过多的中断或上下文切换；
- Idle：0%～5% 
- 

   3 对于上下文切换要结合CPU使用率来看，如果CPU使用满足上述分布，大量的上下文切换也是可以接受的。



### 路由

路由的分类: 精度越高, 优先级越高

* 主机路由
* 网络路由
* 默认路由

路由表的关键构成

* 路由目标: 即上面的3类路由
* 出口网卡:
* 下一跳: 下一跳即当前局域网网关, 所以GateWay就是下一跳. 若Gateway为0.0.0.0, 说明这条路由的范围(scope)在局域网中



### 策略路由

策略路由的配置需要使用 iproute 工具. 策略路由的**规则**, 包括三个方面: 

* 条件
* 优先级别: 数字越小, 级别越高.
* 路由表ID

注意, 规则的匹配过程, 是按优先级别从高倒低匹配. 系统的三个路由表的优先级如下:

```shell
▶ sudo ip rule list
0:	from all lookup local 
32766:	from all lookup main 
32767:	from all lookup default
```

而自己添加的规则默认会使用32765, 32764, ... 的优先级. 这样能确保匹配顺序为: **local表->自定义表->main表->default表.**

一个比较容易混淆的点, 是: **local表的ID是255, 而ID为0的表是一张系统保留总表.** 两者内容是不一样的, 不要因为local表优先级是0, 就以为0表的优先级在0!!!

```shell
sudo ip r l t 0
sudo ip r l t local
```



系统的三个路由表如下:

在 Linux 系统启动时，内核会为路由策略数据库配置三条缺省的规则：

0：匹配任何条件，查询路由表local(ID 255)，该表local是一个特殊的路由表，包含对于**本地和广播地址**的优先级控制路由。rule 0非常特殊，不能被删除或者覆盖。

32766：匹配任何条件，查询路由表main(ID 254)，该表是一个通常的表，包含所有的无策略路由。系统管理员可以删除或者使用另外的规则覆盖这条规则。

32767：匹配任何条件，查询路由表default(ID 253)，该表是一个空表，它是后续处理保留。对于前面的策略没有匹配到的数据包，系统使用这个策略进行处理，这个规则也可以删除。



### 网卡名的更改

虚拟机中网卡会被改名为enp0sX, 和传统的eth0不一致. 这里记录下如何改回来.

```shell
# 1. 修改/etc/default/grub
vi /etc/default/grub
> GRUB_CMDLINE_LINUX="net.ifnames=0 biosdevname=0"

# 2. 生效
grub-mkconfig -o /boot/grub/grub.cfg

# 3. 修改interfaces网卡名配置
vi /etc/network/interfaces

# 4. 重启即可
```



### 网络接口"bonding"

> Bonding”就是将多块网卡绑定同一IP地址对外提供服务，可以实现高可用/负载均衡。
>
> 在企业以及电信Linux服务器环境上，网络配置都会使用Bonding技术做网口硬件层面的冗余，防止单个网络应用的单点故障。

本文将介绍Linux下的`Bonding`技术，利用这种技术可以将多块网卡接口通过绑定虚拟成一块网卡，在用户看来这个聚合起来的设备好像是一个单独的以太网接口设备，通俗点讲就是多块网卡具有相同的IP地址而并行连接聚合成一个逻辑链路工作



### https接口弱网时容易请求超时

在排查海外玩家请求XXMYGW https接口时, 发现总有部分玩家日志显示请求XXMYGW超时了, 但XXMYGW服日志却没有该玩家的请求日志. 经过抓包分析, 原因是: https协议握手过程存在多次客户端和服务端的交互, 弱网时每次交互耗时都会比较长, 导致https协议还没交互完, 客户端就因为超时主动断开连接了.

这里的https采用tlsv1.2协议, 网上查了协议的作用和握手过程如下:

> TLSv1.2协议
> 首先明确TLS的作用三个作用
> （1）身份认证
> 通过证书认证来确认对方的身份，防止中间人攻击
> （2）数据私密性
> 使用对称性密钥加密传输的数据，由于密钥只有客户端/服务端有，其他人无法窥探。
> （3）数据完整性
> 使用摘要算法对报文进行计算，收到消息后校验该值防止数据被篡改或丢失。
>
> TLS传输过程
> 下面是使用wireshark抓包的结果，其中1-4是握手阶段，5是指握手后双方使用商议好的秘钥进行通讯。
>
> ![img](/img/system/tlsv1.2_protocol_handshake.png)
>
> 2中并列着Server Hello,Certificate等多个类型，是因为这是一个Multiple Handshake Messages，一次性发送多个握手协议包。

可以看出, https协议的握手交互, 除了tcp的三次握手, 还要加上tlsv1.2的四次握手, 总共需要经历7次交互, 如果客户端和服务端的单向延迟为200ms, 7次交互最少耗时为1.4s, 此后才能开始发送真正的数据包.

